output_name: 00_test

output_dir:
  result: /scratch/c.c21051562/workspace/arrg_img2text/outputs/results
  model: /scratch/c.c21051562/workspace/arrg_img2text/outputs/models
  checkpoint: /scratch/c.c21051562/workspace/arrg_img2text/outputs/checkpoints

# For preprocess, we will load data from data_path
# For training, we will load dataset from preprocess.cache_path
data_path:
  mimic: /scratch/c.c21051562/resources/data/mimic-cxr
  interpret: /scratch/c.c21051562/resources/data/interpret-cxr
  interpret-test-public: /scratch/c.c21051562/resources/data/interpret-cxr-test-public
  custom_text: /scratch/c.c21051562/resources/data/interpret_text_exp/interpret_text_findings_effusion

target_section: findings

model_name_or_path:
  clip: /scratch/c.c21051562/resources/downloaded_models/clip-vit-base-patch32
  swinv2: /scratch/c.c21051562/resources/downloaded_models/swinv2-base-patch4-window8-256
  rad_dino_maira2: /scratch/c.c21051562/resources/downloaded_models/rad-dino-maira-2
  llama32_1b: /scratch/c.c21051562/resources/downloaded_models/Llama-3.2-1B

mlflow_url: "http://localhost:6006" # mlflow server --host 127.0.0.1 --port 6006
max_checkpoints_to_keep: 3

# path to checkpoint or, bool;
# if True, then use the latest checkpoint from `output_dir.checkpoint/output_name`
resume_from_checkpoint: False
# if eval_only=True, it will load the best pre_trained model saved at `output_dir.model/output_name`, and eval the model on test set.
use_debug_subset: False
run_mode: "finetune" # Choose from [preprocess, pretrain, eval_pretrained, finetune, eval_finetuned]

# Resize the image to according to the encoder processor, this can speed up the training
preprocess:
  image_processor: rad_dino_maira2
  text_processor: llama32_1b
  cache_path: /scratch/c.c21051562/workspace/arrg_img2text/dataset_cache/interpretcxr_full_text_img518
  batched: True
  batch_size: 64
  num_proc: 16

model:
  vision_model: rad_dino_maira2
  language_model: llama32_1b
  chat_template: /scratch/c.c21051562/workspace/arrg_img2text/llama3_chat_template.jinja

pretrain:
  seed: 42
  num_epochs: 1
  batch_size: 1
  grad_accum_steps:
  warmup_proportion: 0.1

  lr: 0.00001
  clip_grad_norm: 1.0
  mixed_precision: "bf16" # no, fp16, bf16. bf16 recommended for A100, fp16 for V100

  print_loss_per_n_steps: 200 # logging loss on .log file
  # eval_per_steps: 100 # 550395 / 32 = 17199, 170801 / 8 = 21350
  ckp_per_steps: 10000 # 550395 / 32 = 17199, 170801 / 8 = 21350

  eval_batch_size: 16
  max_new_tokens: 512
  print_pred_per_n_steps: 500

finetune:
  # if use pretrained=True, then load the pretrained img_projector weights from path,
  # else init the img_projector with random weights
  use_pretrained: False
  pretain_model_path: /scratch/c.c21051562/workspace/arrg_img2text/outputs/models/4_1_fsdo_peft_test_pretrain

  seed: 42
  num_epochs: 1
  batch_size: 1
  grad_accum_steps: 8
  warmup_proportion: 0.1

  lr: 0.00001
  clip_grad_norm: 1.0
  mixed_precision: "bf16" # no, fp16, bf16. bf16 recommended for A100, fp16 for V100

  print_loss_per_n_steps: 200 # logging loss on .log file
  # eval_per_steps: 100 # 550395 / 32 = 17199, 170801 / 8 = 21350
  ckp_per_steps: 10000 # 550395 / 8 = 68800, 170801 / 8 = 21350

  eval_batch_size: 16
  max_new_tokens: 512
  print_pred_per_n_steps: 500
