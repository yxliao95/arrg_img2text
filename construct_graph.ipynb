{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get root nodes from CXRGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import datetime\n",
    "import glob\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "import time\n",
    "from collections import Counter\n",
    "from dataclasses import asdict, dataclass, field\n",
    "from typing import Any, Optional, Tuple, Union\n",
    "\n",
    "import datasets\n",
    "import imagehash\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import requests\n",
    "import torch\n",
    "import transformers\n",
    "import yaml\n",
    "from accelerate import Accelerator, DataLoaderConfiguration\n",
    "from accelerate.logging import MultiProcessAdapter\n",
    "from accelerate.utils import GradientAccumulationPlugin, gather, gather_object, set_seed\n",
    "from datasets import DatasetDict, concatenate_datasets, load_from_disk\n",
    "from PIL import Image\n",
    "from torch import nn\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, Dataset, SequentialSampler\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoProcessor,\n",
    "    AutoImageProcessor,\n",
    "    CLIPModel,\n",
    "    CLIPProcessor,\n",
    "    CLIPVisionModel,\n",
    "    PretrainedConfig,\n",
    "    PreTrainedModel,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "from transformers.modeling_outputs import BaseModelOutput\n",
    "import pstats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_final = load_from_disk(\"/home/yuxiang/liao/workspace/arrg_img2text/dataset_cache/clipbase_rbg224\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Entity:\n",
    "    def __init__(self, start, end, label, sent_id, tok_list=None, tok_str=None):\n",
    "        self.id = None\n",
    "        self.tok_indices = [start, end]\n",
    "        self.label = label\n",
    "\n",
    "        self.sent_id = sent_id\n",
    "        if tok_list:\n",
    "            self.tok_list = tok_list\n",
    "            self.tok_str = \" \".join(tok_list) if not tok_str else tok_str\n",
    "        elif tok_str:\n",
    "            self.tok_str = tok_str\n",
    "            self.tok_list = tok_str.split(\" \")\n",
    "\n",
    "        if \"Observation\" in label:\n",
    "            self.label_type = \"OBS\"\n",
    "        elif \"Anatomy\" == label:\n",
    "            self.label_type = \"ANAT\"\n",
    "        else:\n",
    "            self.label_type = \"LOCATT\"\n",
    "\n",
    "        self.attr_normal = \"NA\"\n",
    "        self.attr_action = \"NA\"\n",
    "        self.attr_change = \"NA\"\n",
    "\n",
    "        self.chain_info = {\n",
    "            \"modify\": {\"from\": [], \"to\": []},\n",
    "            \"part_of\": {\"from\": [], \"to\": []},\n",
    "            \"located_at\": {\"from\": [], \"to\": []},\n",
    "            \"suggestive_of\": {\"from\": [], \"to\": []},\n",
    "        }\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        # return f\"{self.tok_str} {self.tok_indices}: {self.label}, {self.attr_normal, self.attr_action, self.attr_change}\"\n",
    "        return f\"{self.tok_str}\"\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return self.__repr__()\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        if isinstance(other, Entity):\n",
    "            return self.tok_indices == other.tok_indices\n",
    "        else:\n",
    "            return other == self.tok_indices\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(str(self.tok_indices))\n",
    "\n",
    "\n",
    "class Relation:\n",
    "    def __init__(self, subj_ent, obj_ent, label):\n",
    "        self.label = label\n",
    "        self.subj_ent = subj_ent\n",
    "        self.obj_ent = obj_ent\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{self.subj_ent.tok_str} {self.label} {self.obj_ent.tok_str}\"\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return self.__repr__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceGraph:\n",
    "    def __init__(self, ents):\n",
    "        self.id = None\n",
    "        self.ents = sorted(ents, key=lambda x: x.tok_indices[0])\n",
    "        self.sent_id = ents[0].sent_id\n",
    "        self.paired_groups = []\n",
    "        self.unpaired_groups = []\n",
    "\n",
    "        assert len(set([i.sent_id for i in ents])) == 1\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{[i.tok_str for i in self.ents]}\"\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return self.__repr__()\n",
    "\n",
    "\n",
    "class PairedGroup:\n",
    "    def __init__(self, subj, obj, rel, loc_atts=[]):\n",
    "        self.id = None\n",
    "        self.subj = subj\n",
    "        self.obj = obj\n",
    "        self.rel = rel\n",
    "        self.loc_atts = loc_atts\n",
    "\n",
    "        self.expanded_subj_branches = self._resolve_core_branches(subj)\n",
    "        self.expanded_obj_branches = self._resolve_core_branches(obj)\n",
    "\n",
    "    def _resolve_core_branches(self, target_ent):\n",
    "        partof_branches = find_branches(curr_ent=target_ent, keys=[(\"part_of\", \"to\")], types=[target_ent.label_type])\n",
    "        return [sorted(branch, key=lambda x: x.tok_indices[0]) for branch in partof_branches]\n",
    "\n",
    "    def get_in_used_ents(self):\n",
    "        return [ent for branch in self.expanded_subj_branches for ent in branch] + [ent for branch in self.expanded_obj_branches for ent in branch] + self.loc_atts\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{self.subj} {self.rel} {self.obj}\"\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return self.__repr__()\n",
    "\n",
    "\n",
    "class UnpairedGroup:\n",
    "    def __init__(self, root):\n",
    "        self.id = None\n",
    "        self.root = root\n",
    "\n",
    "        self.expanded_root_branches = self._resolve_core_branches(root)\n",
    "\n",
    "    def _resolve_core_branches(self, target_ent):\n",
    "        partof_branches = find_branches(curr_ent=target_ent, keys=[(\"part_of\", \"from\")], types=[target_ent.label_type])\n",
    "        return [sorted(branch, key=lambda x: x.tok_indices[0]) for branch in partof_branches]\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{self.root}\"\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return self.__repr__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_linked_ents(curr_ent, visited, group):\n",
    "    visited.add(curr_ent)\n",
    "    group.append(curr_ent)\n",
    "    neighbors = [ent for nested_dict in curr_ent.chain_info.values() for adjacent_ents in nested_dict.values() for ent in adjacent_ents]\n",
    "    for next_ent in neighbors:\n",
    "        if next_ent not in visited:\n",
    "            search_linked_ents(next_ent, visited, group)\n",
    "\n",
    "\n",
    "def search_linked_ents_by_keys(curr_ent, visited, group, keys):\n",
    "    if curr_ent:\n",
    "        visited.add(curr_ent)\n",
    "        group.append(curr_ent)\n",
    "        neighbors = [ent for k1, k2 in keys for ent in curr_ent.chain_info[k1][k2]]\n",
    "    else:  # provide group rather than curr_ent\n",
    "        neighbors = []\n",
    "        for _curr_ent in group:\n",
    "            visited.add(_curr_ent)\n",
    "            neighbors.extend([ent for k1, k2 in keys for ent in _curr_ent.chain_info[k1][k2]])\n",
    "\n",
    "    for next_ent in neighbors:\n",
    "        if next_ent not in visited:\n",
    "            search_linked_ents_by_keys(next_ent, visited, group, keys)\n",
    "\n",
    "\n",
    "def search_linked_ents_by_keys_types(curr_ent, visited, group, keys, types):\n",
    "    visited.add(curr_ent)\n",
    "    group.append(curr_ent)\n",
    "    neighbors = [ent for k1, k2 in keys for ent in curr_ent.chain_info[k1][k2] if ent.label_type in types]\n",
    "    for next_ent in neighbors:\n",
    "        if next_ent not in visited:\n",
    "            search_linked_ents_by_keys_types(next_ent, visited, group, keys)\n",
    "\n",
    "\n",
    "def find_branches(curr_ent=None, branches=None, keys=[(\"located_at\", \"to\")], types=[\"LOCATT\", \"ANAT\", \"OBS\"], curr_branch_idx=0):\n",
    "    if branches is None:\n",
    "        branches = [[]]\n",
    "\n",
    "    # 如果当前ent不在分支中，就加入分支。然后按照目标key，获取下一批候选ents\n",
    "    if curr_ent not in branches[curr_branch_idx]:\n",
    "        branches[curr_branch_idx].append(curr_ent)\n",
    "    candidate_entities = [ent for k1, k2 in keys for ent in curr_ent.chain_info[k1][k2] if ent.label_type in types]\n",
    "\n",
    "    # 如果节点数量大于1，那么就要增加一个新的分支\n",
    "    for new_branch_idx in range(len(candidate_entities)):\n",
    "        next_branch_idx = curr_branch_idx + new_branch_idx\n",
    "        if next_branch_idx >= len(branches):\n",
    "            branches.append(list(branches[curr_branch_idx]))\n",
    "\n",
    "    for new_branch_idx, next_ent in enumerate(candidate_entities):\n",
    "        next_branch_idx = curr_branch_idx + new_branch_idx\n",
    "        find_branches(curr_ent=next_ent, branches=branches, keys=keys, types=types, curr_branch_idx=next_branch_idx)\n",
    "\n",
    "    return branches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'attr_action': 'Removable',\n",
       "   'attr_change': 'Unchanged',\n",
       "   'attr_normality': 'NA',\n",
       "   'ent_toks': ['Persistent'],\n",
       "   'tok_indices': [0, 1]}],\n",
       " [{'attr_action': 'Removable',\n",
       "   'attr_change': 'Negative',\n",
       "   'attr_normality': 'NA',\n",
       "   'ent_toks': ['Increased'],\n",
       "   'tok_indices': [0, 1]}],\n",
       " [{'attr_action': 'Removable',\n",
       "   'attr_change': 'Negative',\n",
       "   'attr_normality': 'NA',\n",
       "   'ent_toks': ['increased'],\n",
       "   'tok_indices': [5, 6]}],\n",
       " []]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_final[\"test\"][-1][\"cxrgraph_attr\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2136 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2136/2136 [00:41<00:00, 51.72it/s] \n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import bisect\n",
    "\n",
    "\n",
    "def max_coverage_spans(spans):\n",
    "    if not spans:\n",
    "        return [], 0\n",
    "\n",
    "    # 按结束时间升序排序\n",
    "    sorted_spans = sorted(spans, key=lambda x: x[1])\n",
    "    n = len(sorted_spans)\n",
    "    starts = [s[0] for s in sorted_spans]\n",
    "    ends = [s[1] for s in sorted_spans]\n",
    "    lengths = [e - s for s, e in sorted_spans]\n",
    "\n",
    "    # 预处理j_values数组，记录每个i对应的最大的j，使得 ends[j] <= starts[i]\n",
    "    j_values = []\n",
    "    for i in range(n):\n",
    "        start_i = starts[i]\n",
    "        j = bisect.bisect_right(ends, start_i) - 1  # 二分查找, 找到第一个`大于`start_i的位置\n",
    "        j_values.append(j)\n",
    "\n",
    "    # 构建 dp 数组，其中 dp[i] 表示前 i+1 个 span 的最大总覆盖率。通过比较包含当前 span 和不包含当前 span 的情况，确定最优解。\n",
    "    # dp记录了选中下一个span之后的总覆盖率\n",
    "    dp = [0] * n\n",
    "    dp[0] = lengths[0]\n",
    "    for i in range(1, n):\n",
    "        j = j_values[i]\n",
    "        current = lengths[i] + (dp[j] if j >= 0 else 0)\n",
    "        dp[i] = max(dp[i - 1], current)\n",
    "\n",
    "    # 回溯找出选中的span。从最后一个span开始，如果当前span被选中，则跳到j_values[i]对应的span\n",
    "    # 当dp发生变化时，说明\n",
    "    selected_indices = []\n",
    "    i = n - 1\n",
    "    while i >= 0:\n",
    "        if i == 0:\n",
    "            if dp[i] == lengths[i]:\n",
    "                selected_indices.append(i)\n",
    "            break\n",
    "        if dp[i] > dp[i - 1]:\n",
    "            selected_indices.append(i)\n",
    "            i = j_values[i]\n",
    "        else:\n",
    "            i -= 1\n",
    "\n",
    "    selected_indices.reverse()\n",
    "    selected_spans = [sorted_spans[i] for i in selected_indices]\n",
    "    total_coverage = dp[-1]\n",
    "\n",
    "    return selected_indices, selected_spans, total_coverage\n",
    "\n",
    "\n",
    "def resolve_ent_rel(split_sent_idx, cxrgraph_ent_lst, cxrgraph_rel_lst, cxrgraph_attr_lst, radlex_lst):\n",
    "    ent_list = []\n",
    "    rel_list = []\n",
    "    for ent in cxrgraph_ent_lst:\n",
    "        ent = Entity(start=ent[\"tok_indices\"][0], end=ent[\"tok_indices\"][1], label=ent[\"ent_type\"], tok_list=ent[\"ent_toks\"], sent_id=split_sent_idx)\n",
    "        ent_list.append(ent)\n",
    "    for attr in cxrgraph_attr_lst:\n",
    "        ent = ent_list[ent_list.index(attr[\"tok_indices\"])]\n",
    "        ent.attr_normal = attr[\"attr_normality\"]\n",
    "        ent.attr_action = attr[\"attr_action\"]\n",
    "        ent.attr_change = attr[\"attr_change\"]\n",
    "    for rel in cxrgraph_rel_lst:\n",
    "        subj_ent = ent_list[ent_list.index(rel[\"subj_tok_indices\"])]\n",
    "        obj_ent = ent_list[ent_list.index(rel[\"obj_tok_indices\"])]\n",
    "        label = rel[\"rel_type\"]\n",
    "        subj_ent.chain_info[label][\"to\"].append(obj_ent)\n",
    "        obj_ent.chain_info[label][\"from\"].append(subj_ent)\n",
    "        rel_list.append(Relation(subj_ent, obj_ent, label))\n",
    "\n",
    "    # Set ent id\n",
    "    for ent_idx, ent in enumerate(sorted(ent_list, key=lambda x: x.tok_indices[0])):\n",
    "        ent.id = f\"E{ent_idx}\"\n",
    "\n",
    "    # 选择覆盖率最大的radlex子集\n",
    "    radlex_ent_indices = [node[\"tok_indices\"] for node in radlex_lst]\n",
    "    selected_idx_list, _, _ = max_coverage_spans(radlex_ent_indices)\n",
    "\n",
    "    # 用radlex的ent替换cxrgraph的ent\n",
    "    for radlex_idx in selected_idx_list:\n",
    "        radlex_ent = radlex_lst[radlex_idx]\n",
    "        merged_cxrgraph_ents = []\n",
    "        for cxrgraph_ent in ent_list:\n",
    "            # 如果cxrgrpah被radlex包含，那么就加入候选集等待替换；如果cxrgraph和radlex有交集，那么就跳过这个radlex\n",
    "            pos_ab = check_span_relation(cxrgraph_ent.tok_indices, radlex_ent[\"tok_indices\"])\n",
    "            if pos_ab in [\"equal\", \"inside\"]:\n",
    "                merged_cxrgraph_ents.append(cxrgraph_ent)\n",
    "            elif pos_ab == \"overlap\":\n",
    "                break\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        # 如果merged_cxrgraph_ents不为空，那么就用radlex替换候选集的cxrgraph ent\n",
    "        if merged_cxrgraph_ents:\n",
    "            inherited_label = get_label_inheritance(merged_cxrgraph_ents)\n",
    "            inherited_attr_dict = get_attr_inheritance(merged_cxrgraph_ents)\n",
    "\n",
    "            new_ent = Entity(start=radlex_ent[\"tok_indices\"][0], end=radlex_ent[\"tok_indices\"][1], label=inherited_label, tok_str=radlex_ent[\"radlex_name\"], sent_id=split_sent_idx)\n",
    "            new_ent.attr_normal = inherited_attr_dict[\"normality\"]\n",
    "            new_ent.attr_action = inherited_attr_dict[\"action\"]\n",
    "            new_ent.attr_change = inherited_attr_dict[\"change\"]\n",
    "            new_ent.id = radlex_ent[\"radlex_id\"]\n",
    "\n",
    "            # inherit chain info\n",
    "            for cxrgraph_ent in merged_cxrgraph_ents:\n",
    "                for rel_type, from_to_dict in cxrgraph_ent.chain_info.items():\n",
    "                    # 把merged_cxrgraph_ents的from和to的关系都继承过来，如果是内部ents之间指向关系，那么就跳过\n",
    "                    for key, value_lst in from_to_dict.items():\n",
    "                        for value in value_lst:\n",
    "                            if value not in merged_cxrgraph_ents:\n",
    "                                new_ent.chain_info[rel_type][key].append(value)\n",
    "\n",
    "            # replace from ent_list\n",
    "            ent_list.append(new_ent)\n",
    "            for cxrgraph_ent in merged_cxrgraph_ents:\n",
    "                ent_list.remove(cxrgraph_ent)\n",
    "\n",
    "            # replace from rel_list\n",
    "            # pleural_effusion 应该把 pleural 和 effusion 都替换掉。在rel中则包括：\n",
    "            #   opacifications suggestive_of effusions\n",
    "            #   bilateral modify pleural\n",
    "            #   effusions located_at pleural\n",
    "            rel_objs_tobe_removed = []\n",
    "            for rel in rel_list:\n",
    "                if rel.subj_ent in merged_cxrgraph_ents and rel.obj_ent in merged_cxrgraph_ents:\n",
    "                    # 关于 from 和 to 的关系链，在新的ent中已经继承了，所以这里不需要处理\n",
    "                    rel_objs_tobe_removed.append(rel)\n",
    "                elif rel.subj_ent in merged_cxrgraph_ents:\n",
    "                    # subj need to be replaced\n",
    "                    rel.obj_ent.chain_info[rel.label][\"from\"].remove(rel.subj_ent)\n",
    "                    rel.obj_ent.chain_info[rel.label][\"from\"].append(new_ent)\n",
    "                    rel.subj_ent = new_ent\n",
    "                elif rel.obj_ent in merged_cxrgraph_ents:\n",
    "                    rel.subj_ent.chain_info[rel.label][\"to\"].remove(rel.obj_ent)\n",
    "                    rel.subj_ent.chain_info[rel.label][\"to\"].append(new_ent)\n",
    "                    rel.obj_ent = new_ent\n",
    "\n",
    "            for rel in rel_objs_tobe_removed:\n",
    "                rel_list.remove(rel)\n",
    "\n",
    "    assert len(rel_list) == len(set(rel_list)), f\"{rel_list}\"\n",
    "    return ent_list, rel_list\n",
    "\n",
    "\n",
    "def get_label_inheritance(cxrgraph_ents):\n",
    "    candi_labels = [ent.label for ent in cxrgraph_ents]\n",
    "    if \"Observation-Absent\" in candi_labels:\n",
    "        return \"Observation-Absent\"\n",
    "    elif \"Observation-Uncertain\" in candi_labels:\n",
    "        return \"Observation-Uncertain\"\n",
    "    elif \"Observation-Present\" in candi_labels:\n",
    "        return \"Observation-Present\"\n",
    "    elif \"Aanatomy\" in candi_labels:\n",
    "        return \"Aanatomy\"\n",
    "    else:\n",
    "        return \"Location-Attribute\"\n",
    "\n",
    "\n",
    "def get_attr_inheritance(cxrgraph_ents):\n",
    "    candi_attr_normal = [ent.attr_normal for ent in cxrgraph_ents]\n",
    "    candi_attr_action = [ent.attr_action for ent in cxrgraph_ents]\n",
    "    candi_attr_change = [ent.attr_change for ent in cxrgraph_ents]\n",
    "    assert all([i[0].istitle() for i in candi_attr_change]), f\"{candi_attr_change} {candi_attr_normal} {candi_attr_action}\"\n",
    "\n",
    "    output_attr = {\"normality\": \"NA\", \"action\": \"NA\", \"change\": \"NA\"}\n",
    "    if \"Normal\" in candi_attr_normal:\n",
    "        output_attr[\"normality\"] = \"Normal\"\n",
    "    elif \"Abnormal\" in candi_attr_normal:\n",
    "        output_attr[\"normality\"] = \"Abnormal\"\n",
    "\n",
    "    if \"Essential\" in candi_attr_action:\n",
    "        output_attr[\"action\"] = \"Essential\"\n",
    "    elif \"Removable\" in candi_attr_action:\n",
    "        output_attr[\"action\"] = \"Removable\"\n",
    "\n",
    "    if \"Positive\" in candi_attr_change:\n",
    "        output_attr[\"change\"] = \"Positive\"\n",
    "    elif \"Negative\" in candi_attr_change:\n",
    "        output_attr[\"change\"] = \"Negative\"\n",
    "    elif \"Unchanged\" in candi_attr_change:\n",
    "        output_attr[\"change\"] = \"Unchanged\"\n",
    "\n",
    "    return output_attr\n",
    "\n",
    "\n",
    "def check_span_relation(ent_a_indices, ent_b_indices):\n",
    "    if ent_a_indices[1] <= ent_b_indices[0]:\n",
    "        return \"before\"\n",
    "    elif ent_a_indices[0] >= ent_b_indices[1]:\n",
    "        return \"after\"\n",
    "    elif ent_a_indices[0] == ent_b_indices[0] and ent_a_indices[1] == ent_b_indices[1]:\n",
    "        return \"equal\"\n",
    "    elif ent_a_indices[0] <= ent_b_indices[0] and ent_b_indices[1] <= ent_a_indices[1]:\n",
    "        return \"contain\"\n",
    "    elif ent_b_indices[0] <= ent_a_indices[0] and ent_a_indices[1] <= ent_b_indices[1]:\n",
    "        return \"inside\"\n",
    "    else:\n",
    "        return \"overlap\"\n",
    "\n",
    "\n",
    "def resolve_sentence_graphs(ent_list, rel_list):\n",
    "    sentence_graphs = []\n",
    "    visited_ents = set()\n",
    "    for ent in ent_list:\n",
    "        if ent not in visited_ents:\n",
    "            sent_ents = []\n",
    "            search_linked_ents(ent, visited_ents, sent_ents)\n",
    "            sentence_graphs.append(SentenceGraph(sent_ents))\n",
    "\n",
    "    assert len(ent_list) == len(visited_ents)\n",
    "    assert len(ent_list) == len([i for g in sentence_graphs for i in g.ents])\n",
    "\n",
    "    # 从sent_graph提取rel_nodes\n",
    "    groups = []\n",
    "    in_used_ents = set()\n",
    "    for sent_graph in sentence_graphs:\n",
    "        for curr_ent in sent_graph.ents:\n",
    "            if curr_ent.label_type == \"LOCATT\":\n",
    "                continue\n",
    "\n",
    "            # curr_ent -> located_at -> next_ent\n",
    "            for next_ent in curr_ent.chain_info[\"located_at\"][\"to\"]:\n",
    "                next_ent  # It should be OBS, however, it could be others when the inference is wrong\n",
    "                if next_ent.label_type == \"LOCATT\":\n",
    "                    locatt_branches = find_branches(curr_ent=next_ent, keys=[(\"located_at\", \"to\")], types=[\"LOCATT\"])  # incorrect, may have multiple LOCATTs in a chain\n",
    "                    for locatt_chain in locatt_branches:\n",
    "                        locatt_ent = locatt_chain[-1]  # last locatt ent\n",
    "\n",
    "                        # the last locatt ent should have located_at_to node\n",
    "                        # but in a wrong inference, the last LOCATT may not have located_at_to node\n",
    "                        # ? -> located_at -> LOCATT -> located_at -> LOCATT -> not located_at -> ?\n",
    "                        if locatt_ent.chain_info[\"located_at\"][\"to\"]:\n",
    "                            obj_ents = locatt_ent.chain_info[\"located_at\"][\"to\"]\n",
    "                        elif locatt_ent.chain_info[\"part_of\"][\"to\"]:\n",
    "                            obj_ents = locatt_ent.chain_info[\"part_of\"][\"to\"]\n",
    "                        elif locatt_ent.chain_info[\"modify\"][\"to\"]:\n",
    "                            obj_ents = locatt_ent.chain_info[\"modify\"][\"to\"]\n",
    "                        else:\n",
    "                            obj_ents = None\n",
    "\n",
    "                        if obj_ents:\n",
    "                            for obj_ent in obj_ents:\n",
    "                                # curr_ent -> located_at -> LOCATT ... -> located_at -> obj_ent\n",
    "                                p_group = PairedGroup(subj=curr_ent, obj=obj_ent, rel=\"located_at\", loc_atts=locatt_chain)\n",
    "                                sent_graph.paired_groups.append(p_group)\n",
    "                                in_used_ents.update(p_group.get_in_used_ents())\n",
    "                        else:\n",
    "                            p_group = PairedGroup(subj=curr_ent, obj=locatt_ent, rel=\"located_at\")\n",
    "                            sent_graph.paired_groups.append(p_group)\n",
    "                            in_used_ents.update(p_group.get_in_used_ents())\n",
    "                else:\n",
    "                    # ANAT/OBS -> located_at -> ANAT/OBS\n",
    "                    p_group = PairedGroup(subj=curr_ent, obj=next_ent, rel=\"located_at\")\n",
    "                    sent_graph.paired_groups.append(p_group)\n",
    "                    in_used_ents.update(p_group.get_in_used_ents())\n",
    "            # curr_ent -> suggestive_of -> next_ent\n",
    "            for next_ent in curr_ent.chain_info[\"suggestive_of\"][\"to\"]:\n",
    "                p_group = PairedGroup(subj=curr_ent, obj=next_ent, rel=\"suggestive_of\")\n",
    "                sent_graph.paired_groups.append(p_group)\n",
    "                in_used_ents.update(p_group.get_in_used_ents())\n",
    "\n",
    "    # not in-used root nodes (no loc_at, sugg_of)\n",
    "    for sent_graph in sentence_graphs:\n",
    "        root_ents = []  # a > modify > b, a > modify > c, root=[a, c]\n",
    "        for ent in sent_graph.ents:\n",
    "            if ent in in_used_ents:\n",
    "                continue\n",
    "            to_neighbors = ent.chain_info[\"part_of\"][\"to\"] + ent.chain_info[\"modify\"][\"to\"]\n",
    "            if not to_neighbors:\n",
    "                root_ents.append(ent)  # 有向图的最后一个节点为root node\n",
    "                in_used_ents.add(ent)\n",
    "\n",
    "        for root in root_ents:\n",
    "            group = UnpairedGroup(root)\n",
    "            sent_graph.unpaired_groups.append(group)\n",
    "\n",
    "    return sentence_graphs\n",
    "\n",
    "\n",
    "stop = 0\n",
    "for doc in tqdm(ds_final[\"test\"]):\n",
    "    for split_sent_idx, (cxrgraph_ent, cxrgraph_rel, cxrgraph_attr, radlex) in enumerate(zip(doc[\"cxrgraph_ent\"], doc[\"cxrgraph_rel\"], doc[\"cxrgraph_attr\"], doc[\"radlex\"])):\n",
    "\n",
    "        # resolve ent and rel from json\n",
    "        ent_list, rel_list = resolve_ent_rel(split_sent_idx, cxrgraph_ent, cxrgraph_rel, cxrgraph_attr, radlex)\n",
    "\n",
    "        # 构建sent_graph\n",
    "        sentence_graphs = resolve_sentence_graphs(ent_list, rel_list)\n",
    "\n",
    "        sent_graph_repr = []\n",
    "        for sent_graph in sentence_graphs:\n",
    "            for group in sent_graph.paired_groups:\n",
    "                sent_graph_repr.append(graph)\n",
    "\n",
    "            for graph in sent_graph.unpaired_groups:\n",
    "                graph.root\n",
    "                sent_graph_repr.append(graph)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arrg_img2text",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
