{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/c.c21051562/conda/.conda/envs/arrg_img2text/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import ast\n",
    "import datetime\n",
    "import gc\n",
    "import glob\n",
    "import json\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "import time\n",
    "from collections import Counter, defaultdict\n",
    "from dataclasses import asdict, dataclass, field\n",
    "from difflib import get_close_matches\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "from datasets import DatasetDict, concatenate_datasets, load_from_disk\n",
    "import datasets\n",
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_image_datasets(data_paths):\n",
    "    dataset_interpret = load_from_disk(data_paths[\"interpret\"])\n",
    "    dataset_mimic = load_from_disk(data_paths[\"mimic\"])\n",
    "\n",
    "    # Concat both\n",
    "    dataset_train_dev = DatasetDict({\"train\": concatenate_datasets([dataset_interpret[\"train\"], dataset_mimic[\"train\"]]), \"validation\": concatenate_datasets([dataset_interpret[\"validation\"], dataset_mimic[\"validation\"]])})\n",
    "\n",
    "    dataset_test = load_from_disk(data_paths[\"interpret-test-public\"])\n",
    "\n",
    "    ds_img = DatasetDict({\"train\": dataset_train_dev[\"train\"], \"validation\": dataset_train_dev[\"validation\"], \"test\": dataset_test[\"test\"]})\n",
    "    return ds_img\n",
    "\n",
    "def merge_dataset(img_dataset, graph_dataset):\n",
    "    imgId_2_graphRowIdx = {}\n",
    "    for graph_row_idx, doc_key in enumerate(graph_dataset[\"doc_key\"]):\n",
    "        _, img_id, _ = doc_key.split(\"#\")  # doc_key = test#2250#findings\n",
    "        imgId_2_graphRowIdx[int(img_id)] = int(graph_row_idx)\n",
    "\n",
    "    # 如果传入的是 select 后的 img_ds 数据集，那么 img_id 与 img_row_idx 不一定是一一对应的\n",
    "    # data_key: test#89\n",
    "    imgId_2_imgRowIdx = {}\n",
    "    for img_row_idx, img_data_key in enumerate(img_dataset[\"data_key\"]):\n",
    "        _, img_id = img_data_key.split(\"#\")  # data_key = test#89\n",
    "        imgId_2_imgRowIdx[int(img_id)] = int(img_row_idx)\n",
    "\n",
    "    # 以数量较少的数据集为基准\n",
    "    img_ids_in_img_ds = set(imgId_2_imgRowIdx.keys())\n",
    "    img_ids_in_graph_ds = set(imgId_2_graphRowIdx.keys())\n",
    "    intersection_ids = img_ids_in_img_ds.intersection(img_ids_in_graph_ds)\n",
    "\n",
    "    # 按照 img_id 的顺序，将 img_ds 的数据拼接到 graph_ds 的数据中\n",
    "    filtered_img_ds = img_dataset.select([imgId_2_imgRowIdx[img_id] for img_id in intersection_ids])\n",
    "    filtered_graph_ds = graph_dataset.select([imgId_2_graphRowIdx[img_id] for img_id in intersection_ids])\n",
    "    merged_ds = concatenate_datasets([filtered_img_ds, filtered_graph_ds], axis=1)\n",
    "    return merged_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {'output_name': '7_disease_features_pretrain_42obs_without_text_111_10-4', 'output_dir': {'result': '/scratch/c.c21051562/workspace/arrg_img2text/outputs/results/7_disease_features_pretrain_42obs_without_text_111_10-4', 'model': '/scratch/c.c21051562/workspace/arrg_img2text/outputs/models/7_disease_features_pretrain_42obs_without_text_111_10-4', 'checkpoint': '/scratch/c.c21051562/workspace/arrg_img2text/outputs/checkpoints/7_disease_features_pretrain_42obs_without_text_111_10-4'}, 'data_path': {'mimic': '/scratch/c.c21051562/resources/data/mimic-cxr', 'interpret': '/scratch/c.c21051562/resources/data/interpret-cxr', 'interpret-test-public': '/scratch/c.c21051562/resources/data/interpret-cxr-test-public', 'text_graph': '/scratch/c.c21051562/resources/data/interpret_disease'}, 'target_section': 'findings', 'target_observation': ['effusion', 'pneumothorax', 'opacity', 'normal'], 'model_name_or_path': {'clip': '/scratch/c.c21051562/resources/downloaded_models/clip-vit-base-patch32', 'swinv2': '/scratch/c.c21051562/resources/downloaded_models/swinv2-base-patch4-window8-256', 'rad_dino_maira2': '/scratch/c.c21051562/resources/downloaded_models/rad-dino-maira-2', 'llama32_1b': '/scratch/c.c21051562/resources/downloaded_models/Llama-3.2-1B'}, 'mlflow_url': 'http://localhost:6006', 'mlflow_port': '6006', 'max_checkpoints_to_keep': 1, 'resume_from_checkpoint': False, 'use_debug_subset': False, 'run_mode': 'pretrain', 'preprocess': {'image_processor': 'rad_dino_maira2', 'text_processor': 'llama32_1b', 'cache_path': '/scratch/c.c21051562/workspace/arrg_img2text/dataset_cache/interpretcxr_full_text_img518', 'batched': True, 'batch_size': 64, 'num_proc': 16}, 'model': {'vision_model': 'rad_dino_maira2', 'language_model': 'llama32_1b', 'chat_template': '/scratch/c.c21051562/workspace/arrg_img2text/llama3_chat_template7.jinja'}, 'pretrain': {'classification_only': True, 'seed': 42, 'num_epochs': 1, 'batch_size': 1, 'grad_accum_steps': 1, 'warmup_proportion': 0.1, 'lr': 0.0001, 'clip_grad_norm': 1.0, 'mixed_precision': 'bf16', 'print_loss_per_n_steps': 200, 'ckp_per_steps': 10000, 'eval_batch_size': 1, 'max_new_tokens': 512, 'print_pred_per_n_steps': 500, 'eval_valid_split': False, 'num_beams': 3}, 'finetune': {'use_pretrained': False, 'pretain_model_path': '/scratch/c.c21051562/workspace/arrg_img2text/outputs/models/4_1_fsdo_peft_test_pretrain', 'seed': 42, 'num_epochs': 1, 'batch_size': 1, 'grad_accum_steps': 1, 'warmup_proportion': 0.1, 'lr': 0.0001, 'clip_grad_norm': 1.0, 'mixed_precision': 'bf16', 'print_loss_per_n_steps': 200, 'ckp_per_steps': 10000, 'eval_batch_size': 1, 'max_new_tokens': 512, 'print_pred_per_n_steps': 500, 'eval_valid_split': False, 'num_beams': 3}, 'obs_classification_map': ['', 'mentioned', 'absent'], 'observation_map': ['effusion', 'pneumothorax', 'opacity', 'normal', 'consolidation', 'edema', 'atelectasis', 'tube', 'clear', 'catheter', 'pneumonia', 'infiltrate', 'pathophysiologic finding', 'infection', 'congestion', 'enlargement', 'wire', 'degeneration', 'fracture', 'thickening', 'pacemaker', 'emphysema', 'surgical drain', 'surgical clip', 'medical device', 'scoliosis', 'valve', 'chronic obstructive pulmonary disease', 'calcification', 'cirrhosis-associated nodules', 'atherosclerosis', 'calcifications', 'deformity', 'hernia', 'scar', 'pulmonary nodule', 'granuloma', 'automated implantable cardiac defibrillator', 'prosthesis', 'collapse', 'reticular pattern', 'heart failure'], 'jobid': 8081282, 'classification_only': True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_img_path=CONFIG[\"preprocess\"][\"cache_path\"]\n",
    "ds_graph_path=CONFIG[\"data_path\"][\"text_graph\"]\n",
    "target_section=CONFIG[\"target_section\"]\n",
    "\n",
    "\n",
    "ds_img = load_image_datasets(data_paths=CONFIG[\"data_path\"])\n",
    "for data_split in [\"train\", \"validation\", \"test\"]:\n",
    "    img_dataset = ds_img[data_split]\n",
    "    img_dataset = img_dataset.add_column(\"data_key\", [f\"{data_split}#{idx}\" for idx in range(len(img_dataset))])\n",
    "    ds_img[data_split] = img_dataset\n",
    "\n",
    "# ds_img = load_from_disk(ds_img_path)\n",
    "\n",
    "ds_graph_path = os.path.join(ds_graph_path, f\"interpret_disease_{target_section}\")\n",
    "ds_graph = load_from_disk(ds_graph_path)\n",
    "\n",
    "ds_dict = {}\n",
    "for split in [\"train\", \"validation\", \"test\"]:\n",
    "    ds_dict[split] = merge_dataset(img_dataset=ds_img[split], graph_dataset=ds_graph[split])\n",
    "\n",
    "ds_final = DatasetDict(ds_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['source', 'images_path', 'images', 'impression', 'findings', 'data_key', 'doc_key', 'split_sents', 'radlex_types', 'radlex_to_splitsents_map'],\n",
       "        num_rows: 344394\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['source', 'images_path', 'images', 'impression', 'findings', 'data_key', 'doc_key', 'split_sents', 'radlex_types', 'radlex_to_splitsents_map'],\n",
       "        num_rows: 8839\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['images', 'findings', 'impression', 'data_key', 'doc_key', 'split_sents', 'radlex_types', 'radlex_to_splitsents_map'],\n",
       "        num_rows: 2692\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n",
      "40\n",
      "68\n",
      "69\n",
      "130\n",
      "283\n",
      "349\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mds_img\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mimages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m>\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/scratch/c.c21051562/conda/.conda/envs/arrg_img2text/lib/python3.11/site-packages/datasets/arrow_dataset.py:2390\u001b[39m, in \u001b[36mDataset.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2388\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(pa_subtable.num_rows):\n\u001b[32m   2389\u001b[39m             pa_subtable_ex = pa_subtable.slice(i, \u001b[32m1\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2390\u001b[39m             formatted_output = \u001b[43mformat_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2391\u001b[39m \u001b[43m                \u001b[49m\u001b[43mpa_subtable_ex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2392\u001b[39m \u001b[43m                \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2393\u001b[39m \u001b[43m                \u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2394\u001b[39m \u001b[43m                \u001b[49m\u001b[43mformat_columns\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_format_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2395\u001b[39m \u001b[43m                \u001b[49m\u001b[43moutput_all_columns\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_output_all_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2396\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2397\u001b[39m             \u001b[38;5;28;01myield\u001b[39;00m formatted_output\n\u001b[32m   2398\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/scratch/c.c21051562/conda/.conda/envs/arrg_img2text/lib/python3.11/site-packages/datasets/formatting/formatting.py:639\u001b[39m, in \u001b[36mformat_table\u001b[39m\u001b[34m(table, key, formatter, format_columns, output_all_columns)\u001b[39m\n\u001b[32m    637\u001b[39m python_formatter = PythonFormatter(features=formatter.features)\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m format_columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mformatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    640\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m query_type == \u001b[33m\"\u001b[39m\u001b[33mcolumn\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    641\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m format_columns:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/scratch/c.c21051562/conda/.conda/envs/arrg_img2text/lib/python3.11/site-packages/datasets/formatting/formatting.py:403\u001b[39m, in \u001b[36mFormatter.__call__\u001b[39m\u001b[34m(self, pa_table, query_type)\u001b[39m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pa_table: pa.Table, query_type: \u001b[38;5;28mstr\u001b[39m) -> Union[RowFormat, ColumnFormat, BatchFormat]:\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m query_type == \u001b[33m\"\u001b[39m\u001b[33mrow\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m403\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mformat_row\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    404\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m query_type == \u001b[33m\"\u001b[39m\u001b[33mcolumn\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    405\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.format_column(pa_table)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/scratch/c.c21051562/conda/.conda/envs/arrg_img2text/lib/python3.11/site-packages/datasets/formatting/formatting.py:444\u001b[39m, in \u001b[36mPythonFormatter.format_row\u001b[39m\u001b[34m(self, pa_table)\u001b[39m\n\u001b[32m    442\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m LazyRow(pa_table, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m    443\u001b[39m row = \u001b[38;5;28mself\u001b[39m.python_arrow_extractor().extract_row(pa_table)\n\u001b[32m--> \u001b[39m\u001b[32m444\u001b[39m row = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpython_features_decoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode_row\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    445\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m row\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/scratch/c.c21051562/conda/.conda/envs/arrg_img2text/lib/python3.11/site-packages/datasets/formatting/formatting.py:222\u001b[39m, in \u001b[36mPythonFeaturesDecoder.decode_row\u001b[39m\u001b[34m(self, row)\u001b[39m\n\u001b[32m    221\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode_row\u001b[39m(\u001b[38;5;28mself\u001b[39m, row: \u001b[38;5;28mdict\u001b[39m) -> \u001b[38;5;28mdict\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m222\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode_example\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.features \u001b[38;5;28;01melse\u001b[39;00m row\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/scratch/c.c21051562/conda/.conda/envs/arrg_img2text/lib/python3.11/site-packages/datasets/features/features.py:2045\u001b[39m, in \u001b[36mFeatures.decode_example\u001b[39m\u001b[34m(self, example, token_per_repo_id)\u001b[39m\n\u001b[32m   2031\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode_example\u001b[39m(\u001b[38;5;28mself\u001b[39m, example: \u001b[38;5;28mdict\u001b[39m, token_per_repo_id: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m]]] = \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m   2032\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Decode example with custom feature decoding.\u001b[39;00m\n\u001b[32m   2033\u001b[39m \n\u001b[32m   2034\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2042\u001b[39m \u001b[33;03m        `dict[str, Any]`\u001b[39;00m\n\u001b[32m   2043\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2045\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m{\u001b[49m\n\u001b[32m   2046\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolumn_name\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_nested_example\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_per_repo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_per_repo_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2047\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_column_requires_decoding\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcolumn_name\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m   2048\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\n\u001b[32m   2049\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcolumn_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mzip_dict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2050\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample\u001b[49m\n\u001b[32m   2051\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2052\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/scratch/c.c21051562/conda/.conda/envs/arrg_img2text/lib/python3.11/site-packages/datasets/features/features.py:2046\u001b[39m, in \u001b[36m<dictcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m   2031\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode_example\u001b[39m(\u001b[38;5;28mself\u001b[39m, example: \u001b[38;5;28mdict\u001b[39m, token_per_repo_id: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m]]] = \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m   2032\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Decode example with custom feature decoding.\u001b[39;00m\n\u001b[32m   2033\u001b[39m \n\u001b[32m   2034\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2042\u001b[39m \u001b[33;03m        `dict[str, Any]`\u001b[39;00m\n\u001b[32m   2043\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   2045\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m-> \u001b[39m\u001b[32m2046\u001b[39m         column_name: \u001b[43mdecode_nested_example\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_per_repo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_per_repo_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2047\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._column_requires_decoding[column_name]\n\u001b[32m   2048\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m value\n\u001b[32m   2049\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m column_name, (feature, value) \u001b[38;5;129;01min\u001b[39;00m zip_dict(\n\u001b[32m   2050\u001b[39m             {key: value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.items() \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m example}, example\n\u001b[32m   2051\u001b[39m         )\n\u001b[32m   2052\u001b[39m     }\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/scratch/c.c21051562/conda/.conda/envs/arrg_img2text/lib/python3.11/site-packages/datasets/features/features.py:1400\u001b[39m, in \u001b[36mdecode_nested_example\u001b[39m\u001b[34m(schema, obj, token_per_repo_id)\u001b[39m\n\u001b[32m   1398\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m {k: decode_nested_example([schema.feature[k]], obj[k]) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m schema.feature}\n\u001b[32m   1399\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1400\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdecode_nested_example\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1401\u001b[39m \u001b[38;5;66;03m# Object with special decoding:\u001b[39;00m\n\u001b[32m   1402\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(schema, \u001b[33m\"\u001b[39m\u001b[33mdecode_example\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(schema, \u001b[33m\"\u001b[39m\u001b[33mdecode\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m   1403\u001b[39m     \u001b[38;5;66;03m# we pass the token to read and decode files from private repositories in streaming mode\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/scratch/c.c21051562/conda/.conda/envs/arrg_img2text/lib/python3.11/site-packages/datasets/features/features.py:1380\u001b[39m, in \u001b[36mdecode_nested_example\u001b[39m\u001b[34m(schema, obj, token_per_repo_id)\u001b[39m\n\u001b[32m   1378\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m _check_non_null_non_empty_recursive(first_elmt, sub_schema):\n\u001b[32m   1379\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1380\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mdecode_nested_example\u001b[49m\u001b[43m(\u001b[49m\u001b[43msub_schema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst_elmt\u001b[49m\u001b[43m)\u001b[49m != first_elmt:\n\u001b[32m   1381\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m [decode_nested_example(sub_schema, o) \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m obj]\n\u001b[32m   1382\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(obj)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/scratch/c.c21051562/conda/.conda/envs/arrg_img2text/lib/python3.11/site-packages/datasets/features/features.py:1404\u001b[39m, in \u001b[36mdecode_nested_example\u001b[39m\u001b[34m(schema, obj, token_per_repo_id)\u001b[39m\n\u001b[32m   1401\u001b[39m \u001b[38;5;66;03m# Object with special decoding:\u001b[39;00m\n\u001b[32m   1402\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(schema, \u001b[33m\"\u001b[39m\u001b[33mdecode_example\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(schema, \u001b[33m\"\u001b[39m\u001b[33mdecode\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m   1403\u001b[39m     \u001b[38;5;66;03m# we pass the token to read and decode files from private repositories in streaming mode\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1404\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mschema\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode_example\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_per_repo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_per_repo_id\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1405\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/scratch/c.c21051562/conda/.conda/envs/arrg_img2text/lib/python3.11/site-packages/datasets/features/image.py:188\u001b[39m, in \u001b[36mImage.decode_example\u001b[39m\u001b[34m(self, value, token_per_repo_id)\u001b[39m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    187\u001b[39m     image = PIL.Image.open(BytesIO(bytes_))\n\u001b[32m--> \u001b[39m\u001b[32m188\u001b[39m \u001b[43mimage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# to avoid \"Too many open files\" errors\u001b[39;00m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m image.getexif().get(PIL.Image.ExifTags.Base.Orientation) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    190\u001b[39m     image = PIL.ImageOps.exif_transpose(image)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/scratch/c.c21051562/conda/.conda/envs/arrg_img2text/lib/python3.11/site-packages/PIL/ImageFile.py:300\u001b[39m, in \u001b[36mImageFile.load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    297\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[32m    299\u001b[39m b = b + s\n\u001b[32m--> \u001b[39m\u001b[32m300\u001b[39m n, err_code = \u001b[43mdecoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    301\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n < \u001b[32m0\u001b[39m:\n\u001b[32m    302\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for idx, item in enumerate(ds_img[\"train\"]):\n",
    "    if len(item[\"images\"]) > 2:\n",
    "        print(idx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arrg_img2text",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
