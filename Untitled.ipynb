{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8143331d-b0ef-4f33-be9f-6f51d865b6e3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/c.c21051562/conda/.conda/envs/arrg_img2text/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 426kB [00:00, 76.4MB/s]                                                                            \n",
      "2025-04-24 19:19:37 INFO: Downloaded file to /home/c.c21051562/stanza_resources/resources.json\n",
      "2025-04-24 19:19:37 WARNING: Language en package default expects mwt, which has been added\n",
      "2025-04-24 19:19:37 INFO: Downloading these customized packages for language: en (English)...\n",
      "===============================================\n",
      "| Processor       | Package                   |\n",
      "-----------------------------------------------\n",
      "| tokenize        | combined                  |\n",
      "| mwt             | combined                  |\n",
      "| pos             | combined_charlm           |\n",
      "| lemma           | combined_nocharlm         |\n",
      "| ner             | ontonotes-ww-multi_charlm |\n",
      "| backward_charlm | 1billion                  |\n",
      "| pretrain        | conll17                   |\n",
      "| forward_charlm  | 1billion                  |\n",
      "===============================================\n",
      "\n",
      "2025-04-24 19:19:37 INFO: File exists: /home/c.c21051562/stanza_resources/en/tokenize/combined.pt\n",
      "2025-04-24 19:19:37 INFO: File exists: /home/c.c21051562/stanza_resources/en/mwt/combined.pt\n",
      "2025-04-24 19:19:37 INFO: File exists: /home/c.c21051562/stanza_resources/en/pos/combined_charlm.pt\n",
      "2025-04-24 19:19:37 INFO: File exists: /home/c.c21051562/stanza_resources/en/lemma/combined_nocharlm.pt\n",
      "2025-04-24 19:19:37 INFO: File exists: /home/c.c21051562/stanza_resources/en/ner/ontonotes-ww-multi_charlm.pt\n",
      "2025-04-24 19:19:37 INFO: File exists: /home/c.c21051562/stanza_resources/en/backward_charlm/1billion.pt\n",
      "2025-04-24 19:19:38 INFO: File exists: /home/c.c21051562/stanza_resources/en/pretrain/conll17.pt\n",
      "2025-04-24 19:19:38 INFO: File exists: /home/c.c21051562/stanza_resources/en/forward_charlm/1billion.pt\n",
      "2025-04-24 19:19:38 INFO: Finished downloading models and saved to /home/c.c21051562/stanza_resources\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 426kB [00:00, 62.4MB/s]                                                                            \n",
      "2025-04-24 19:19:38 INFO: Downloaded file to /home/c.c21051562/stanza_resources/resources.json\n",
      "2025-04-24 19:19:38 INFO: Downloading these customized packages for language: en (English)...\n",
      "===============================\n",
      "| Processor       | Package   |\n",
      "-------------------------------\n",
      "| ner             | radiology |\n",
      "| pretrain        | mimic     |\n",
      "| forward_charlm  | mimic     |\n",
      "| backward_charlm | mimic     |\n",
      "===============================\n",
      "\n",
      "2025-04-24 19:19:38 INFO: File exists: /home/c.c21051562/stanza_resources/en/ner/radiology.pt\n",
      "2025-04-24 19:19:38 INFO: File exists: /home/c.c21051562/stanza_resources/en/pretrain/mimic.pt\n",
      "2025-04-24 19:19:38 INFO: File exists: /home/c.c21051562/stanza_resources/en/forward_charlm/mimic.pt\n",
      "2025-04-24 19:19:38 INFO: File exists: /home/c.c21051562/stanza_resources/en/backward_charlm/mimic.pt\n",
      "2025-04-24 19:19:38 INFO: Finished downloading models and saved to /home/c.c21051562/stanza_resources\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import datetime\n",
    "import gc\n",
    "import glob\n",
    "import json\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "import time\n",
    "from collections import Counter\n",
    "from dataclasses import asdict, dataclass, field\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "from safetensors.torch import load_file\n",
    "import datasets\n",
    "import imagehash\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import requests\n",
    "import torch\n",
    "import transformers\n",
    "import yaml\n",
    "from accelerate import Accelerator, DataLoaderConfiguration\n",
    "from accelerate.logging import MultiProcessAdapter\n",
    "from accelerate.utils import (\n",
    "    DistributedDataParallelKwargs,\n",
    "    FullyShardedDataParallelPlugin,\n",
    "    GradientAccumulationPlugin,\n",
    "    gather,\n",
    "    gather_object,\n",
    ")\n",
    "from datasets import DatasetDict, concatenate_datasets, load_from_disk\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftModel,\n",
    "    TaskType,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    ")\n",
    "from peft.tuners import lora\n",
    "from peft.utils import AuxiliaryTrainingWrapper\n",
    "from PIL import Image\n",
    "from scipy.ndimage import zoom\n",
    "from scorers.scores import compute_scores\n",
    "from torch import nn\n",
    "from torch.distributed.fsdp import FullyShardedDataParallel, MixedPrecision\n",
    "from torch.distributed.fsdp.wrap import size_based_auto_wrap_policy\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, Dataset, SequentialSampler\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoImageProcessor,\n",
    "    AutoTokenizer,\n",
    "    Dinov2Config,\n",
    "    Dinov2Model,\n",
    "    LlamaConfig,\n",
    "    PretrainedConfig,\n",
    "    PreTrainedModel,\n",
    "    VisionEncoderDecoderModel,\n",
    "    get_cosine_schedule_with_warmup,\n",
    ")\n",
    "from transformers.generation import GenerationConfig\n",
    "from transformers.generation import utils as tf_generation_utils\n",
    "from transformers.modeling_outputs import BaseModelOutput, ModelOutput\n",
    "from transformers.models.dinov2.modeling_dinov2 import Dinov2Embeddings\n",
    "from transformers.models.llama.modeling_llama import LlamaRMSNorm, LlamaRotaryEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02075bde-2f26-4b52-bbe4-14f44cc2bc80",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Vision2LanguageOutputWithPast(ModelOutput):\n",
    "    loss: Optional[torch.FloatTensor] = None\n",
    "    logits: torch.FloatTensor = None\n",
    "    past_key_values: Optional[List[torch.FloatTensor]] = None\n",
    "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    image_hidden_states: Optional[torch.FloatTensor] = None\n",
    "\n",
    "\n",
    "class VisionLanguageProjector(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear_1 = nn.Linear(config.encoder_hidden_size, config.decoder_hidden_size, bias=True)\n",
    "        self.act = nn.SiLU()\n",
    "        self.linear_2 = nn.Linear(config.decoder_hidden_size, config.decoder_hidden_size, bias=True)\n",
    "\n",
    "    def forward(self, image_features):\n",
    "        hidden_states = self.linear_1(image_features)\n",
    "        hidden_states = self.act(hidden_states)\n",
    "        hidden_states = self.linear_2(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class Vision2LanguageModel(VisionEncoderDecoderModel):\n",
    "    def __init__(self, config=None, encoder=None, decoder=None):\n",
    "\n",
    "        super().__init__(config=config, encoder=encoder, decoder=decoder)\n",
    "        self.config.encoder_hidden_size = self.encoder.config.hidden_size\n",
    "        self.config.decoder_hidden_size = self.decoder.config.hidden_size\n",
    "\n",
    "        # replace enc_to_dec_proj with VisionLanguageProjector\n",
    "        self.v2l_projector = VisionLanguageProjector(self.config)\n",
    "        if hasattr(self, \"enc_to_dec_proj\"):\n",
    "            del self.enc_to_dec_proj  # 移除投影层\n",
    "\n",
    "    def _inject_image_features(self, input_ids, decoder_input_ids, image_features):\n",
    "        # image_indices_map 是一个嵌套list，每个样本对应一个list，list中的元素是图像在 last_hidden_state 中的索引\n",
    "        # e.g. [[0], [1], [2, 3], ...]\n",
    "\n",
    "        # replace img features with the <|image_token|> placeholder token in the input text\n",
    "        special_image_mask = (input_ids == self.config.image_token_index).unsqueeze(-1)\n",
    "        special_image_mask = special_image_mask.expand_as(decoder_input_ids).to(decoder_input_ids.device)\n",
    "\n",
    "        # 保证所有 image_features 都能够被复制到 decoder_input_ids 中\n",
    "        assert special_image_mask.sum() == image_features.numel(), f\"special_image_mask.sum()={special_image_mask.sum()}, image_features.numel()={image_features.numel()}, should be equal to guarantee that all image features are copied to decoder_input_ids\"\n",
    "\n",
    "        image_features = image_features.to(decoder_input_ids.device, decoder_input_ids.dtype)\n",
    "        decoder_input_ids = decoder_input_ids.masked_scatter(special_image_mask, image_features)\n",
    "\n",
    "        return decoder_input_ids\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        pixel_values: Optional[torch.FloatTensor] = None,\n",
    "        decoder_input_ids: Optional[torch.LongTensor] = None,\n",
    "        decoder_attention_mask: Optional[torch.BoolTensor] = None,\n",
    "        decoder_assistant_masks: Optional[torch.Tensor] = None,\n",
    "        encoder_outputs: Optional[Tuple[torch.FloatTensor]] = None,\n",
    "        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n",
    "        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = True,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "        logits_to_keep: Union[int, torch.Tensor] = 0,\n",
    "        output_loss: Optional[bool] = False,\n",
    "        **kwargs,\n",
    "    ) -> Union[Tuple, Vision2LanguageOutputWithPast]:\n",
    "        \"\"\"Additional args:\n",
    "        `decoder_inputs_embeds`: should represent the text embeddings with image features injected.\n",
    "        `encoder_outputs`: in inference statge, we encode `pixel_values` and get `encoder_outputs` outside this forward method. This is because the `pixel_values` and `decoder_input_ids` have different batch sizes, which cause error in generate().\n",
    "\n",
    "        If `output_loss` is True, by default we use `decoder_input_ids` as `labels`.\n",
    "        And the `decoder_assistant_masks` should be provided to compute the loss.\n",
    "        `decoder_assistant_masks` is provided by `tokenizer.apply_chat_template`.\n",
    "        `decoder_assistant_masks` is a tensor with the same shape as decoder_input_ids, and the value is 0 or 1. 0: system/user tokens, 1: assistant tokens, which is the tokens that need to be generated.\n",
    "        \"\"\"\n",
    "        LOGGER.debug(\"rank[%s], kwargs %s\", ACCELERATOR.process_index, kwargs)\n",
    "        LOGGER.debug(\"rank[%s], pixel_values: %s\", ACCELERATOR.process_index, pixel_values.shape if pixel_values is not None else None)\n",
    "        LOGGER.debug(\"rank[%s], decoder_input_ids: %s\", ACCELERATOR.process_index, decoder_input_ids)\n",
    "        LOGGER.debug(\"rank[%s], decoder_attention_mask: %s\", ACCELERATOR.process_index, decoder_attention_mask.shape)\n",
    "        LOGGER.debug(\"rank[%s], encoder_outputs.last_hidden_state: %s\", ACCELERATOR.process_index, encoder_outputs.last_hidden_state.shape if encoder_outputs is not None else None)\n",
    "        LOGGER.debug(\"rank[%s], past_key_values: %s\", ACCELERATOR.process_index, past_key_values)\n",
    "        LOGGER.debug(\"rank[%s], decoder_inputs_embeds: %s\", ACCELERATOR.process_index, decoder_inputs_embeds)\n",
    "        LOGGER.debug(\"rank[%s], position_ids: %s\", ACCELERATOR.process_index, position_ids)\n",
    "        LOGGER.debug(\"rank[%s], logits_to_keep: %s\", ACCELERATOR.process_index, logits_to_keep)\n",
    "\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "\n",
    "        # train时，有pixel_values，没有encoder_outputs\n",
    "        # inference时，没有pixel_values，有encoder_outputs；encoder_outputs只有第一轮才需要，后续需要忽略\n",
    "        if (pixel_values is not None) and (encoder_outputs is not None):\n",
    "            raise ValueError(\"You must not specify both pixel_values and encoder_outputs.\")\n",
    "\n",
    "        # 我们目前没有使用过 decoder_inputs_embeds\n",
    "        if (decoder_input_ids is None) ^ (decoder_inputs_embeds is not None):\n",
    "            raise ValueError(\"You must specify exactly one of decoder_input_ids or decoder_inputs_embeds\")\n",
    "\n",
    "        if (pixel_values is not None or encoder_outputs is not None) and decoder_inputs_embeds is not None:\n",
    "            raise ValueError(\"You cannot specify both `pixel_values`/`encoder_outputs` and `decoder_inputs_embeds` at the same time, and must specify either one\")\n",
    "\n",
    "        if decoder_inputs_embeds is None:\n",
    "            # get text embeddings\n",
    "            decoder_inputs_embeds = self.decoder.get_input_embeddings()(decoder_input_ids)\n",
    "\n",
    "        # 如果有encoder_outputs，就不需要再次 encode pixel_values\n",
    "        if (pixel_values is not None) and (encoder_outputs is None):\n",
    "            # get img features\n",
    "            encoder_outputs = self.encoder(pixel_values=pixel_values, return_dict=True)\n",
    "\n",
    "        # train forward 以及 inference first round，需要进行这一步\n",
    "        # train forward 会提供 pixel_values\n",
    "        # inference all rounds 会提供 encoder_outputs，而pixel_values=None；在first round时，past_key_values=None，后续为past_key_values=DynamicCache()\n",
    "        if encoder_outputs is not None and past_key_values is None:\n",
    "            image_features = encoder_outputs.last_hidden_state  # torch.Size([4, 1370, enc_dim])\n",
    "            # project image features\n",
    "            LOGGER.debug(\"rank[%s], v2lmodel forward image_features shape: %s\", ACCELERATOR.process_index, image_features.shape)\n",
    "            image_features = self.v2l_projector(image_features)\n",
    "            # inject image features into text embeddings\n",
    "            decoder_inputs_embeds = self._inject_image_features(decoder_input_ids, decoder_inputs_embeds, image_features)\n",
    "\n",
    "        # Text generation. decoder_inputs_embeds is used in replace of decoder_input_ids on decoder in all cases.\n",
    "        # In train statge, decoder_input_ids is encoded into decoder_inputs_embeds and then merged with image features.\n",
    "        # In inference stage, encoder_outputs is passed from generate() in replace of pixel_values.\n",
    "        decoder_outputs = self.decoder(\n",
    "            attention_mask=decoder_attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=decoder_inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=True,\n",
    "            cache_position=cache_position,\n",
    "            logits_to_keep=logits_to_keep,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        logits = decoder_outputs.logits\n",
    "\n",
    "        # text loss\n",
    "        loss = None\n",
    "        if output_loss:\n",
    "            labels = labels if labels is not None else decoder_input_ids\n",
    "\n",
    "            # Shift so that tokens < n predict n\n",
    "            if decoder_assistant_masks is not None:\n",
    "                shift_label_mask = decoder_assistant_masks[:, 1:]  # torch.Size([bsz, seq_len - 1])\n",
    "            elif decoder_attention_mask is not None:\n",
    "                shift_label_mask = decoder_attention_mask[:, 1:]\n",
    "            else:\n",
    "                raise ValueError(\"decoder_assistant_masks or decoder_attention_mask should be provided\")\n",
    "\n",
    "            shift_logits = logits[:, :-1, :]  # torch.Size([bsz, seq_len - 1, vocab_size])\n",
    "            shift_labels = labels[:, 1:]  # torch.Size([bsz, seq_len - 1])\n",
    "            active_shift_logits = shift_logits[shift_label_mask != 0].contiguous()  # torch.Size([num_acitve_labels, vocab_size])\n",
    "            active_shift_labels = shift_labels[shift_label_mask != 0].contiguous()  # torch.Size([num_acitve_labels])\n",
    "\n",
    "            ce_loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = ce_loss_fct(active_shift_logits, active_shift_labels)\n",
    "\n",
    "        return Vision2LanguageOutputWithPast(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            past_key_values=decoder_outputs.past_key_values,\n",
    "            hidden_states=decoder_outputs.hidden_states,\n",
    "            attentions=decoder_outputs.attentions,\n",
    "            image_hidden_states=image_features if pixel_values is not None else None,\n",
    "        )\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(\n",
    "        self,\n",
    "        inputs,\n",
    "        generation_config=None,\n",
    "        logits_processor=None,\n",
    "        stopping_criteria=None,\n",
    "        prefix_allowed_tokens_fn=None,\n",
    "        synced_gpus=None,\n",
    "        assistant_model=None,\n",
    "        streamer=None,\n",
    "        negative_prompt_ids=None,\n",
    "        negative_prompt_attention_mask=None,\n",
    "        **kwargs,  # If the model is an encoder-decoder model, encoder specific kwargs should not be prefixed and decoder specific kwargs should be prefixed with decoder_.\n",
    "    ):\n",
    "        # 1. Handle `generation_config` and kwargs that might update it, and validate the `.generate()` call\n",
    "        self._validate_model_class()\n",
    "        tokenizer = kwargs.pop(\"tokenizer\", None)  # Pull this out first, we only use it for stopping criteria\n",
    "        assistant_tokenizer = kwargs.pop(\"assistant_tokenizer\", None)  # only used for assisted generation\n",
    "        LOGGER.debug(\"rank[%s], step1\", ACCELERATOR.process_index)\n",
    "        LOGGER.debug(\"rank[%s], tokenizer %s\", ACCELERATOR.process_index, tokenizer)\n",
    "        LOGGER.debug(\"rank[%s], assistant_tokenizer: %s\", ACCELERATOR.process_index, assistant_tokenizer)\n",
    "\n",
    "        generation_config, model_kwargs = self._prepare_generation_config(generation_config, **kwargs)\n",
    "        self._validate_model_kwargs(model_kwargs.copy())\n",
    "        self._validate_assistant(assistant_model, tokenizer, assistant_tokenizer)\n",
    "        LOGGER.debug(\"rank[%s], generation_config: %s\", ACCELERATOR.process_index, generation_config)\n",
    "        LOGGER.debug(\"rank[%s], model_kwargs step1: %s\", ACCELERATOR.process_index, model_kwargs)\n",
    "        LOGGER.debug(\"rank[%s], decoder_input_ids: %s\", ACCELERATOR.process_index, model_kwargs[\"decoder_input_ids\"].shape)\n",
    "        LOGGER.debug(\"rank[%s], decoder_attention_mask: %s\", ACCELERATOR.process_index, model_kwargs[\"decoder_attention_mask\"].shape)\n",
    "\n",
    "        # 2. Set generation parameters if not already defined\n",
    "        if synced_gpus is None:\n",
    "            synced_gpus = (tf_generation_utils.is_deepspeed_zero3_enabled() or tf_generation_utils.is_fsdp_managed_module(self)) and tf_generation_utils.dist.get_world_size() > 1\n",
    "        LOGGER.debug(\"rank[%s], step2\", ACCELERATOR.process_index)\n",
    "        LOGGER.debug(\"rank[%s], synced_gpus: %s (should be True)\", ACCELERATOR.process_index, synced_gpus, main_process_only=False)\n",
    "\n",
    "        logits_processor = logits_processor if logits_processor is not None else tf_generation_utils.LogitsProcessorList()\n",
    "        stopping_criteria = stopping_criteria if stopping_criteria is not None else tf_generation_utils.StoppingCriteriaList()\n",
    "        LOGGER.debug(\"rank[%s], logits_processor: %s\", ACCELERATOR.process_index, logits_processor)\n",
    "        LOGGER.debug(\"rank[%s], stopping_criteria: %s\", ACCELERATOR.process_index, stopping_criteria)\n",
    "\n",
    "        accepts_attention_mask = \"attention_mask\" in set(tf_generation_utils.inspect.signature(self.forward).parameters.keys())\n",
    "        requires_attention_mask = \"encoder_outputs\" not in model_kwargs\n",
    "        kwargs_has_attention_mask = model_kwargs.get(\"attention_mask\", None) is not None\n",
    "        LOGGER.debug(\"rank[%s], accepts_attention_mask: %s\", ACCELERATOR.process_index, accepts_attention_mask)\n",
    "        LOGGER.debug(\"rank[%s], requires_attention_mask: %s\", ACCELERATOR.process_index, requires_attention_mask)\n",
    "        LOGGER.debug(\"rank[%s], kwargs_has_attention_mask: %s\", ACCELERATOR.process_index, kwargs_has_attention_mask)\n",
    "\n",
    "        # 3. Define model inputs\n",
    "        inputs_tensor, model_input_name, model_kwargs = self._prepare_model_inputs(inputs, generation_config.bos_token_id, model_kwargs)\n",
    "        # batch_size = inputs_tensor.shape[0]\n",
    "        # encoder和decoder的bsz可能不一样，我们以decoder的bsz为准\n",
    "        batch_size = model_kwargs[\"decoder_input_ids\"].shape[0]\n",
    "        LOGGER.debug(\"rank[%s], step3\", ACCELERATOR.process_index)\n",
    "        LOGGER.debug(\"rank[%s], inputs_tensor: %s\", ACCELERATOR.process_index, inputs_tensor.shape)\n",
    "        LOGGER.debug(\"rank[%s], model_input_name: %s\", ACCELERATOR.process_index, model_input_name)\n",
    "        LOGGER.debug(\"rank[%s], model_kwargs step3: %s\", ACCELERATOR.process_index, model_kwargs)\n",
    "        LOGGER.debug(\"rank[%s], batch_size: %s\", ACCELERATOR.process_index, batch_size)\n",
    "\n",
    "        device = inputs_tensor.device\n",
    "        self._prepare_special_tokens(generation_config, kwargs_has_attention_mask, device=device)\n",
    "\n",
    "        # decoder-only models must use left-padding for batched generation.\n",
    "        LOGGER.debug(\"rank[%s], self.config.is_encoder_decoder %s\", ACCELERATOR.process_index, self.config.is_encoder_decoder)\n",
    "        if not self.config.is_encoder_decoder and not tf_generation_utils.is_torchdynamo_compiling():\n",
    "            # If `input_ids` was given, check if the last id in any sequence is `pad_token_id`\n",
    "            # Note: If using, `inputs_embeds` this check does not work, because we want to be more hands-off.\n",
    "            LOGGER.warning(\"Should not see this warning!!! A decoder-only architecture is detected, while we are using encoder-decoder model.\")\n",
    "            if generation_config._pad_token_tensor is not None and batch_size > 1 and len(inputs_tensor.shape) == 2 and torch.sum(inputs_tensor[:, -1] == generation_config._pad_token_tensor) > 0:\n",
    "                LOGGER.warning(\"A decoder-only architecture is being used, but right-padding was detected! For correct \" \"generation results, please set `padding_side='left'` when initializing the tokenizer.\")\n",
    "\n",
    "        # 4. Define other model kwargs\n",
    "        # decoder-only models with inputs_embeds forwarding must use caching (otherwise we can't detect whether we are\n",
    "        # generating the first new token or not, and we only want to use the embeddings for the first new token)\n",
    "        LOGGER.debug(\"rank[%s], step4\", ACCELERATOR.process_index)\n",
    "        LOGGER.debug(\"rank[%s], Conv2D weight shape: %s\", ACCELERATOR.process_index, self.encoder.embeddings.patch_embeddings.projection.weight.shape, main_process_only=False)\n",
    "        if not self.config.is_encoder_decoder and model_input_name == \"inputs_embeds\":\n",
    "            generation_config.use_cache = True\n",
    "\n",
    "        if not kwargs_has_attention_mask and requires_attention_mask and accepts_attention_mask:\n",
    "            model_kwargs[\"attention_mask\"] = self._prepare_attention_mask_for_generation(inputs_tensor, generation_config, model_kwargs)\n",
    "            LOGGER.debug(\"rank[%s], model_kwargs['attention_mask']: %s\", ACCELERATOR.process_index, model_kwargs[\"attention_mask\"].shape)\n",
    "        elif kwargs_has_attention_mask:\n",
    "            # TODO (joao): generalize this check with other types of inputs\n",
    "            if model_input_name == \"input_ids\" and len(model_kwargs[\"attention_mask\"].shape) > 2:\n",
    "                raise ValueError(\"`attention_mask` passed to `generate` must be 2D.\")\n",
    "\n",
    "        if self.config.is_encoder_decoder and \"encoder_outputs\" not in model_kwargs:\n",
    "            # if model is encoder decoder encoder_outputs are created and added to `model_kwargs`\n",
    "            model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(inputs_tensor, model_kwargs, model_input_name, generation_config)\n",
    "            LOGGER.debug(\"rank[%s], model_kwargs step4: %s\", ACCELERATOR.process_index, model_kwargs)\n",
    "            LOGGER.debug(\"rank[%s], model_kwargs['encoder_outputs'].last_hidden_state: %s\", ACCELERATOR.process_index, model_kwargs[\"encoder_outputs\"].last_hidden_state.shape)\n",
    "            LOGGER.debug(\"rank[%s], model_kwargs['encoder_outputs'].pooler_output: %s\", ACCELERATOR.process_index, model_kwargs[\"encoder_outputs\"].pooler_output.shape)\n",
    "\n",
    "        # 5. Prepare `input_ids` which will be used for auto-regressive generation\n",
    "        LOGGER.debug(\"rank[%s], step5\", ACCELERATOR.process_index)\n",
    "        if self.config.is_encoder_decoder:\n",
    "            LOGGER.debug(\"rank[%s], model_input_name: %s\", ACCELERATOR.process_index, model_input_name)\n",
    "            LOGGER.debug(\"rank[%s], before decoder_start_token_id: %s\", ACCELERATOR.process_index, generation_config._decoder_start_token_tensor)\n",
    "            # 原始方法，当input_ids不是以decoder_start_token_id开头时，添加decoder_start_token_id\n",
    "            # 更新后的方法，当input_ids不是以decoder_start_token_id 或 pad_token_id 开头时，添加decoder_start_token_id\n",
    "            # 因为我们在collect_fn中，会将input_ids以8的倍数填充left padding，然后紧跟着decoder_start_token_id和正文\n",
    "            input_ids, model_kwargs = self._prepare_decoder_input_ids_for_generation(\n",
    "                batch_size=batch_size,\n",
    "                model_input_name=model_input_name,\n",
    "                model_kwargs=model_kwargs,\n",
    "                decoder_start_token_id=generation_config._decoder_start_token_tensor,\n",
    "                pad_token_id=torch.tensor(generation_config.pad_token_id, device=inputs_tensor.device),\n",
    "                device=inputs_tensor.device,\n",
    "            )\n",
    "            LOGGER.debug(\"rank[%s], input_ids: %s\", ACCELERATOR.process_index, input_ids.shape)\n",
    "            LOGGER.debug(\"rank[%s], input_ids: %s\", ACCELERATOR.process_index, input_ids.tolist())\n",
    "            LOGGER.debug(\"rank[%s], model_kwargs step5: %s\", ACCELERATOR.process_index, model_kwargs)\n",
    "        else:\n",
    "            input_ids = inputs_tensor if model_input_name == \"input_ids\" else model_kwargs.pop(\"input_ids\")\n",
    "\n",
    "        if generation_config.token_healing:\n",
    "            input_ids = self.heal_tokens(input_ids, tokenizer)\n",
    "\n",
    "        if streamer is not None:\n",
    "            streamer.put(input_ids.cpu())\n",
    "\n",
    "        # 6. Prepare `max_length` depending on other stopping criteria.\n",
    "        LOGGER.debug(\"rank[%s], step6\", ACCELERATOR.process_index)\n",
    "        input_ids_length = input_ids.shape[-1]\n",
    "        has_default_max_length = kwargs.get(\"max_length\") is None and generation_config.max_length is not None\n",
    "        has_default_min_length = kwargs.get(\"min_length\") is None and generation_config.min_length is not None\n",
    "        generation_config = self._prepare_generated_length(\n",
    "            generation_config=generation_config,\n",
    "            has_default_max_length=has_default_max_length,\n",
    "            has_default_min_length=has_default_min_length,\n",
    "            model_input_name=model_input_name,\n",
    "            inputs_tensor=inputs_tensor,\n",
    "            input_ids_length=input_ids_length,\n",
    "        )\n",
    "        LOGGER.debug(\"rank[%s], input_ids_length: %s\", ACCELERATOR.process_index, input_ids_length)\n",
    "        LOGGER.debug(\"rank[%s], has_default_max_length: %s\", ACCELERATOR.process_index, has_default_max_length)\n",
    "        LOGGER.debug(\"rank[%s], has_default_min_length: %s\", ACCELERATOR.process_index, has_default_min_length)\n",
    "        LOGGER.debug(\"rank[%s], generation_config: %s\", ACCELERATOR.process_index, type(generation_config))\n",
    "\n",
    "        # If the model supports `logits_to_keep` in forward(), set it to 1 to avoid computing the whole\n",
    "        # logit matrix. This can save a lot of memory during the first forward pass. Note that assisted decoding\n",
    "        # dynamically overrides this value as it can need more than the last token logits\n",
    "        if self._supports_logits_to_keep() and \"logits_to_keep\" not in model_kwargs:\n",
    "            model_kwargs[\"logits_to_keep\"] = 1\n",
    "            LOGGER.debug(\"rank[%s], model_kwargs step6: %s\", ACCELERATOR.process_index, model_kwargs)\n",
    "\n",
    "        self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)\n",
    "\n",
    "        # 7. Prepare the cache.\n",
    "        # - `model_kwargs` may be updated in place with a cache as defined by the parameters in `generation_config`.\n",
    "        # - different models have a different cache name expected by the model (default = \"past_key_values\")\n",
    "        # - `max_length`, prepared above, is used to determine the maximum cache length\n",
    "        max_cache_length = generation_config.max_length - 1\n",
    "        if inputs_tensor.shape[1] != input_ids_length and model_input_name == \"inputs_embeds\" and not self.config.is_encoder_decoder:\n",
    "            max_cache_length += inputs_tensor.shape[1]\n",
    "        self._prepare_cache_for_generation(generation_config, model_kwargs, assistant_model, batch_size, max_cache_length, device)\n",
    "\n",
    "        # 8. determine generation mode\n",
    "        LOGGER.debug(\"rank[%s], step8\", ACCELERATOR.process_index)\n",
    "        generation_mode = generation_config.get_generation_mode(assistant_model)\n",
    "        LOGGER.debug(\"rank[%s], generation_mode %s\", ACCELERATOR.process_index, generation_mode)\n",
    "\n",
    "        if streamer is not None and (generation_config.num_beams > 1):\n",
    "            raise ValueError(\"`streamer` cannot be used with beam search (yet!). Make sure that `num_beams` is set to 1.\")\n",
    "\n",
    "        if not tf_generation_utils.is_torchdynamo_compiling() and self.device.type != input_ids.device.type:\n",
    "            tf_generation_utils.warnings.warn(\n",
    "                \"You are calling .generate() with the `input_ids` being on a device type different\" f\" than your model's device. `input_ids` is on {input_ids.device.type}, whereas the model\" f\" is on {self.device.type}. You may experience unexpected behaviors or slower generation.\" \" Please make sure that you have put `input_ids` to the\" f\" correct device by calling for example input_ids = input_ids.to('{self.device.type}') before\" \" running `.generate()`.\",\n",
    "                UserWarning,\n",
    "            )\n",
    "\n",
    "        # 9. prepare logits processors and stopping criteria\n",
    "        prepared_logits_processor = self._get_logits_processor(\n",
    "            generation_config=generation_config,\n",
    "            input_ids_seq_length=input_ids_length,\n",
    "            encoder_input_ids=inputs_tensor,\n",
    "            prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,\n",
    "            logits_processor=logits_processor,\n",
    "            device=inputs_tensor.device,\n",
    "            model_kwargs=model_kwargs,\n",
    "            negative_prompt_ids=negative_prompt_ids,\n",
    "            negative_prompt_attention_mask=negative_prompt_attention_mask,\n",
    "        )\n",
    "        prepared_stopping_criteria = self._get_stopping_criteria(generation_config=generation_config, stopping_criteria=stopping_criteria, tokenizer=tokenizer, **kwargs)\n",
    "\n",
    "        # Set model_kwargs `use_cache` so we can use it later in forward runs\n",
    "        model_kwargs[\"use_cache\"] = generation_config.use_cache\n",
    "        LOGGER.debug(\"rank[%s], model_kwargs step9: %s\", ACCELERATOR.process_index, model_kwargs)\n",
    "\n",
    "        # 10. go into different generation modes\n",
    "        result = None\n",
    "        if generation_mode == tf_generation_utils.GenerationMode.ASSISTED_GENERATION:\n",
    "            if generation_config.num_return_sequences > 1:\n",
    "                raise ValueError(\"num_return_sequences has to be 1 when doing assisted generate, \" f\"but is {generation_config.num_return_sequences}.\")\n",
    "            if batch_size > 1:\n",
    "                raise ValueError(\"assisted generate is only supported for batch_size = 1\")\n",
    "            if not model_kwargs[\"use_cache\"]:\n",
    "                raise ValueError(\"assisted generate requires `use_cache=True`\")\n",
    "            if generation_config.cache_implementation in [\"static\", \"hybrid\", \"sliding_window\"]:\n",
    "                raise ValueError(\"assisted generate is not supported with Static cache classes`\")\n",
    "            if self._is_stateful:\n",
    "                # In assisted generation we need the ability to confirm whether the model would pick certain tokens,\n",
    "                # which is not possible with stateful models (they can't reset to a previous subset of generated text)\n",
    "                raise ValueError(f\"assisted generation is not supported with stateful models, such as {self.__class__.__name__}\")\n",
    "\n",
    "            # 11. Get the candidate generator, given the parameterization\n",
    "            candidate_generator = self._get_candidate_generator(\n",
    "                generation_config=generation_config,\n",
    "                input_ids=input_ids,\n",
    "                inputs_tensor=inputs_tensor,\n",
    "                assistant_model=assistant_model,\n",
    "                logits_processor=logits_processor,\n",
    "                target_tokenizer=tokenizer,\n",
    "                assistant_tokenizer=assistant_tokenizer,\n",
    "                model_kwargs=model_kwargs,\n",
    "            )\n",
    "\n",
    "            # 12. run assisted generate\n",
    "            result = self._assisted_decoding(\n",
    "                input_ids,\n",
    "                candidate_generator=candidate_generator,\n",
    "                logits_processor=prepared_logits_processor,\n",
    "                stopping_criteria=prepared_stopping_criteria,\n",
    "                generation_config=generation_config,\n",
    "                synced_gpus=synced_gpus,\n",
    "                streamer=streamer,\n",
    "                **model_kwargs,\n",
    "            )\n",
    "        elif generation_mode == tf_generation_utils.GenerationMode.DOLA_GENERATION:\n",
    "            if self._is_stateful:\n",
    "                # DoLa decoding was not designed for stateful models, and would require some changes\n",
    "                raise ValueError(f\"dola decoding is not supported with stateful models, such as {self.__class__.__name__}\")\n",
    "            result = self._dola_decoding(\n",
    "                input_ids,\n",
    "                dola_layers=generation_config.dola_layers,\n",
    "                logits_processor=prepared_logits_processor,\n",
    "                stopping_criteria=prepared_stopping_criteria,\n",
    "                generation_config=generation_config,\n",
    "                synced_gpus=synced_gpus,\n",
    "                streamer=streamer,\n",
    "                **model_kwargs,\n",
    "            )\n",
    "\n",
    "        elif generation_mode == tf_generation_utils.GenerationMode.CONTRASTIVE_SEARCH:\n",
    "            if not model_kwargs[\"use_cache\"]:\n",
    "                raise ValueError(\"Contrastive search requires `use_cache=True`\")\n",
    "            if self._is_stateful:\n",
    "                # Just like assisted generation, we need to be able to rollback to a previous state (see comment above)\n",
    "                raise ValueError(f\"contrastive search is not supported with stateful models, such as {self.__class__.__name__}\")\n",
    "\n",
    "            result = self._contrastive_search(\n",
    "                input_ids,\n",
    "                logits_processor=prepared_logits_processor,\n",
    "                stopping_criteria=prepared_stopping_criteria,\n",
    "                generation_config=generation_config,\n",
    "                synced_gpus=synced_gpus,\n",
    "                streamer=streamer,\n",
    "                **model_kwargs,\n",
    "            )\n",
    "\n",
    "        elif generation_mode in (tf_generation_utils.GenerationMode.SAMPLE, tf_generation_utils.GenerationMode.GREEDY_SEARCH):\n",
    "            # 11. expand input_ids with `num_return_sequences` additional sequences per batch\n",
    "            input_ids, model_kwargs = self._expand_inputs_for_generation(\n",
    "                input_ids=input_ids,\n",
    "                expand_size=generation_config.num_return_sequences,\n",
    "                is_encoder_decoder=self.config.is_encoder_decoder,\n",
    "                **model_kwargs,\n",
    "            )\n",
    "\n",
    "            # 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\n",
    "            result = self._sample(\n",
    "                input_ids,\n",
    "                logits_processor=prepared_logits_processor,\n",
    "                stopping_criteria=prepared_stopping_criteria,\n",
    "                generation_config=generation_config,\n",
    "                synced_gpus=synced_gpus,\n",
    "                streamer=streamer,\n",
    "                **model_kwargs,\n",
    "            )\n",
    "\n",
    "        elif generation_mode in (tf_generation_utils.GenerationMode.BEAM_SAMPLE, tf_generation_utils.GenerationMode.BEAM_SEARCH):\n",
    "            # 11. prepare beam search scorer\n",
    "            beam_scorer = tf_generation_utils.BeamSearchScorer(\n",
    "                batch_size=batch_size,\n",
    "                num_beams=generation_config.num_beams,\n",
    "                device=inputs_tensor.device,\n",
    "                length_penalty=generation_config.length_penalty,\n",
    "                do_early_stopping=generation_config.early_stopping,\n",
    "                num_beam_hyps_to_keep=generation_config.num_return_sequences,\n",
    "                max_length=generation_config.max_length,\n",
    "            )\n",
    "\n",
    "            # 12. interleave input_ids with `num_beams` additional sequences per batch\n",
    "            input_ids, model_kwargs = self._expand_inputs_for_generation(\n",
    "                input_ids=input_ids,\n",
    "                expand_size=generation_config.num_beams,\n",
    "                is_encoder_decoder=self.config.is_encoder_decoder,\n",
    "                **model_kwargs,\n",
    "            )\n",
    "\n",
    "            # 13. run beam sample\n",
    "            result = self._beam_search(\n",
    "                input_ids,\n",
    "                beam_scorer,\n",
    "                logits_processor=prepared_logits_processor,\n",
    "                stopping_criteria=prepared_stopping_criteria,\n",
    "                generation_config=generation_config,\n",
    "                synced_gpus=synced_gpus,\n",
    "                **model_kwargs,\n",
    "            )\n",
    "\n",
    "        elif generation_mode == tf_generation_utils.GenerationMode.GROUP_BEAM_SEARCH:\n",
    "            # 11. prepare beam search scorer\n",
    "            beam_scorer = tf_generation_utils.BeamSearchScorer(\n",
    "                batch_size=batch_size,\n",
    "                num_beams=generation_config.num_beams,\n",
    "                device=inputs_tensor.device,\n",
    "                length_penalty=generation_config.length_penalty,\n",
    "                do_early_stopping=generation_config.early_stopping,\n",
    "                num_beam_hyps_to_keep=generation_config.num_return_sequences,\n",
    "                num_beam_groups=generation_config.num_beam_groups,\n",
    "                max_length=generation_config.max_length,\n",
    "            )\n",
    "            # 12. interleave input_ids with `num_beams` additional sequences per batch\n",
    "            input_ids, model_kwargs = self._expand_inputs_for_generation(\n",
    "                input_ids=input_ids,\n",
    "                expand_size=generation_config.num_beams,\n",
    "                is_encoder_decoder=self.config.is_encoder_decoder,\n",
    "                **model_kwargs,\n",
    "            )\n",
    "            # 13. run beam search\n",
    "            result = self._group_beam_search(\n",
    "                input_ids,\n",
    "                beam_scorer,\n",
    "                logits_processor=prepared_logits_processor,\n",
    "                stopping_criteria=prepared_stopping_criteria,\n",
    "                generation_config=generation_config,\n",
    "                synced_gpus=synced_gpus,\n",
    "                **model_kwargs,\n",
    "            )\n",
    "\n",
    "        elif generation_mode == tf_generation_utils.GenerationMode.CONSTRAINED_BEAM_SEARCH:\n",
    "            final_constraints = []\n",
    "            if generation_config.constraints is not None:\n",
    "                final_constraints = generation_config.constraints\n",
    "\n",
    "            if generation_config.force_words_ids is not None:\n",
    "\n",
    "                def typeerror():\n",
    "                    raise ValueError(\"`force_words_ids` has to either be a `List[List[List[int]]]` or `List[List[int]]` \" f\"of positive integers, but is {generation_config.force_words_ids}.\")\n",
    "\n",
    "                if not isinstance(generation_config.force_words_ids, list) or len(generation_config.force_words_ids) == 0:\n",
    "                    typeerror()\n",
    "\n",
    "                for word_ids in generation_config.force_words_ids:\n",
    "                    if isinstance(word_ids[0], list):\n",
    "                        if not isinstance(word_ids, list) or len(word_ids) == 0:\n",
    "                            typeerror()\n",
    "                        if any(not isinstance(token_ids, list) for token_ids in word_ids):\n",
    "                            typeerror()\n",
    "                        if any(any((not isinstance(token_id, int) or token_id < 0) for token_id in token_ids) for token_ids in word_ids):\n",
    "                            typeerror()\n",
    "\n",
    "                        constraint = tf_generation_utils.DisjunctiveConstraint(word_ids)\n",
    "                    else:\n",
    "                        if not isinstance(word_ids, list) or len(word_ids) == 0:\n",
    "                            typeerror()\n",
    "                        if any((not isinstance(token_id, int) or token_id < 0) for token_id in word_ids):\n",
    "                            typeerror()\n",
    "\n",
    "                        constraint = tf_generation_utils.PhrasalConstraint(word_ids)\n",
    "                    final_constraints.append(constraint)\n",
    "\n",
    "            # 11. prepare beam search scorer\n",
    "            constrained_beam_scorer = tf_generation_utils.ConstrainedBeamSearchScorer(\n",
    "                constraints=final_constraints,\n",
    "                batch_size=batch_size,\n",
    "                num_beams=generation_config.num_beams,\n",
    "                device=inputs_tensor.device,\n",
    "                length_penalty=generation_config.length_penalty,\n",
    "                do_early_stopping=generation_config.early_stopping,\n",
    "                num_beam_hyps_to_keep=generation_config.num_return_sequences,\n",
    "                max_length=generation_config.max_length,\n",
    "            )\n",
    "            # 12. interleave input_ids with `num_beams` additional sequences per batch\n",
    "            input_ids, model_kwargs = self._expand_inputs_for_generation(\n",
    "                input_ids=input_ids,\n",
    "                expand_size=generation_config.num_beams,\n",
    "                is_encoder_decoder=self.config.is_encoder_decoder,\n",
    "                **model_kwargs,\n",
    "            )\n",
    "            # 13. run beam search\n",
    "            result = self._constrained_beam_search(\n",
    "                input_ids,\n",
    "                constrained_beam_scorer=constrained_beam_scorer,\n",
    "                logits_processor=prepared_logits_processor,\n",
    "                stopping_criteria=prepared_stopping_criteria,\n",
    "                generation_config=generation_config,\n",
    "                synced_gpus=synced_gpus,\n",
    "                **model_kwargs,\n",
    "            )\n",
    "\n",
    "        # Convert to legacy cache format if requested\n",
    "        if generation_config.return_legacy_cache is True and not tf_generation_utils.is_torchdynamo_compiling() and hasattr(result, \"past_key_values\") and getattr(result.past_key_values, \"to_legacy_cache\") is not None:\n",
    "            result.past_key_values = result.past_key_values.to_legacy_cache()\n",
    "        return result\n",
    "\n",
    "    def _prepare_decoder_input_ids_for_generation(\n",
    "        self,\n",
    "        batch_size: int,\n",
    "        model_input_name: str,\n",
    "        model_kwargs: Dict[str, torch.Tensor],\n",
    "        decoder_start_token_id: torch.Tensor,\n",
    "        pad_token_id: torch.Tensor,\n",
    "        device: torch.device = None,\n",
    "    ) -> Tuple[torch.LongTensor, Dict[str, torch.Tensor]]:\n",
    "        \"\"\"Prepares `decoder_input_ids` for generation with encoder-decoder models\n",
    "        Update: if the first token is not decoder_start_token_id or pad_token_id, we need to prepend decoder_start_token_id. Because our input_ids are left padded to multiple of 8, and then followed by decoder_start_token_id and the real input_ids. It is done in the collate_fn.\n",
    "        \"\"\"\n",
    "        # 1. Check whether the user has defined `decoder_input_ids` manually. To facilitate in terms of input naming,\n",
    "        # we also allow the user to pass it under `input_ids`, if the encoder does not use it as the main input.\n",
    "        if model_kwargs is not None and \"decoder_input_ids\" in model_kwargs:\n",
    "            decoder_input_ids = model_kwargs.pop(\"decoder_input_ids\")\n",
    "        elif \"input_ids\" in model_kwargs and model_input_name != \"input_ids\":\n",
    "            decoder_input_ids = model_kwargs.pop(\"input_ids\")\n",
    "        else:\n",
    "            decoder_input_ids = None\n",
    "\n",
    "        # 2. `decoder_start_token_id` must have shape (batch_size, 1)\n",
    "        if device is None:\n",
    "            device = self.device\n",
    "        if decoder_start_token_id.ndim == 1:\n",
    "            if decoder_start_token_id.shape[0] != batch_size:\n",
    "                raise ValueError(f\"`decoder_start_token_id` expected to have length {batch_size} but got {decoder_start_token_id.shape[0]}\")\n",
    "            decoder_start_token_id = decoder_start_token_id.view(-1, 1)\n",
    "        else:\n",
    "            decoder_start_token_id = torch.ones((batch_size, 1), dtype=torch.long, device=device) * decoder_start_token_id\n",
    "\n",
    "        # 3. Encoder-decoder models expect the `decoder_input_ids` to start with a special token. Let's ensure that.\n",
    "        # no user input -> use decoder_start_token_id as decoder_input_ids\n",
    "        if decoder_input_ids is None:\n",
    "            decoder_input_ids = decoder_start_token_id\n",
    "        # exception: Donut checkpoints have task-specific decoder starts and don't expect a BOS token. Note that the\n",
    "        # original checkpoints can't be detected through `self.__class__.__name__.lower()`, needing custom logic.\n",
    "        # See: https://github.com/huggingface/transformers/pull/31470\n",
    "        elif \"donut\" in self.__class__.__name__.lower() or (self.config.model_type == \"vision-encoder-decoder\" and \"donut\" in self.config.encoder.model_type.lower()):\n",
    "            pass\n",
    "        elif self.config.model_type in [\"whisper\"]:\n",
    "            pass\n",
    "        # user input but doesn't start with decoder_start_token_id -> prepend decoder_start_token_id (and adjust\n",
    "        # decoder_attention_mask if provided)\n",
    "        #######################################\n",
    "        # !!! Update: if the first token is not decoder_start_token_id or pad_token_id, we need to prepend decoder_start_token_id\n",
    "        #######################################\n",
    "        elif ((decoder_input_ids[:, 0] != decoder_start_token_id[:, 0]) & (decoder_input_ids[:, 0] != pad_token_id)).all().item():\n",
    "            decoder_input_ids = torch.cat([decoder_start_token_id, decoder_input_ids], dim=-1)\n",
    "            if \"decoder_attention_mask\" in model_kwargs:\n",
    "                decoder_attention_mask = model_kwargs[\"decoder_attention_mask\"]\n",
    "                decoder_attention_mask = torch.cat(\n",
    "                    (torch.ones_like(decoder_attention_mask)[:, :1], decoder_attention_mask),\n",
    "                    dim=-1,\n",
    "                )\n",
    "                model_kwargs[\"decoder_attention_mask\"] = decoder_attention_mask\n",
    "\n",
    "        return decoder_input_ids, model_kwargs\n",
    "\n",
    "\n",
    "class ImageTextDataset(Dataset):\n",
    "    def __init__(self, hf_dataset, img_processor, tokenizer, split):\n",
    "        # column_names: ['source', 'images_path', 'images', 'section_text', 'doc_key', 'split_sents', 'split_sent_toks', 'sent_idx_split_idx', 'radlex', 'cxrgraph_ent', 'cxrgraph_attr', 'cxrgraph_rel']\n",
    "        self.split = split\n",
    "        self.src_path = os.path.dirname(hf_dataset.cache_files[0][\"filename\"]) if hf_dataset.cache_files else \"\"\n",
    "        self.img_processor = img_processor\n",
    "        self.tokenizer = tokenizer\n",
    "        self.samples = hf_dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    # 返回索引的数据与标签\n",
    "    def __getitem__(self, index):\n",
    "        return self.samples[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e3dce2-5ca7-47f0-b51a-01e6d2845565",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "CONFIG = None\n",
    "LOGGER = logging.getLogger(\"root\")\n",
    "TENSORBOARD = None\n",
    "DEVICE = None\n",
    "ACCELERATOR = None\n",
    "STATUS_INFO = None\n",
    "MLFLOW_TRACKER = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GlobalVariables:\n",
    "    \"\"\"\n",
    "    Some global variables for the script.\n",
    "    \"\"\"\n",
    "\n",
    "    peak_mem = 0\n",
    "    num_image_tokens = 0\n",
    "    additional_special_tokens = [\"<|image_token|>\", \"<|image_start|>\", \"<|image_end|>\"]\n",
    "\n",
    "\n",
    "GLOBAL_VARS = GlobalVariables()\n",
    "\n",
    "\n",
    "#############################################\n",
    "# Model Classes\n",
    "#############################################\n",
    "@dataclass\n",
    "class Vision2LanguageOutputWithPast(ModelOutput):\n",
    "    loss: Optional[torch.FloatTensor] = None\n",
    "    logits: torch.FloatTensor = None\n",
    "    past_key_values: Optional[List[torch.FloatTensor]] = None\n",
    "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    image_hidden_states: Optional[torch.FloatTensor] = None\n",
    "\n",
    "\n",
    "class VisionLanguageProjector(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear_1 = nn.Linear(config.encoder_hidden_size, config.decoder_hidden_size, bias=True)\n",
    "        self.act = nn.SiLU()\n",
    "        self.linear_2 = nn.Linear(config.decoder_hidden_size, config.decoder_hidden_size, bias=True)\n",
    "\n",
    "    def forward(self, image_features):\n",
    "        hidden_states = self.linear_1(image_features)\n",
    "        hidden_states = self.act(hidden_states)\n",
    "        hidden_states = self.linear_2(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class Vision2LanguageModel(VisionEncoderDecoderModel):\n",
    "    def __init__(self, config=None, encoder=None, decoder=None):\n",
    "\n",
    "        super().__init__(config=config, encoder=encoder, decoder=decoder)\n",
    "        self.config.encoder_hidden_size = self.encoder.config.hidden_size\n",
    "        self.config.decoder_hidden_size = self.decoder.config.hidden_size\n",
    "\n",
    "        # replace enc_to_dec_proj with VisionLanguageProjector\n",
    "        self.v2l_projector = VisionLanguageProjector(self.config)\n",
    "        if hasattr(self, \"enc_to_dec_proj\"):\n",
    "            del self.enc_to_dec_proj  # 移除投影层\n",
    "\n",
    "    def _inject_image_features(self, input_ids, decoder_input_ids, image_features):\n",
    "        # image_indices_map 是一个嵌套list，每个样本对应一个list，list中的元素是图像在 last_hidden_state 中的索引\n",
    "        # e.g. [[0], [1], [2, 3], ...]\n",
    "\n",
    "        # replace img features with the <|image_token|> placeholder token in the input text\n",
    "        special_image_mask = (input_ids == self.config.image_token_index).unsqueeze(-1)\n",
    "        special_image_mask = special_image_mask.expand_as(decoder_input_ids).to(decoder_input_ids.device)\n",
    "\n",
    "        # 保证所有 image_features 都能够被复制到 decoder_input_ids 中\n",
    "        assert special_image_mask.sum() == image_features.numel(), f\"special_image_mask.sum()={special_image_mask.sum()}, image_features.numel()={image_features.numel()}, should be equal to guarantee that all image features are copied to decoder_input_ids\"\n",
    "\n",
    "        image_features = image_features.to(decoder_input_ids.device, decoder_input_ids.dtype)\n",
    "        decoder_input_ids = decoder_input_ids.masked_scatter(special_image_mask, image_features)\n",
    "\n",
    "        return decoder_input_ids\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        pixel_values: Optional[torch.FloatTensor] = None,\n",
    "        decoder_input_ids: Optional[torch.LongTensor] = None,\n",
    "        decoder_attention_mask: Optional[torch.BoolTensor] = None,\n",
    "        decoder_assistant_masks: Optional[torch.Tensor] = None,\n",
    "        encoder_outputs: Optional[Tuple[torch.FloatTensor]] = None,\n",
    "        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n",
    "        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = True,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "        logits_to_keep: Union[int, torch.Tensor] = 0,\n",
    "        output_loss: Optional[bool] = False,\n",
    "        **kwargs,\n",
    "    ) -> Union[Tuple, Vision2LanguageOutputWithPast]:\n",
    "        \"\"\"Additional args:\n",
    "        `decoder_inputs_embeds`: should represent the text embeddings with image features injected.\n",
    "        `encoder_outputs`: in inference statge, we encode `pixel_values` and get `encoder_outputs` outside this forward method. This is because the `pixel_values` and `decoder_input_ids` have different batch sizes, which cause error in generate().\n",
    "\n",
    "        If `output_loss` is True, by default we use `decoder_input_ids` as `labels`.\n",
    "        And the `decoder_assistant_masks` should be provided to compute the loss.\n",
    "        `decoder_assistant_masks` is provided by `tokenizer.apply_chat_template`.\n",
    "        `decoder_assistant_masks` is a tensor with the same shape as decoder_input_ids, and the value is 0 or 1. 0: system/user tokens, 1: assistant tokens, which is the tokens that need to be generated.\n",
    "        \"\"\"\n",
    "        LOGGER.debug(\"rank[%s], kwargs %s\", ACCELERATOR.process_index, kwargs)\n",
    "        LOGGER.debug(\"rank[%s], pixel_values: %s\", ACCELERATOR.process_index, pixel_values.shape if pixel_values is not None else None)\n",
    "        LOGGER.debug(\"rank[%s], decoder_input_ids: %s\", ACCELERATOR.process_index, decoder_input_ids)\n",
    "        LOGGER.debug(\"rank[%s], decoder_attention_mask: %s\", ACCELERATOR.process_index, decoder_attention_mask.shape)\n",
    "        LOGGER.debug(\"rank[%s], encoder_outputs.last_hidden_state: %s\", ACCELERATOR.process_index, encoder_outputs.last_hidden_state.shape if encoder_outputs is not None else None)\n",
    "        LOGGER.debug(\"rank[%s], past_key_values: %s\", ACCELERATOR.process_index, past_key_values)\n",
    "        LOGGER.debug(\"rank[%s], decoder_inputs_embeds: %s\", ACCELERATOR.process_index, decoder_inputs_embeds)\n",
    "        LOGGER.debug(\"rank[%s], position_ids: %s\", ACCELERATOR.process_index, position_ids)\n",
    "        LOGGER.debug(\"rank[%s], logits_to_keep: %s\", ACCELERATOR.process_index, logits_to_keep)\n",
    "\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "\n",
    "        # train时，有pixel_values，没有encoder_outputs\n",
    "        # inference时，没有pixel_values，有encoder_outputs；encoder_outputs只有第一轮才需要，后续需要忽略\n",
    "        if (pixel_values is not None) and (encoder_outputs is not None):\n",
    "            raise ValueError(\"You must not specify both pixel_values and encoder_outputs.\")\n",
    "\n",
    "        # 我们目前没有使用过 decoder_inputs_embeds\n",
    "        if (decoder_input_ids is None) ^ (decoder_inputs_embeds is not None):\n",
    "            raise ValueError(\"You must specify exactly one of decoder_input_ids or decoder_inputs_embeds\")\n",
    "\n",
    "        if (pixel_values is not None or encoder_outputs is not None) and decoder_inputs_embeds is not None:\n",
    "            raise ValueError(\"You cannot specify both `pixel_values`/`encoder_outputs` and `decoder_inputs_embeds` at the same time, and must specify either one\")\n",
    "\n",
    "        if decoder_inputs_embeds is None:\n",
    "            # get text embeddings\n",
    "            decoder_inputs_embeds = self.decoder.get_input_embeddings()(decoder_input_ids)\n",
    "\n",
    "        # 如果有encoder_outputs，就不需要再次 encode pixel_values\n",
    "        if (pixel_values is not None) and (encoder_outputs is None):\n",
    "            # get img features\n",
    "            encoder_outputs = self.encoder(pixel_values=pixel_values, return_dict=True)\n",
    "\n",
    "        # train forward 以及 inference first round，需要进行这一步\n",
    "        # train forward 会提供 pixel_values\n",
    "        # inference all rounds 会提供 encoder_outputs，而pixel_values=None；在first round时，past_key_values=None，后续为past_key_values=DynamicCache()\n",
    "        if encoder_outputs is not None and past_key_values is None:\n",
    "            image_features = encoder_outputs.last_hidden_state  # torch.Size([4, 1370, enc_dim])\n",
    "            # project image features\n",
    "            LOGGER.debug(\"rank[%s], v2lmodel forward image_features shape: %s\", ACCELERATOR.process_index, image_features.shape)\n",
    "            image_features = self.v2l_projector(image_features)\n",
    "            # inject image features into text embeddings\n",
    "            decoder_inputs_embeds = self._inject_image_features(decoder_input_ids, decoder_inputs_embeds, image_features)\n",
    "\n",
    "        # Text generation. decoder_inputs_embeds is used in replace of decoder_input_ids on decoder in all cases.\n",
    "        # In train statge, decoder_input_ids is encoded into decoder_inputs_embeds and then merged with image features.\n",
    "        # In inference stage, encoder_outputs is passed from generate() in replace of pixel_values.\n",
    "        decoder_outputs = self.decoder(\n",
    "            attention_mask=decoder_attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=decoder_inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=True,\n",
    "            cache_position=cache_position,\n",
    "            logits_to_keep=logits_to_keep,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        logits = decoder_outputs.logits\n",
    "\n",
    "        # text loss\n",
    "        loss = None\n",
    "        if output_loss:\n",
    "            labels = labels if labels is not None else decoder_input_ids\n",
    "\n",
    "            # Shift so that tokens < n predict n\n",
    "            if decoder_assistant_masks is not None:\n",
    "                shift_label_mask = decoder_assistant_masks[:, 1:]  # torch.Size([bsz, seq_len - 1])\n",
    "            elif decoder_attention_mask is not None:\n",
    "                shift_label_mask = decoder_attention_mask[:, 1:]\n",
    "            else:\n",
    "                raise ValueError(\"decoder_assistant_masks or decoder_attention_mask should be provided\")\n",
    "\n",
    "            shift_logits = logits[:, :-1, :]  # torch.Size([bsz, seq_len - 1, vocab_size])\n",
    "            shift_labels = labels[:, 1:]  # torch.Size([bsz, seq_len - 1])\n",
    "            active_shift_logits = shift_logits[shift_label_mask != 0].contiguous()  # torch.Size([num_acitve_labels, vocab_size])\n",
    "            active_shift_labels = shift_labels[shift_label_mask != 0].contiguous()  # torch.Size([num_acitve_labels])\n",
    "\n",
    "            ce_loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = ce_loss_fct(active_shift_logits, active_shift_labels)\n",
    "\n",
    "        return Vision2LanguageOutputWithPast(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            past_key_values=decoder_outputs.past_key_values,\n",
    "            hidden_states=decoder_outputs.hidden_states,\n",
    "            attentions=decoder_outputs.attentions,\n",
    "            image_hidden_states=image_features if pixel_values is not None else None,\n",
    "        )\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(\n",
    "        self,\n",
    "        inputs,\n",
    "        generation_config=None,\n",
    "        logits_processor=None,\n",
    "        stopping_criteria=None,\n",
    "        prefix_allowed_tokens_fn=None,\n",
    "        synced_gpus=None,\n",
    "        assistant_model=None,\n",
    "        streamer=None,\n",
    "        negative_prompt_ids=None,\n",
    "        negative_prompt_attention_mask=None,\n",
    "        **kwargs,  # If the model is an encoder-decoder model, encoder specific kwargs should not be prefixed and decoder specific kwargs should be prefixed with decoder_.\n",
    "    ):\n",
    "        # 1. Handle `generation_config` and kwargs that might update it, and validate the `.generate()` call\n",
    "        self._validate_model_class()\n",
    "        tokenizer = kwargs.pop(\"tokenizer\", None)  # Pull this out first, we only use it for stopping criteria\n",
    "        assistant_tokenizer = kwargs.pop(\"assistant_tokenizer\", None)  # only used for assisted generation\n",
    "        LOGGER.debug(\"rank[%s], step1\", ACCELERATOR.process_index)\n",
    "        LOGGER.debug(\"rank[%s], tokenizer %s\", ACCELERATOR.process_index, tokenizer)\n",
    "        LOGGER.debug(\"rank[%s], assistant_tokenizer: %s\", ACCELERATOR.process_index, assistant_tokenizer)\n",
    "\n",
    "        generation_config, model_kwargs = self._prepare_generation_config(generation_config, **kwargs)\n",
    "        self._validate_model_kwargs(model_kwargs.copy())\n",
    "        self._validate_assistant(assistant_model, tokenizer, assistant_tokenizer)\n",
    "        LOGGER.debug(\"rank[%s], generation_config: %s\", ACCELERATOR.process_index, generation_config)\n",
    "        LOGGER.debug(\"rank[%s], model_kwargs step1: %s\", ACCELERATOR.process_index, model_kwargs)\n",
    "        LOGGER.debug(\"rank[%s], decoder_input_ids: %s\", ACCELERATOR.process_index, model_kwargs[\"decoder_input_ids\"].shape)\n",
    "        LOGGER.debug(\"rank[%s], decoder_attention_mask: %s\", ACCELERATOR.process_index, model_kwargs[\"decoder_attention_mask\"].shape)\n",
    "\n",
    "        # 2. Set generation parameters if not already defined\n",
    "        if synced_gpus is None:\n",
    "            synced_gpus = (tf_generation_utils.is_deepspeed_zero3_enabled() or tf_generation_utils.is_fsdp_managed_module(self)) and tf_generation_utils.dist.get_world_size() > 1\n",
    "        LOGGER.debug(\"rank[%s], step2\", ACCELERATOR.process_index)\n",
    "        LOGGER.debug(\"rank[%s], synced_gpus: %s (should be True)\", ACCELERATOR.process_index, synced_gpus, main_process_only=False)\n",
    "\n",
    "        logits_processor = logits_processor if logits_processor is not None else tf_generation_utils.LogitsProcessorList()\n",
    "        stopping_criteria = stopping_criteria if stopping_criteria is not None else tf_generation_utils.StoppingCriteriaList()\n",
    "        LOGGER.debug(\"rank[%s], logits_processor: %s\", ACCELERATOR.process_index, logits_processor)\n",
    "        LOGGER.debug(\"rank[%s], stopping_criteria: %s\", ACCELERATOR.process_index, stopping_criteria)\n",
    "\n",
    "        accepts_attention_mask = \"attention_mask\" in set(tf_generation_utils.inspect.signature(self.forward).parameters.keys())\n",
    "        requires_attention_mask = \"encoder_outputs\" not in model_kwargs\n",
    "        kwargs_has_attention_mask = model_kwargs.get(\"attention_mask\", None) is not None\n",
    "        LOGGER.debug(\"rank[%s], accepts_attention_mask: %s\", ACCELERATOR.process_index, accepts_attention_mask)\n",
    "        LOGGER.debug(\"rank[%s], requires_attention_mask: %s\", ACCELERATOR.process_index, requires_attention_mask)\n",
    "        LOGGER.debug(\"rank[%s], kwargs_has_attention_mask: %s\", ACCELERATOR.process_index, kwargs_has_attention_mask)\n",
    "\n",
    "        # 3. Define model inputs\n",
    "        inputs_tensor, model_input_name, model_kwargs = self._prepare_model_inputs(inputs, generation_config.bos_token_id, model_kwargs)\n",
    "        # batch_size = inputs_tensor.shape[0]\n",
    "        # encoder和decoder的bsz可能不一样，我们以decoder的bsz为准\n",
    "        batch_size = model_kwargs[\"decoder_input_ids\"].shape[0]\n",
    "        LOGGER.debug(\"rank[%s], step3\", ACCELERATOR.process_index)\n",
    "        LOGGER.debug(\"rank[%s], inputs_tensor: %s\", ACCELERATOR.process_index, inputs_tensor.shape)\n",
    "        LOGGER.debug(\"rank[%s], model_input_name: %s\", ACCELERATOR.process_index, model_input_name)\n",
    "        LOGGER.debug(\"rank[%s], model_kwargs step3: %s\", ACCELERATOR.process_index, model_kwargs)\n",
    "        LOGGER.debug(\"rank[%s], batch_size: %s\", ACCELERATOR.process_index, batch_size)\n",
    "\n",
    "        device = inputs_tensor.device\n",
    "        self._prepare_special_tokens(generation_config, kwargs_has_attention_mask, device=device)\n",
    "\n",
    "        # decoder-only models must use left-padding for batched generation.\n",
    "        LOGGER.debug(\"rank[%s], self.config.is_encoder_decoder %s\", ACCELERATOR.process_index, self.config.is_encoder_decoder)\n",
    "        if not self.config.is_encoder_decoder and not tf_generation_utils.is_torchdynamo_compiling():\n",
    "            # If `input_ids` was given, check if the last id in any sequence is `pad_token_id`\n",
    "            # Note: If using, `inputs_embeds` this check does not work, because we want to be more hands-off.\n",
    "            LOGGER.warning(\"Should not see this warning!!! A decoder-only architecture is detected, while we are using encoder-decoder model.\")\n",
    "            if generation_config._pad_token_tensor is not None and batch_size > 1 and len(inputs_tensor.shape) == 2 and torch.sum(inputs_tensor[:, -1] == generation_config._pad_token_tensor) > 0:\n",
    "                LOGGER.warning(\"A decoder-only architecture is being used, but right-padding was detected! For correct \" \"generation results, please set `padding_side='left'` when initializing the tokenizer.\")\n",
    "\n",
    "        # 4. Define other model kwargs\n",
    "        # decoder-only models with inputs_embeds forwarding must use caching (otherwise we can't detect whether we are\n",
    "        # generating the first new token or not, and we only want to use the embeddings for the first new token)\n",
    "        LOGGER.debug(\"rank[%s], step4\", ACCELERATOR.process_index)\n",
    "        LOGGER.debug(\"rank[%s], Conv2D weight shape: %s\", ACCELERATOR.process_index, self.encoder.embeddings.patch_embeddings.projection.weight.shape, main_process_only=False)\n",
    "        if not self.config.is_encoder_decoder and model_input_name == \"inputs_embeds\":\n",
    "            generation_config.use_cache = True\n",
    "\n",
    "        if not kwargs_has_attention_mask and requires_attention_mask and accepts_attention_mask:\n",
    "            model_kwargs[\"attention_mask\"] = self._prepare_attention_mask_for_generation(inputs_tensor, generation_config, model_kwargs)\n",
    "            LOGGER.debug(\"rank[%s], model_kwargs['attention_mask']: %s\", ACCELERATOR.process_index, model_kwargs[\"attention_mask\"].shape)\n",
    "        elif kwargs_has_attention_mask:\n",
    "            # TODO (joao): generalize this check with other types of inputs\n",
    "            if model_input_name == \"input_ids\" and len(model_kwargs[\"attention_mask\"].shape) > 2:\n",
    "                raise ValueError(\"`attention_mask` passed to `generate` must be 2D.\")\n",
    "\n",
    "        if self.config.is_encoder_decoder and \"encoder_outputs\" not in model_kwargs:\n",
    "            # if model is encoder decoder encoder_outputs are created and added to `model_kwargs`\n",
    "            model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(inputs_tensor, model_kwargs, model_input_name, generation_config)\n",
    "            LOGGER.debug(\"rank[%s], model_kwargs step4: %s\", ACCELERATOR.process_index, model_kwargs)\n",
    "            LOGGER.debug(\"rank[%s], model_kwargs['encoder_outputs'].last_hidden_state: %s\", ACCELERATOR.process_index, model_kwargs[\"encoder_outputs\"].last_hidden_state.shape)\n",
    "            LOGGER.debug(\"rank[%s], model_kwargs['encoder_outputs'].pooler_output: %s\", ACCELERATOR.process_index, model_kwargs[\"encoder_outputs\"].pooler_output.shape)\n",
    "\n",
    "        # 5. Prepare `input_ids` which will be used for auto-regressive generation\n",
    "        LOGGER.debug(\"rank[%s], step5\", ACCELERATOR.process_index)\n",
    "        if self.config.is_encoder_decoder:\n",
    "            LOGGER.debug(\"rank[%s], model_input_name: %s\", ACCELERATOR.process_index, model_input_name)\n",
    "            LOGGER.debug(\"rank[%s], before decoder_start_token_id: %s\", ACCELERATOR.process_index, generation_config._decoder_start_token_tensor)\n",
    "            # 原始方法，当input_ids不是以decoder_start_token_id开头时，添加decoder_start_token_id\n",
    "            # 更新后的方法，当input_ids不是以decoder_start_token_id 或 pad_token_id 开头时，添加decoder_start_token_id\n",
    "            # 因为我们在collect_fn中，会将input_ids以8的倍数填充left padding，然后紧跟着decoder_start_token_id和正文\n",
    "            input_ids, model_kwargs = self._prepare_decoder_input_ids_for_generation(\n",
    "                batch_size=batch_size,\n",
    "                model_input_name=model_input_name,\n",
    "                model_kwargs=model_kwargs,\n",
    "                decoder_start_token_id=generation_config._decoder_start_token_tensor,\n",
    "                pad_token_id=torch.tensor(generation_config.pad_token_id, device=inputs_tensor.device),\n",
    "                device=inputs_tensor.device,\n",
    "            )\n",
    "            LOGGER.debug(\"rank[%s], input_ids: %s\", ACCELERATOR.process_index, input_ids.shape)\n",
    "            LOGGER.debug(\"rank[%s], input_ids: %s\", ACCELERATOR.process_index, input_ids.tolist())\n",
    "            LOGGER.debug(\"rank[%s], model_kwargs step5: %s\", ACCELERATOR.process_index, model_kwargs)\n",
    "        else:\n",
    "            input_ids = inputs_tensor if model_input_name == \"input_ids\" else model_kwargs.pop(\"input_ids\")\n",
    "\n",
    "        if generation_config.token_healing:\n",
    "            input_ids = self.heal_tokens(input_ids, tokenizer)\n",
    "\n",
    "        if streamer is not None:\n",
    "            streamer.put(input_ids.cpu())\n",
    "\n",
    "        # 6. Prepare `max_length` depending on other stopping criteria.\n",
    "        LOGGER.debug(\"rank[%s], step6\", ACCELERATOR.process_index)\n",
    "        input_ids_length = input_ids.shape[-1]\n",
    "        has_default_max_length = kwargs.get(\"max_length\") is None and generation_config.max_length is not None\n",
    "        has_default_min_length = kwargs.get(\"min_length\") is None and generation_config.min_length is not None\n",
    "        generation_config = self._prepare_generated_length(\n",
    "            generation_config=generation_config,\n",
    "            has_default_max_length=has_default_max_length,\n",
    "            has_default_min_length=has_default_min_length,\n",
    "            model_input_name=model_input_name,\n",
    "            inputs_tensor=inputs_tensor,\n",
    "            input_ids_length=input_ids_length,\n",
    "        )\n",
    "        LOGGER.debug(\"rank[%s], input_ids_length: %s\", ACCELERATOR.process_index, input_ids_length)\n",
    "        LOGGER.debug(\"rank[%s], has_default_max_length: %s\", ACCELERATOR.process_index, has_default_max_length)\n",
    "        LOGGER.debug(\"rank[%s], has_default_min_length: %s\", ACCELERATOR.process_index, has_default_min_length)\n",
    "        LOGGER.debug(\"rank[%s], generation_config: %s\", ACCELERATOR.process_index, type(generation_config))\n",
    "\n",
    "        # If the model supports `logits_to_keep` in forward(), set it to 1 to avoid computing the whole\n",
    "        # logit matrix. This can save a lot of memory during the first forward pass. Note that assisted decoding\n",
    "        # dynamically overrides this value as it can need more than the last token logits\n",
    "        if self._supports_logits_to_keep() and \"logits_to_keep\" not in model_kwargs:\n",
    "            model_kwargs[\"logits_to_keep\"] = 1\n",
    "            LOGGER.debug(\"rank[%s], model_kwargs step6: %s\", ACCELERATOR.process_index, model_kwargs)\n",
    "\n",
    "        self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)\n",
    "\n",
    "        # 7. Prepare the cache.\n",
    "        # - `model_kwargs` may be updated in place with a cache as defined by the parameters in `generation_config`.\n",
    "        # - different models have a different cache name expected by the model (default = \"past_key_values\")\n",
    "        # - `max_length`, prepared above, is used to determine the maximum cache length\n",
    "        max_cache_length = generation_config.max_length - 1\n",
    "        if inputs_tensor.shape[1] != input_ids_length and model_input_name == \"inputs_embeds\" and not self.config.is_encoder_decoder:\n",
    "            max_cache_length += inputs_tensor.shape[1]\n",
    "        self._prepare_cache_for_generation(generation_config, model_kwargs, assistant_model, batch_size, max_cache_length, device)\n",
    "\n",
    "        # 8. determine generation mode\n",
    "        LOGGER.debug(\"rank[%s], step8\", ACCELERATOR.process_index)\n",
    "        generation_mode = generation_config.get_generation_mode(assistant_model)\n",
    "        LOGGER.debug(\"rank[%s], generation_mode %s\", ACCELERATOR.process_index, generation_mode)\n",
    "\n",
    "        if streamer is not None and (generation_config.num_beams > 1):\n",
    "            raise ValueError(\"`streamer` cannot be used with beam search (yet!). Make sure that `num_beams` is set to 1.\")\n",
    "\n",
    "        if not tf_generation_utils.is_torchdynamo_compiling() and self.device.type != input_ids.device.type:\n",
    "            tf_generation_utils.warnings.warn(\n",
    "                \"You are calling .generate() with the `input_ids` being on a device type different\" f\" than your model's device. `input_ids` is on {input_ids.device.type}, whereas the model\" f\" is on {self.device.type}. You may experience unexpected behaviors or slower generation.\" \" Please make sure that you have put `input_ids` to the\" f\" correct device by calling for example input_ids = input_ids.to('{self.device.type}') before\" \" running `.generate()`.\",\n",
    "                UserWarning,\n",
    "            )\n",
    "\n",
    "        # 9. prepare logits processors and stopping criteria\n",
    "        prepared_logits_processor = self._get_logits_processor(\n",
    "            generation_config=generation_config,\n",
    "            input_ids_seq_length=input_ids_length,\n",
    "            encoder_input_ids=inputs_tensor,\n",
    "            prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,\n",
    "            logits_processor=logits_processor,\n",
    "            device=inputs_tensor.device,\n",
    "            model_kwargs=model_kwargs,\n",
    "            negative_prompt_ids=negative_prompt_ids,\n",
    "            negative_prompt_attention_mask=negative_prompt_attention_mask,\n",
    "        )\n",
    "        prepared_stopping_criteria = self._get_stopping_criteria(generation_config=generation_config, stopping_criteria=stopping_criteria, tokenizer=tokenizer, **kwargs)\n",
    "\n",
    "        # Set model_kwargs `use_cache` so we can use it later in forward runs\n",
    "        model_kwargs[\"use_cache\"] = generation_config.use_cache\n",
    "        LOGGER.debug(\"rank[%s], model_kwargs step9: %s\", ACCELERATOR.process_index, model_kwargs)\n",
    "\n",
    "        # 10. go into different generation modes\n",
    "        result = None\n",
    "        if generation_mode == tf_generation_utils.GenerationMode.ASSISTED_GENERATION:\n",
    "            if generation_config.num_return_sequences > 1:\n",
    "                raise ValueError(\"num_return_sequences has to be 1 when doing assisted generate, \" f\"but is {generation_config.num_return_sequences}.\")\n",
    "            if batch_size > 1:\n",
    "                raise ValueError(\"assisted generate is only supported for batch_size = 1\")\n",
    "            if not model_kwargs[\"use_cache\"]:\n",
    "                raise ValueError(\"assisted generate requires `use_cache=True`\")\n",
    "            if generation_config.cache_implementation in [\"static\", \"hybrid\", \"sliding_window\"]:\n",
    "                raise ValueError(\"assisted generate is not supported with Static cache classes`\")\n",
    "            if self._is_stateful:\n",
    "                # In assisted generation we need the ability to confirm whether the model would pick certain tokens,\n",
    "                # which is not possible with stateful models (they can't reset to a previous subset of generated text)\n",
    "                raise ValueError(f\"assisted generation is not supported with stateful models, such as {self.__class__.__name__}\")\n",
    "\n",
    "            # 11. Get the candidate generator, given the parameterization\n",
    "            candidate_generator = self._get_candidate_generator(\n",
    "                generation_config=generation_config,\n",
    "                input_ids=input_ids,\n",
    "                inputs_tensor=inputs_tensor,\n",
    "                assistant_model=assistant_model,\n",
    "                logits_processor=logits_processor,\n",
    "                target_tokenizer=tokenizer,\n",
    "                assistant_tokenizer=assistant_tokenizer,\n",
    "                model_kwargs=model_kwargs,\n",
    "            )\n",
    "\n",
    "            # 12. run assisted generate\n",
    "            result = self._assisted_decoding(\n",
    "                input_ids,\n",
    "                candidate_generator=candidate_generator,\n",
    "                logits_processor=prepared_logits_processor,\n",
    "                stopping_criteria=prepared_stopping_criteria,\n",
    "                generation_config=generation_config,\n",
    "                synced_gpus=synced_gpus,\n",
    "                streamer=streamer,\n",
    "                **model_kwargs,\n",
    "            )\n",
    "        elif generation_mode == tf_generation_utils.GenerationMode.DOLA_GENERATION:\n",
    "            if self._is_stateful:\n",
    "                # DoLa decoding was not designed for stateful models, and would require some changes\n",
    "                raise ValueError(f\"dola decoding is not supported with stateful models, such as {self.__class__.__name__}\")\n",
    "            result = self._dola_decoding(\n",
    "                input_ids,\n",
    "                dola_layers=generation_config.dola_layers,\n",
    "                logits_processor=prepared_logits_processor,\n",
    "                stopping_criteria=prepared_stopping_criteria,\n",
    "                generation_config=generation_config,\n",
    "                synced_gpus=synced_gpus,\n",
    "                streamer=streamer,\n",
    "                **model_kwargs,\n",
    "            )\n",
    "\n",
    "        elif generation_mode == tf_generation_utils.GenerationMode.CONTRASTIVE_SEARCH:\n",
    "            if not model_kwargs[\"use_cache\"]:\n",
    "                raise ValueError(\"Contrastive search requires `use_cache=True`\")\n",
    "            if self._is_stateful:\n",
    "                # Just like assisted generation, we need to be able to rollback to a previous state (see comment above)\n",
    "                raise ValueError(f\"contrastive search is not supported with stateful models, such as {self.__class__.__name__}\")\n",
    "\n",
    "            result = self._contrastive_search(\n",
    "                input_ids,\n",
    "                logits_processor=prepared_logits_processor,\n",
    "                stopping_criteria=prepared_stopping_criteria,\n",
    "                generation_config=generation_config,\n",
    "                synced_gpus=synced_gpus,\n",
    "                streamer=streamer,\n",
    "                **model_kwargs,\n",
    "            )\n",
    "\n",
    "        elif generation_mode in (tf_generation_utils.GenerationMode.SAMPLE, tf_generation_utils.GenerationMode.GREEDY_SEARCH):\n",
    "            # 11. expand input_ids with `num_return_sequences` additional sequences per batch\n",
    "            input_ids, model_kwargs = self._expand_inputs_for_generation(\n",
    "                input_ids=input_ids,\n",
    "                expand_size=generation_config.num_return_sequences,\n",
    "                is_encoder_decoder=self.config.is_encoder_decoder,\n",
    "                **model_kwargs,\n",
    "            )\n",
    "\n",
    "            # 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\n",
    "            result = self._sample(\n",
    "                input_ids,\n",
    "                logits_processor=prepared_logits_processor,\n",
    "                stopping_criteria=prepared_stopping_criteria,\n",
    "                generation_config=generation_config,\n",
    "                synced_gpus=synced_gpus,\n",
    "                streamer=streamer,\n",
    "                **model_kwargs,\n",
    "            )\n",
    "\n",
    "        elif generation_mode in (tf_generation_utils.GenerationMode.BEAM_SAMPLE, tf_generation_utils.GenerationMode.BEAM_SEARCH):\n",
    "            # 11. prepare beam search scorer\n",
    "            beam_scorer = tf_generation_utils.BeamSearchScorer(\n",
    "                batch_size=batch_size,\n",
    "                num_beams=generation_config.num_beams,\n",
    "                device=inputs_tensor.device,\n",
    "                length_penalty=generation_config.length_penalty,\n",
    "                do_early_stopping=generation_config.early_stopping,\n",
    "                num_beam_hyps_to_keep=generation_config.num_return_sequences,\n",
    "                max_length=generation_config.max_length,\n",
    "            )\n",
    "\n",
    "            # 12. interleave input_ids with `num_beams` additional sequences per batch\n",
    "            input_ids, model_kwargs = self._expand_inputs_for_generation(\n",
    "                input_ids=input_ids,\n",
    "                expand_size=generation_config.num_beams,\n",
    "                is_encoder_decoder=self.config.is_encoder_decoder,\n",
    "                **model_kwargs,\n",
    "            )\n",
    "\n",
    "            # 13. run beam sample\n",
    "            result = self._beam_search(\n",
    "                input_ids,\n",
    "                beam_scorer,\n",
    "                logits_processor=prepared_logits_processor,\n",
    "                stopping_criteria=prepared_stopping_criteria,\n",
    "                generation_config=generation_config,\n",
    "                synced_gpus=synced_gpus,\n",
    "                **model_kwargs,\n",
    "            )\n",
    "\n",
    "        elif generation_mode == tf_generation_utils.GenerationMode.GROUP_BEAM_SEARCH:\n",
    "            # 11. prepare beam search scorer\n",
    "            beam_scorer = tf_generation_utils.BeamSearchScorer(\n",
    "                batch_size=batch_size,\n",
    "                num_beams=generation_config.num_beams,\n",
    "                device=inputs_tensor.device,\n",
    "                length_penalty=generation_config.length_penalty,\n",
    "                do_early_stopping=generation_config.early_stopping,\n",
    "                num_beam_hyps_to_keep=generation_config.num_return_sequences,\n",
    "                num_beam_groups=generation_config.num_beam_groups,\n",
    "                max_length=generation_config.max_length,\n",
    "            )\n",
    "            # 12. interleave input_ids with `num_beams` additional sequences per batch\n",
    "            input_ids, model_kwargs = self._expand_inputs_for_generation(\n",
    "                input_ids=input_ids,\n",
    "                expand_size=generation_config.num_beams,\n",
    "                is_encoder_decoder=self.config.is_encoder_decoder,\n",
    "                **model_kwargs,\n",
    "            )\n",
    "            # 13. run beam search\n",
    "            result = self._group_beam_search(\n",
    "                input_ids,\n",
    "                beam_scorer,\n",
    "                logits_processor=prepared_logits_processor,\n",
    "                stopping_criteria=prepared_stopping_criteria,\n",
    "                generation_config=generation_config,\n",
    "                synced_gpus=synced_gpus,\n",
    "                **model_kwargs,\n",
    "            )\n",
    "\n",
    "        elif generation_mode == tf_generation_utils.GenerationMode.CONSTRAINED_BEAM_SEARCH:\n",
    "            final_constraints = []\n",
    "            if generation_config.constraints is not None:\n",
    "                final_constraints = generation_config.constraints\n",
    "\n",
    "            if generation_config.force_words_ids is not None:\n",
    "\n",
    "                def typeerror():\n",
    "                    raise ValueError(\"`force_words_ids` has to either be a `List[List[List[int]]]` or `List[List[int]]` \" f\"of positive integers, but is {generation_config.force_words_ids}.\")\n",
    "\n",
    "                if not isinstance(generation_config.force_words_ids, list) or len(generation_config.force_words_ids) == 0:\n",
    "                    typeerror()\n",
    "\n",
    "                for word_ids in generation_config.force_words_ids:\n",
    "                    if isinstance(word_ids[0], list):\n",
    "                        if not isinstance(word_ids, list) or len(word_ids) == 0:\n",
    "                            typeerror()\n",
    "                        if any(not isinstance(token_ids, list) for token_ids in word_ids):\n",
    "                            typeerror()\n",
    "                        if any(any((not isinstance(token_id, int) or token_id < 0) for token_id in token_ids) for token_ids in word_ids):\n",
    "                            typeerror()\n",
    "\n",
    "                        constraint = tf_generation_utils.DisjunctiveConstraint(word_ids)\n",
    "                    else:\n",
    "                        if not isinstance(word_ids, list) or len(word_ids) == 0:\n",
    "                            typeerror()\n",
    "                        if any((not isinstance(token_id, int) or token_id < 0) for token_id in word_ids):\n",
    "                            typeerror()\n",
    "\n",
    "                        constraint = tf_generation_utils.PhrasalConstraint(word_ids)\n",
    "                    final_constraints.append(constraint)\n",
    "\n",
    "            # 11. prepare beam search scorer\n",
    "            constrained_beam_scorer = tf_generation_utils.ConstrainedBeamSearchScorer(\n",
    "                constraints=final_constraints,\n",
    "                batch_size=batch_size,\n",
    "                num_beams=generation_config.num_beams,\n",
    "                device=inputs_tensor.device,\n",
    "                length_penalty=generation_config.length_penalty,\n",
    "                do_early_stopping=generation_config.early_stopping,\n",
    "                num_beam_hyps_to_keep=generation_config.num_return_sequences,\n",
    "                max_length=generation_config.max_length,\n",
    "            )\n",
    "            # 12. interleave input_ids with `num_beams` additional sequences per batch\n",
    "            input_ids, model_kwargs = self._expand_inputs_for_generation(\n",
    "                input_ids=input_ids,\n",
    "                expand_size=generation_config.num_beams,\n",
    "                is_encoder_decoder=self.config.is_encoder_decoder,\n",
    "                **model_kwargs,\n",
    "            )\n",
    "            # 13. run beam search\n",
    "            result = self._constrained_beam_search(\n",
    "                input_ids,\n",
    "                constrained_beam_scorer=constrained_beam_scorer,\n",
    "                logits_processor=prepared_logits_processor,\n",
    "                stopping_criteria=prepared_stopping_criteria,\n",
    "                generation_config=generation_config,\n",
    "                synced_gpus=synced_gpus,\n",
    "                **model_kwargs,\n",
    "            )\n",
    "\n",
    "        # Convert to legacy cache format if requested\n",
    "        if generation_config.return_legacy_cache is True and not tf_generation_utils.is_torchdynamo_compiling() and hasattr(result, \"past_key_values\") and getattr(result.past_key_values, \"to_legacy_cache\") is not None:\n",
    "            result.past_key_values = result.past_key_values.to_legacy_cache()\n",
    "        return result\n",
    "\n",
    "    def _prepare_decoder_input_ids_for_generation(\n",
    "        self,\n",
    "        batch_size: int,\n",
    "        model_input_name: str,\n",
    "        model_kwargs: Dict[str, torch.Tensor],\n",
    "        decoder_start_token_id: torch.Tensor,\n",
    "        pad_token_id: torch.Tensor,\n",
    "        device: torch.device = None,\n",
    "    ) -> Tuple[torch.LongTensor, Dict[str, torch.Tensor]]:\n",
    "        \"\"\"Prepares `decoder_input_ids` for generation with encoder-decoder models\n",
    "        Update: if the first token is not decoder_start_token_id or pad_token_id, we need to prepend decoder_start_token_id. Because our input_ids are left padded to multiple of 8, and then followed by decoder_start_token_id and the real input_ids. It is done in the collate_fn.\n",
    "        \"\"\"\n",
    "        # 1. Check whether the user has defined `decoder_input_ids` manually. To facilitate in terms of input naming,\n",
    "        # we also allow the user to pass it under `input_ids`, if the encoder does not use it as the main input.\n",
    "        if model_kwargs is not None and \"decoder_input_ids\" in model_kwargs:\n",
    "            decoder_input_ids = model_kwargs.pop(\"decoder_input_ids\")\n",
    "        elif \"input_ids\" in model_kwargs and model_input_name != \"input_ids\":\n",
    "            decoder_input_ids = model_kwargs.pop(\"input_ids\")\n",
    "        else:\n",
    "            decoder_input_ids = None\n",
    "\n",
    "        # 2. `decoder_start_token_id` must have shape (batch_size, 1)\n",
    "        if device is None:\n",
    "            device = self.device\n",
    "        if decoder_start_token_id.ndim == 1:\n",
    "            if decoder_start_token_id.shape[0] != batch_size:\n",
    "                raise ValueError(f\"`decoder_start_token_id` expected to have length {batch_size} but got {decoder_start_token_id.shape[0]}\")\n",
    "            decoder_start_token_id = decoder_start_token_id.view(-1, 1)\n",
    "        else:\n",
    "            decoder_start_token_id = torch.ones((batch_size, 1), dtype=torch.long, device=device) * decoder_start_token_id\n",
    "\n",
    "        # 3. Encoder-decoder models expect the `decoder_input_ids` to start with a special token. Let's ensure that.\n",
    "        # no user input -> use decoder_start_token_id as decoder_input_ids\n",
    "        if decoder_input_ids is None:\n",
    "            decoder_input_ids = decoder_start_token_id\n",
    "        # exception: Donut checkpoints have task-specific decoder starts and don't expect a BOS token. Note that the\n",
    "        # original checkpoints can't be detected through `self.__class__.__name__.lower()`, needing custom logic.\n",
    "        # See: https://github.com/huggingface/transformers/pull/31470\n",
    "        elif \"donut\" in self.__class__.__name__.lower() or (self.config.model_type == \"vision-encoder-decoder\" and \"donut\" in self.config.encoder.model_type.lower()):\n",
    "            pass\n",
    "        elif self.config.model_type in [\"whisper\"]:\n",
    "            pass\n",
    "        # user input but doesn't start with decoder_start_token_id -> prepend decoder_start_token_id (and adjust\n",
    "        # decoder_attention_mask if provided)\n",
    "        #######################################\n",
    "        # !!! Update: if the first token is not decoder_start_token_id or pad_token_id, we need to prepend decoder_start_token_id\n",
    "        #######################################\n",
    "        elif ((decoder_input_ids[:, 0] != decoder_start_token_id[:, 0]) & (decoder_input_ids[:, 0] != pad_token_id)).all().item():\n",
    "            decoder_input_ids = torch.cat([decoder_start_token_id, decoder_input_ids], dim=-1)\n",
    "            if \"decoder_attention_mask\" in model_kwargs:\n",
    "                decoder_attention_mask = model_kwargs[\"decoder_attention_mask\"]\n",
    "                decoder_attention_mask = torch.cat(\n",
    "                    (torch.ones_like(decoder_attention_mask)[:, :1], decoder_attention_mask),\n",
    "                    dim=-1,\n",
    "                )\n",
    "                model_kwargs[\"decoder_attention_mask\"] = decoder_attention_mask\n",
    "\n",
    "        return decoder_input_ids, model_kwargs\n",
    "\n",
    "\n",
    "class ImageTextDataset(Dataset):\n",
    "    def __init__(self, hf_dataset, img_processor, tokenizer, split):\n",
    "        # column_names: ['source', 'images_path', 'images', 'section_text', 'doc_key', 'split_sents', 'split_sent_toks', 'sent_idx_split_idx', 'radlex', 'cxrgraph_ent', 'cxrgraph_attr', 'cxrgraph_rel']\n",
    "        self.split = split\n",
    "        self.src_path = os.path.dirname(hf_dataset.cache_files[0][\"filename\"]) if hf_dataset.cache_files else \"\"\n",
    "        self.img_processor = img_processor\n",
    "        self.tokenizer = tokenizer\n",
    "        self.samples = hf_dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    # 返回索引的数据与标签\n",
    "    def __getitem__(self, index):\n",
    "        return self.samples[index]\n",
    "\n",
    "\n",
    "def collate_fn(batch_data, img_processor, tokenizer, do_inference=False):\n",
    "\n",
    "    # 处理图像，因为每个样本的图像数量不一样，所以需要image_indices_map来记录每个样本的图像在batch中的索引\n",
    "    nested_images = [i[\"images\"] for i in batch_data]  # nested list of imgs: [[img1, img2], [img1], ...]\n",
    "    piexl_values_tensor = img_processor(images=[img for imgs in nested_images for img in imgs], return_tensors=\"pt\", do_convert_rgb=True).pixel_values\n",
    "\n",
    "    img_count = 0\n",
    "    image_indices_map = []  # e.g. [[0], [1], [2, 3], ...]\n",
    "    for item_idx, item_images in enumerate(nested_images):\n",
    "        num_images = len(item_images)\n",
    "        assert num_images <= 2, f\"num_images should be less equal than 2, but got {num_images}\"\n",
    "        image_indices_map.append(list(range(img_count, img_count + num_images)))\n",
    "        img_count += num_images\n",
    "\n",
    "    # 处理对话数据\n",
    "    conversations = []\n",
    "    num_image_tokens = GLOBAL_VARS.num_image_tokens\n",
    "    for idx, item in enumerate(batch_data):\n",
    "        num_images = len(image_indices_map[idx])\n",
    "        conversation = [\n",
    "            {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": \"You are an expert radiology assistant tasked with interpreting a chest X-ray study.\"}]},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"image\", \"num_images\": num_images, \"num_image_tokens\": num_image_tokens},\n",
    "                    {\"type\": \"text\", \"text\": \"Given the chest X-ray images, generate a description of the findings.\"},\n",
    "                ],\n",
    "            },\n",
    "        ]\n",
    "        if not do_inference:\n",
    "            conversation.append(\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": [{\"type\": \"text\", \"text\": \" \".join(item[\"split_sents\"])}],\n",
    "                }\n",
    "            )\n",
    "        conversations.append(conversation)\n",
    "\n",
    "    # See descriptions for assistant_tokens_mask\n",
    "    # Assistant tokens are the tokens that need to be generated, we use these tokens to compute the loss\n",
    "    # https://huggingface.co/docs/transformers/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.apply_chat_template.return_assistant_tokens_mask\n",
    "\n",
    "    tokenizer_kwargs = {\"pad_to_multiple_of\": 8}\n",
    "\n",
    "    if do_inference:\n",
    "        add_generation_prompt = True\n",
    "        return_assistant_tokens_mask = False\n",
    "        tokenizer_kwargs[\"padding_side\"] = \"left\"\n",
    "    else:\n",
    "        add_generation_prompt = False\n",
    "        return_assistant_tokens_mask = True\n",
    "        tokenizer_kwargs[\"padding_side\"] = \"right\"\n",
    "\n",
    "    input_text_tensor_dict = tokenizer.apply_chat_template(conversations, add_generation_prompt=add_generation_prompt, tokenize=True, padding=True, return_dict=True, return_tensors=\"pt\", tokenizer_kwargs=tokenizer_kwargs, return_assistant_tokens_mask=return_assistant_tokens_mask)\n",
    "\n",
    "    decoder_assistant_masks = None\n",
    "    if \"decoder_assistant_masks\" in input_text_tensor_dict:\n",
    "        decoder_assistant_masks = input_text_tensor_dict.decoder_assistant_masks\n",
    "        if isinstance(decoder_assistant_masks, list):  # transformers==4.47.1 will return decoder_assistant_masks in nested list\n",
    "            decoder_assistant_masks = torch.tensor(decoder_assistant_masks)\n",
    "        decoder_assistant_masks = decoder_assistant_masks.to(DEVICE)\n",
    "\n",
    "    gold_text_list = None\n",
    "    if do_inference:\n",
    "        gold_text_list = [\" \".join(i[\"split_sents\"]) for i in batch_data]\n",
    "\n",
    "    # LOGGER.debug(\"rank[%s], input text: %s\", ACCELERATOR.process_index, tokenizer.batch_decode(input_text_tensor_dict.input_ids)\n",
    "\n",
    "    return {\n",
    "        \"pixel_values\": piexl_values_tensor.to(DEVICE),  # torch.Size([bsz < x < 2*bsz, 3, 224, 224])\n",
    "        \"image_indices_map\": image_indices_map,  # [[0], [1], [2, 3], ...]\n",
    "        \"decoder_input_ids\": input_text_tensor_dict.input_ids.to(DEVICE),\n",
    "        \"decoder_attention_mask\": input_text_tensor_dict.attention_mask.to(DEVICE),\n",
    "        \"decoder_assistant_masks\": decoder_assistant_masks,\n",
    "        \"data_id_list\": [i[\"doc_key\"] for i in batch_data],\n",
    "        \"gold_text_list\": gold_text_list,\n",
    "    }\n",
    "\n",
    "\n",
    "#############################################\n",
    "# Status Class and Tracker\n",
    "#############################################\n",
    "@dataclass\n",
    "class StatusInfo:\n",
    "    curr_epoch: int = field(default=0)\n",
    "    curr_batch_iter: int = field(default=0)\n",
    "    curr_checkpoint_at: str = field(default=\"\")  # \"epoch\" or \"batch\"\n",
    "    curr_eval_split: str = field(default=\"\")  # \"validation\" or \"test\"\n",
    "\n",
    "    global_iters: int = field(default=0)\n",
    "    global_update_steps: int = field(default=0)\n",
    "    dev_best: dict = field(default_factory=lambda: {\"text_score\": 0.0, \"at_epoch\": 0, \"at_iter\": 0, \"check_at\": \"\"})\n",
    "\n",
    "    batch_loss: int = field(default=0)\n",
    "    batch_trained_examples: int = field(default=0)\n",
    "\n",
    "    run_id: dict = field(default=\"\")\n",
    "\n",
    "    grad_accum_eval_mark: int = field(default=0)\n",
    "\n",
    "    def is_achieving_best_dev_score(self, text_score):\n",
    "        if text_score >= self.dev_best[\"text_score\"]:\n",
    "            self.dev_best[\"text_score\"] = text_score\n",
    "            self.dev_best[\"at_iter\"] = self.curr_batch_iter\n",
    "            self.dev_best[\"at_epoch\"] = self.curr_epoch\n",
    "            self.dev_best[\"check_at\"] = self.curr_checkpoint_at\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def get_resumed_epoch_and_iter(self):\n",
    "        epoch_resumed = 0\n",
    "        iter_resumed = 0\n",
    "        if self.curr_checkpoint_at == \"\":\n",
    "            return epoch_resumed, iter_resumed\n",
    "\n",
    "        # Prepare the next epoch and iter indices for continue training\n",
    "        if self.curr_checkpoint_at == \"epoch\":\n",
    "            epoch_resumed = self.curr_epoch + 1\n",
    "        elif self.curr_checkpoint_at == \"batch\":\n",
    "            epoch_resumed = self.curr_epoch\n",
    "            iter_resumed = self.curr_batch_iter + 1\n",
    "        else:\n",
    "            raise ValueError(f\"Invaild STATUS_INFO.curr_checkpoint_at: {self.curr_checkpoint_at}\")\n",
    "        return epoch_resumed, iter_resumed\n",
    "\n",
    "    def state_dict(self):\n",
    "        return asdict(self)\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        for k, v in state_dict.items():\n",
    "            if hasattr(self, k):\n",
    "                self.__setattr__(k, v)\n",
    "\n",
    "\n",
    "class MLflowTracker:\n",
    "    def __init__(self, url=None, exp_name=None, run_id=None, run_name=None):\n",
    "        self.run = None\n",
    "        self.run_id = None\n",
    "        self.run_name = None\n",
    "\n",
    "        if ACCELERATOR.is_main_process:\n",
    "            mlflow.set_tracking_uri(uri=url)\n",
    "            exp_name = exp_name if exp_name else os.path.basename(__file__)\n",
    "            mlflow.set_experiment(exp_name)\n",
    "            if run_id:\n",
    "                mlflow.start_run(run_id=run_id, log_system_metrics=False)\n",
    "            elif run_name:\n",
    "                mlflow.start_run(run_name=f\"{run_name}\", log_system_metrics=False)\n",
    "            else:\n",
    "                raise ValueError(f\"Either run_id or run_name should be provided.\")\n",
    "\n",
    "            self.run = mlflow.last_active_run()\n",
    "            self.run_id = self.run.info.run_id\n",
    "            self.run_name = self.run.info.run_name\n",
    "\n",
    "    def set_tag(self, key, value):\n",
    "        if ACCELERATOR.is_main_process:\n",
    "            mlflow.set_tag(key, value)\n",
    "\n",
    "    def log_params(self, kv_dict: dict):\n",
    "        if ACCELERATOR.is_main_process:\n",
    "            mlflow.log_params(kv_dict)\n",
    "\n",
    "    def log(self, kv_dict, step):\n",
    "        if ACCELERATOR.is_main_process:\n",
    "            for k, v in kv_dict.items():\n",
    "                k = re.sub(r\"[\\W\\s]\", \"_\", k)  # replace all symbols and spaces by _\n",
    "                mlflow.log_metric(k, v, step)\n",
    "\n",
    "    def finish(self):\n",
    "        if ACCELERATOR.is_main_process:\n",
    "            mlflow.end_run()\n",
    "\n",
    "    @staticmethod\n",
    "    def launch_tracker():\n",
    "        if CONFIG[\"resume_from_checkpoint\"]:\n",
    "            mlf_tracker = MLflowTracker(url=CONFIG[\"mlflow_url\"], run_id=STATUS_INFO.run_id)\n",
    "            epoch_resumed, iter_resumed = STATUS_INFO.get_resumed_epoch_and_iter()\n",
    "            mlf_tracker.set_tag(str(CONFIG[\"jobid\"]), f\"resume:{epoch_resumed},{iter_resumed}\")\n",
    "        else:\n",
    "            # Start a new mlflow run\n",
    "            mlf_tracker = MLflowTracker(url=CONFIG[\"mlflow_url\"], run_name=CONFIG[\"output_name\"])\n",
    "            mlf_tracker.set_tag(str(CONFIG[\"jobid\"]), \"scratch\")\n",
    "            STATUS_INFO.run_id = mlf_tracker.run_id\n",
    "            # should only log params once, otherwise 500 errors\n",
    "            mlf_tracker.log_params({k: v for k, v in CONFIG.items() if k[0] != \"_\"})\n",
    "            mlf_tracker.log_params({\"use_distributed\": ACCELERATOR.use_distributed, \"num_process\": ACCELERATOR.num_processes, \"mixed_precision\": ACCELERATOR.mixed_precision, \"tracking_process_idx\": ACCELERATOR.process_index})\n",
    "        return mlf_tracker\n",
    "\n",
    "\n",
    "#############################################\n",
    "# Training\n",
    "#############################################\n",
    "\n",
    "\n",
    "def train(model, train_dataloader, train_cfg):\n",
    "    global MLFLOW_TRACKER, STATUS_INFO\n",
    "\n",
    "    # hyperparameters\n",
    "    model_params = list(model.named_parameters())\n",
    "    optimizer_grouped_parameters = prepare_optimizer_grouped_parameters(model_params, train_cfg)\n",
    "    LOGGER.debug(\"Model trainable params:\\n%s\", \"\\n\".join([n for n, p in model.named_parameters() if p.requires_grad]))\n",
    "\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, eps=1e-8)\n",
    "    # optimizer = AdamW(\n",
    "    #     optimizer_grouped_parameters,\n",
    "    #     eps=1e-8,\n",
    "    #     foreach=False,  # 禁用 multi-tensor\n",
    "    #     fused=False,  # 禁用 fused 优化\n",
    "    # )\n",
    "    total_num_steps = len(train_dataloader) // train_cfg[\"grad_accum_steps\"] * train_cfg[\"num_epochs\"]\n",
    "    scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=int(total_num_steps * train_cfg[\"warmup_proportion\"]), num_training_steps=total_num_steps)\n",
    "\n",
    "    # 1. Prepare for multi GPUs. All prepared and registered objs will be checkpointed automatically\n",
    "    model, train_dataloader, optimizer, scheduler = ACCELERATOR.prepare(model, train_dataloader, optimizer, scheduler)\n",
    "    STATUS_INFO = StatusInfo()\n",
    "    ACCELERATOR.register_for_checkpointing(STATUS_INFO)\n",
    "    LOGGER.debug(\"Final model structure:\\n%s\", model)\n",
    "\n",
    "    # 2. Check and resume checkpoint if needed\n",
    "    epoch_resumed, iter_resumed = check_status_and_resume_checkpoint()\n",
    "\n",
    "    # 3. Launch after resuming STATUS_INFO\n",
    "    MLFLOW_TRACKER = MLflowTracker.launch_tracker()\n",
    "\n",
    "    LOGGER.info(\"****************************** Training ******************************\")\n",
    "    LOGGER.info(\"Total samples = %d, batch size = %d\", len(train_dataloader.dataset), train_cfg[\"batch_size\"])\n",
    "    LOGGER.info(\"Total epochs = %d, total iterations per epoch = %d\", train_cfg[\"num_epochs\"], len(train_dataloader))\n",
    "    LOGGER.info(\"Total optimization steps = %d\", total_num_steps)\n",
    "    LOGGER.info(\"Gradient accumulation steps = %d\", train_cfg[\"grad_accum_steps\"])\n",
    "    LOGGER.info(\"Accelerator mixed_precision = %s\", ACCELERATOR.mixed_precision)\n",
    "    check_memory()\n",
    "\n",
    "    model.zero_grad()\n",
    "    for curr_epoch in range(epoch_resumed, train_cfg[\"num_epochs\"]):\n",
    "        # Ensure dataloader batches is reproducable\n",
    "        train_dataloader.set_epoch(curr_epoch)\n",
    "\n",
    "        # We need to skip steps until we reach the resumed step\n",
    "        # After the first iteration, we need to go back to the original dataloader\n",
    "        if CONFIG[\"resume_from_checkpoint\"] and curr_epoch == epoch_resumed and iter_resumed > 0:\n",
    "            active_dataloader = ACCELERATOR.skip_first_batches(train_dataloader, iter_resumed)\n",
    "        else:\n",
    "            active_dataloader = train_dataloader\n",
    "\n",
    "        start = time.time()\n",
    "        for curr_iter, batch_inputs_dict in enumerate(active_dataloader, start=iter_resumed if curr_epoch == epoch_resumed else 0):\n",
    "            with ACCELERATOR.accumulate(model):\n",
    "                # LOGGER.debug(\"Train proc=%s, epoch=%s, iter=%s, global_update_steps=%s, data_ids=%s\", ACCELERATOR.process_index, curr_epoch, curr_iter, STATUS_INFO.global_iters, batch_inputs_dict[\"data_id_list\"], main_process_only=False)\n",
    "                # Not necessarily need ACCELERATOR.autocast()\n",
    "                # Accelerate enables automatic mixed precision, so autocast() is only needed if there are other mixed precision operations besides those performed on loss by backward() which already handles the scaling.\n",
    "                with ACCELERATOR.autocast():\n",
    "                    model.train()\n",
    "                    out = model.forward(output_loss=True, **batch_inputs_dict)\n",
    "                    loss = out.loss\n",
    "\n",
    "                ACCELERATOR.backward(loss)\n",
    "                if train_cfg[\"clip_grad_norm\"] > 0:\n",
    "                    ACCELERATOR.clip_grad_norm_(model.parameters(), train_cfg[\"clip_grad_norm\"])\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                check_memory(show_only_if_peak=True)\n",
    "                log_and_update_status(curr_epoch=curr_epoch, curr_iter=curr_iter, loss=loss.item(), bsz=batch_inputs_dict[\"decoder_input_ids\"].size(0), lr=scheduler.get_last_lr()[0], train_cfg=train_cfg)\n",
    "\n",
    "                # we dont do validation, as it cost too much time\n",
    "                check_and_save_checkpoint(model, max_num_iters_per_epoch=len(train_dataloader), train_cfg=train_cfg)\n",
    "\n",
    "        end = time.time()\n",
    "        LOGGER.info(\"Batch training time: %s \", seconds_to_time_str(end - start))\n",
    "\n",
    "    LOGGER.info(\"Best achieved: %s\", STATUS_INFO.dev_best)\n",
    "    MLFLOW_TRACKER.finish()\n",
    "\n",
    "\n",
    "def prepare_optimizer_grouped_parameters(model_params, train_cfg):\n",
    "    # 为了节省计算资源和显存，应将需要冻结的参数的 `requires_grad` 显式设置为 `False`，并且在优化器中过滤不可训练参数\n",
    "\n",
    "    optimizer_grouped_parameters = []\n",
    "    if CONFIG[\"run_mode\"] == \"pretrain\":\n",
    "        encoder_params = [(n, p) for n, p in model_params if n.startswith(\"encoder\")]\n",
    "        decoder_params = [(n, p) for n, p in model_params if n.startswith(\"decoder\")]\n",
    "        adaptor_params = [(n, p) for n, p in model_params if n.startswith(\"v2l_projector\")]\n",
    "        assert encoder_params and decoder_params and adaptor_params\n",
    "\n",
    "        # 冻结 encoder, decoder，训练 v2l_projector\n",
    "        for n, p in encoder_params + decoder_params:\n",
    "            p.requires_grad = False\n",
    "        for n, p in adaptor_params:\n",
    "            p.requires_grad = True\n",
    "\n",
    "        # no_decay_names = [\"bias\", \"norm1.weight\", \"norm2.weight\", \"layernorm.weight\", \"layer_scale\"]\n",
    "        optimizer_grouped_parameters.append({\"params\": [p for n, p in adaptor_params], \"lr\": train_cfg[\"lr\"], \"weight_decay\": 0.0})\n",
    "\n",
    "    elif CONFIG[\"run_mode\"] == \"finetune\":\n",
    "        # When using peft, params requires_grad are set during initialization of PeftModel. See `apply_peft_to_model()`.\n",
    "        # We only need to group them for optimizer.\n",
    "        optimizer_grouped_parameters.append({\"params\": [p for n, p in model_params if p.requires_grad], \"lr\": train_cfg[\"lr\"], \"weight_decay\": 0.0})\n",
    "\n",
    "    return optimizer_grouped_parameters\n",
    "\n",
    "\n",
    "def check_status_and_resume_checkpoint():\n",
    "    epoch_resumed, iter_resumed = 0, 0\n",
    "    resume_from_checkpoint = CONFIG[\"resume_from_checkpoint\"]\n",
    "\n",
    "    if resume_from_checkpoint:\n",
    "        LOGGER.info(\"****************************** Resume checkpoint ******************************\")\n",
    "        # STATUS_INFO will also be loaded automatically in load_state as we have already registered it via ACCELERATOR.register_for_checkpointing(STATUS_INFO)\n",
    "        checkpoint_dir = resume_from_checkpoint if isinstance(resume_from_checkpoint, str) and os.path.exists(resume_from_checkpoint) else CONFIG[\"output_dir\"][\"checkpoint\"]\n",
    "        load_checkpoint(checkpoint_dir)\n",
    "        LOGGER.info(\"p=%d, Resumed status info %s\", ACCELERATOR.process_index, STATUS_INFO.state_dict(), main_process_only=False)\n",
    "\n",
    "        # Prepare the next epoch and iter indices for continue training\n",
    "        epoch_resumed, iter_resumed = STATUS_INFO.get_resumed_epoch_and_iter()\n",
    "        LOGGER.info(\"p=%d, epoch_resumed %d, iter_resumed %d\", ACCELERATOR.process_index, epoch_resumed, iter_resumed, main_process_only=False)\n",
    "\n",
    "    return epoch_resumed, iter_resumed\n",
    "\n",
    "\n",
    "def log_and_update_status(curr_epoch, curr_iter, loss, bsz, lr, train_cfg):\n",
    "    STATUS_INFO.curr_epoch = curr_epoch\n",
    "    STATUS_INFO.curr_batch_iter = curr_iter\n",
    "    STATUS_INFO.batch_trained_examples += bsz\n",
    "    STATUS_INFO.batch_loss += loss * bsz\n",
    "    STATUS_INFO.global_iters += 1\n",
    "\n",
    "    if ACCELERATOR.sync_gradients:\n",
    "        STATUS_INFO.global_update_steps += 1\n",
    "\n",
    "    # Logging too often may slow down the process\n",
    "    print_loss_per_n_steps = train_cfg[\"print_loss_per_n_steps\"]\n",
    "    if STATUS_INFO.global_update_steps == 1 or STATUS_INFO.global_update_steps % print_loss_per_n_steps == 0:\n",
    "        avg_loss = STATUS_INFO.batch_loss / STATUS_INFO.batch_trained_examples\n",
    "\n",
    "        MLFLOW_TRACKER.log(\n",
    "            {\n",
    "                \"lr\": lr,\n",
    "                \"avg_loss\": avg_loss,\n",
    "                \"epoch\": STATUS_INFO.curr_epoch,\n",
    "                \"global_update_steps\": STATUS_INFO.global_update_steps,\n",
    "            },\n",
    "            step=STATUS_INFO.global_iters,\n",
    "        )\n",
    "\n",
    "        LOGGER.info(\n",
    "            \"p=%s, Epoch=%d, iter=%d, steps=%d, loss=%.9f\",\n",
    "            ACCELERATOR.process_index,\n",
    "            STATUS_INFO.curr_epoch,\n",
    "            STATUS_INFO.curr_batch_iter,\n",
    "            STATUS_INFO.global_update_steps,\n",
    "            avg_loss,\n",
    "            main_process_only=True,\n",
    "        )\n",
    "        STATUS_INFO.batch_loss, STATUS_INFO.batch_trained_examples = 0, 0\n",
    "\n",
    "\n",
    "def check_and_save_checkpoint(model, max_num_iters_per_epoch, train_cfg):\n",
    "    do_ckp = True\n",
    "    # do_ckp at the end of each epoch\n",
    "    if STATUS_INFO.curr_batch_iter + 1 == max_num_iters_per_epoch:\n",
    "        STATUS_INFO.curr_checkpoint_at = \"epoch\"\n",
    "    # do_ckp at specific steps:\n",
    "    elif train_cfg[\"ckp_per_steps\"] > 0 and STATUS_INFO.global_update_steps % train_cfg[\"ckp_per_steps\"] == 0:\n",
    "        STATUS_INFO.curr_checkpoint_at = \"batch\"\n",
    "    else:\n",
    "        do_ckp = False\n",
    "\n",
    "    # 当 grad_accum = N > 1 时，这 N 个 iters 的 STATUS_INFO.global_update_steps 都是一样的。不做处理时，都会激活 do_eval。\n",
    "    # 我们希望这 N 个 iters 只进行一次 eval。\n",
    "    # 目前的逻辑是，当进入这个条件时，说明在这个 global_update_steps 中，已经进行过一次 eval 了，其余的 iters 不需要进行 eval。\n",
    "    # 由于 grad_accum_eval_mark 默认值为 0，所以 global_update_steps == 0 时，也默认不评估。\n",
    "    if STATUS_INFO.grad_accum_eval_mark == STATUS_INFO.global_update_steps:\n",
    "        do_ckp = False\n",
    "\n",
    "    if do_ckp:\n",
    "        check_memory()\n",
    "        STATUS_INFO.grad_accum_eval_mark = STATUS_INFO.global_update_steps  # this line shoud runs before save_checkpoint(), to set the correct STATUS_INFO.grad_accum_eval_mark for checkpoingting\n",
    "        save_checkpoint(checkpoint_dir=CONFIG[\"output_dir\"][\"checkpoint\"])\n",
    "\n",
    "\n",
    "#############################################\n",
    "# Validation\n",
    "# Since the validation process takes a lot of time, we skip the validation process in training.\n",
    "#############################################\n",
    "def validation_process(model, valid_dataloader, max_num_iters_per_epoch, train_cfg):\n",
    "\n",
    "    do_eval = True\n",
    "    # eval at the end of each epoch\n",
    "    if STATUS_INFO.curr_batch_iter + 1 == max_num_iters_per_epoch:\n",
    "        STATUS_INFO.curr_checkpoint_at = \"epoch\"\n",
    "        STATUS_INFO.curr_eval_split = \"validation\"\n",
    "    # eval at specific steps:\n",
    "    elif train_cfg[\"eval_per_steps\"] > 0 and STATUS_INFO.global_update_steps % train_cfg[\"eval_per_steps\"] == 0:\n",
    "        STATUS_INFO.curr_checkpoint_at = \"batch\"\n",
    "        STATUS_INFO.curr_eval_split = \"validation\"\n",
    "    else:\n",
    "        do_eval = False\n",
    "\n",
    "    # 当 grad_accum = N > 1 时，这 N 个 iters 的 STATUS_INFO.global_update_steps 都是一样的。不做处理时，都会激活 do_eval。\n",
    "    # 我们希望这 N 个 iters 只进行一次 eval。\n",
    "    # 目前的逻辑是，当进入这个条件时，说明在这个 global_update_steps 中，已经进行过一次 eval 了，其余的 iters 不需要进行 eval。\n",
    "    # 由于 grad_accum_eval_mark 默认值为 0，所以 global_update_steps == 0 时，也默认不评估。\n",
    "    if STATUS_INFO.grad_accum_eval_mark == STATUS_INFO.global_update_steps:\n",
    "        do_eval = False\n",
    "\n",
    "    if do_eval:\n",
    "        check_memory()\n",
    "        eval_result_dict = evaluate(model, target_dataloader=valid_dataloader)\n",
    "        STATUS_INFO.grad_accum_eval_mark = STATUS_INFO.global_update_steps  # this line shoud runs before check_results_and_save_model(), to set the correct STATUS_INFO.grad_accum_eval_mark for checkpoingting\n",
    "        check_results_and_save_model(model, eval_result_dict)\n",
    "\n",
    "\n",
    "def check_results_and_save_model(model, eval_result_dict):\n",
    "    # Check\n",
    "    text_score = 0\n",
    "    num_metrics = 0\n",
    "    for metric_key in [\"BLEU\", \"ROUGEL\", \"chexbert-all_micro avg_f1-score\", \"radgraph_partial\", \"bertscore\"]:\n",
    "        if metric_key in eval_result_dict:\n",
    "            MLFLOW_TRACKER.log(\n",
    "                {f\"{STATUS_INFO.curr_eval_split}_{metric_key}\": eval_result_dict[metric_key]},\n",
    "                step=STATUS_INFO.global_iters,\n",
    "            )\n",
    "            text_score += eval_result_dict[metric_key]\n",
    "            num_metrics += 1\n",
    "    text_score = text_score / num_metrics\n",
    "\n",
    "    LOGGER.info(\"****************************** Checkpoint ******************************\")\n",
    "    LOGGER.info(\"Current [%s] text_avg_f1: %.3f, at epoch %d, iter %d (%s)\", STATUS_INFO.curr_eval_split, text_score * 100, STATUS_INFO.curr_epoch, STATUS_INFO.curr_batch_iter, STATUS_INFO.curr_checkpoint_at)\n",
    "    LOGGER.info(\"Best [%s] avg-f1: %.3f, at epoch %d, iter %d\", STATUS_INFO.curr_eval_split, STATUS_INFO.dev_best[\"text_score\"] * 100, STATUS_INFO.dev_best[\"at_epoch\"], STATUS_INFO.dev_best[\"at_iter\"])\n",
    "    MLFLOW_TRACKER.log({f\"{STATUS_INFO.curr_eval_split}_text_avg_f1\": text_score}, step=STATUS_INFO.global_iters)\n",
    "\n",
    "    # checkpointing\n",
    "    save_checkpoint(checkpoint_dir=CONFIG[\"output_dir\"][\"checkpoint\"])\n",
    "\n",
    "    # Save the best\n",
    "    if STATUS_INFO.is_achieving_best_dev_score(text_score):\n",
    "        save_model(model, CONFIG[\"output_dir\"][\"model\"])\n",
    "\n",
    "\n",
    "#############################################\n",
    "# Evaluation\n",
    "#############################################\n",
    "def evaluate(model, target_dataloader, output_result=False, **kwargs):\n",
    "    GLOBAL_VARS.peak_mem = 0\n",
    "\n",
    "    eval_bsz = kwargs[\"eval_batch_size\"]\n",
    "    max_new_tokens = kwargs[\"max_new_tokens\"]\n",
    "    print_pred_per_n_steps = kwargs[\"print_pred_per_n_steps\"]\n",
    "\n",
    "    LOGGER.info(\"****************************** Evaluation ******************************\")\n",
    "    LOGGER.info(\"Source = %s\", target_dataloader.dataset.src_path)\n",
    "    LOGGER.info(\"Batch size = %d\", eval_bsz)\n",
    "    LOGGER.info(\"Num samples = %d\", len(target_dataloader.dataset))\n",
    "    tokenizer = target_dataloader.dataset.tokenizer\n",
    "\n",
    "    data_ids = []\n",
    "    pred_seqs = []\n",
    "    gold_seqs = []\n",
    "\n",
    "    start = time.time()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, input_tensors_dict in enumerate(target_dataloader):\n",
    "            # Model inference, check args in https://huggingface.co/docs/transformers/main/en/main_classes/text_generation#transformers.GenerationMixin\n",
    "            with ACCELERATOR.autocast():\n",
    "                LOGGER.debug(\"rank[%s], eval pixel_values shape: %s\", ACCELERATOR.process_index, input_tensors_dict[\"pixel_values\"].shape, main_process_only=False)\n",
    "\n",
    "                # https://huggingface.co/docs/transformers/v4.51.1/en/main_classes/text_generation#transformers.GenerationConfig\n",
    "                generation_config = GenerationConfig(\n",
    "                    max_new_tokens=max_new_tokens,\n",
    "                    pad_token_id=tokenizer.pad_token_id,\n",
    "                    bos_token_id=tokenizer.bos_token_id,\n",
    "                    eos_token_id=tokenizer.eos_token_id,\n",
    "                    do_sample=False,\n",
    "                    num_beams=3,\n",
    "                    return_dict_in_generate=True,\n",
    "                    output_logits=True,\n",
    "                )\n",
    "                # https://huggingface.co/docs/transformers/v4.51.1/en/main_classes/text_generation#transformers.GenerationMixin\n",
    "                outputs = model.generate(\n",
    "                    generation_config=generation_config,\n",
    "                    inputs=input_tensors_dict[\"pixel_values\"],\n",
    "                    decoder_input_ids=input_tensors_dict[\"decoder_input_ids\"],\n",
    "                    decoder_attention_mask=input_tensors_dict[\"decoder_attention_mask\"],\n",
    "                )\n",
    "                check_memory(show_only_if_peak=True)\n",
    "\n",
    "            pred_seq_start_ids = input_tensors_dict[\"decoder_input_ids\"].size(1)  # 生成的序列的起始位置\n",
    "            pred_sequences_ids = outputs.sequences[:, pred_seq_start_ids:]\n",
    "            pred_sequences = tokenizer.batch_decode(pred_sequences_ids, skip_special_tokens=True)\n",
    "            gold_sequences = input_tensors_dict[\"gold_text_list\"]\n",
    "\n",
    "            # Gathers input_data and potentially drops duplicates in the last batch if on a distributed system.\n",
    "            data_ids.extend(ACCELERATOR.gather_for_metrics(input_tensors_dict[\"data_id_list\"], use_gather_object=True))\n",
    "            pred_seqs.extend(ACCELERATOR.gather_for_metrics(pred_sequences, use_gather_object=True))\n",
    "            gold_seqs.extend(ACCELERATOR.gather_for_metrics(gold_sequences, use_gather_object=True))\n",
    "\n",
    "            if (print_pred_per_n_steps > 0 and batch_idx % print_pred_per_n_steps == 0) or (batch_idx + 1 == len(target_dataloader)):\n",
    "                LOGGER.info(\n",
    "                    \"Eval at: p=%s, iter=%d, curr_seq_len=%s, pred_seq_example=%s\",\n",
    "                    ACCELERATOR.process_index,\n",
    "                    batch_idx,\n",
    "                    len(pred_seqs),\n",
    "                    pred_sequences[0],\n",
    "                    main_process_only=False,\n",
    "                )\n",
    "\n",
    "    assert len(data_ids) == len(set(data_ids)), f\"Duplicated data exists, please check {data_ids}\"\n",
    "    assert len(data_ids) == target_dataloader.total_dataset_length, f\"Gathered data size ({len(data_ids)}) should equals to dataset size ({target_dataloader.total_dataset_length})\"\n",
    "    # LOGGER.debug(\"p=%s, len=%s, data_ids: %s\", ACCELERATOR.process_index, len(data_ids), data_ids)\n",
    "    # LOGGER.debug(\"p=%s, len=%s, pred_seqs: %s\", ACCELERATOR.process_index, len(pred_seqs), pred_seqs)\n",
    "    # LOGGER.debug(\"p=%s, len=%s, gold_seqs: %s\", ACCELERATOR.process_index, len(gold_seqs), gold_seqs)\n",
    "    if output_result:\n",
    "        with open(f\"{CONFIG['output_dir']['result']}/{target_dataloader.dataset.split}_{ACCELERATOR.process_index}.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(json.dumps({\"gold_seqs\": gold_seqs, \"pred_seqs\": pred_seqs}))\n",
    "\n",
    "    # Evaluate the results\n",
    "    text_scores_dict = compute_generation_score(gold_text_list=gold_seqs, pred_text_list=pred_seqs)\n",
    "    LOGGER.info(\"[TextGen]: %s\", json.dumps(text_scores_dict))\n",
    "\n",
    "    if STATUS_INFO:\n",
    "        for metric_name, metric_val in text_scores_dict.items():\n",
    "            k = f\"{target_dataloader.dataset.split}_{metric_name}\"\n",
    "            MLFLOW_TRACKER.log({k: metric_val}, step=STATUS_INFO.global_iters)\n",
    "\n",
    "    end = time.time()\n",
    "    LOGGER.info(\"Evaluation time: %s\", seconds_to_time_str(end - start))\n",
    "    check_memory()\n",
    "    return text_scores_dict\n",
    "\n",
    "\n",
    "def compute_generation_score(gold_text_list, pred_text_list):\n",
    "    \"\"\"Based on the script from https://vilmedic.app/misc/bionlp24/leaderboard#anchor-baseline\"\"\"\n",
    "    if DEVICE.type == \"cpu\":\n",
    "        use_metrics = [\"BLEU\", \"ROUGEL\", \"radgraph\", \"chexbert\"]\n",
    "    else:\n",
    "        use_metrics = [\"BLEU\", \"ROUGEL\", \"radgraph\", \"chexbert\", \"bertscore\"]\n",
    "\n",
    "    refs = [\" \".join(wordpunct_tokenize(s.lower())) for s in gold_text_list]\n",
    "    hyps = [\" \".join(wordpunct_tokenize(s.lower())) for s in pred_text_list]\n",
    "\n",
    "    # https://github.com/jbdel/vilmedic/blob/main/vilmedic/blocks/scorers/scores.py\n",
    "    out_dict = compute_scores(use_metrics, refs=refs, hyps=hyps, split=None, seed=None, config=None, epoch=None, logger=LOGGER, dump=False)\n",
    "    out_dict = {k: float(v) for k, v in out_dict.items()}\n",
    "    return out_dict\n",
    "\n",
    "\n",
    "#############################################\n",
    "# Utils\n",
    "#############################################\n",
    "def check_model_params(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            LOGGER.debug(\"Parameter %s has requires_grad=True\", name)\n",
    "        elif param.grad is not None:\n",
    "            LOGGER.debug(\"Parameter %s has gradient: %s\", name, param.grad)\n",
    "        else:\n",
    "            LOGGER.debug(\"Parameter %s is correctly frozen\", name)\n",
    "\n",
    "\n",
    "def check_memory(show_only_if_peak=False):\n",
    "    if not torch.cuda.is_available():\n",
    "        return\n",
    "    # 获取当前 GPU 设备的属性\n",
    "\n",
    "    device = torch.cuda.current_device()\n",
    "    device_properties = torch.cuda.get_device_properties(device)\n",
    "    # 获取 GPU 总显存\n",
    "    total_memory = device_properties.total_memory / 1024**3  # 转换为 GB\n",
    "    # 获取Torch总占用显存\n",
    "    total_reserved = torch.cuda.memory_reserved() / 1024**3  # GB\n",
    "\n",
    "    if show_only_if_peak:\n",
    "        # 显存占用的峰值值\n",
    "        peak_reserved = torch.cuda.max_memory_reserved() / 1024**3  # GB\n",
    "        if peak_reserved > GLOBAL_VARS.peak_mem:\n",
    "            GLOBAL_VARS.peak_mem = peak_reserved\n",
    "            LOGGER.info(\"Peak memory reached: %.2f / %.2f GB\", peak_reserved, total_memory)\n",
    "        # torch.cuda.reset_max_memory_reserved()  # 重置峰值值\n",
    "    else:\n",
    "        LOGGER.info(\"Memory reserved: %.2f / %.2f GB\", total_reserved, total_memory)\n",
    "\n",
    "\n",
    "def seconds_to_time_str(seconds):\n",
    "    hours = seconds // 3600  # 1小时 = 3600秒\n",
    "    minutes = (seconds % 3600) // 60  # 除去小时部分后，再计算分钟\n",
    "    seconds = seconds % 60  # 剩余的秒数\n",
    "\n",
    "    return f\"{hours:.0f}h {minutes:.0f}min {seconds:.1f}s\"\n",
    "\n",
    "\n",
    "def load_checkpoint(checkpoint_dir):\n",
    "    \"\"\"training_info will be loaded automatically in load_state as we have already registered it via ACCELERATOR.register_for_checkpointing(training_info)\"\"\"\n",
    "    # 如果 checkpoint_dir 是 CONFIG[\"output_dir\"][\"checkpoint\"]，就选择最新的 checkpoint\n",
    "    checkpoint_list = sorted(glob.glob(os.path.join(checkpoint_dir, \"epoch_*_iter_*\")), key=os.path.getctime)\n",
    "    if len(checkpoint_list) > 0:\n",
    "        checkpoint_dir = checkpoint_list[-1]\n",
    "\n",
    "    ACCELERATOR.load_state(checkpoint_dir)\n",
    "    LOGGER.info(\"Checkpoint loaded from %s\", checkpoint_dir)\n",
    "\n",
    "\n",
    "def save_checkpoint(checkpoint_dir, max_to_keep=5):\n",
    "    ckp_path = os.path.join(checkpoint_dir, f\"epoch_{STATUS_INFO.curr_epoch}_iter_{STATUS_INFO.curr_batch_iter}\")\n",
    "    ACCELERATOR.save_state(ckp_path)\n",
    "    LOGGER.info(\"Checkpoint saved to %s\", ckp_path)\n",
    "\n",
    "    # 如果文件数量超过 max_to_keep，删除旧的 checkpoint\n",
    "    max_to_keep = CONFIG[\"max_checkpoints_to_keep\"] if CONFIG[\"max_checkpoints_to_keep\"] else max_to_keep\n",
    "    if ACCELERATOR.is_main_process:\n",
    "        # os.path.getctime 按创建时间排序\n",
    "        checkpoint_files = sorted(glob.glob(os.path.join(checkpoint_dir, \"epoch_*_iter_*\")), key=os.path.getctime)\n",
    "        if len(checkpoint_files) > max_to_keep:\n",
    "            old_checkpoints = checkpoint_files[:-max_to_keep]  # 排除最近的 max_to_keep 个\n",
    "            for old_checkpoint in old_checkpoints:\n",
    "                if os.path.isdir(old_checkpoint):\n",
    "                    shutil.rmtree(old_checkpoint)\n",
    "                LOGGER.info(\"Old checkpoint removed: %s\", old_checkpoint)\n",
    "\n",
    "\n",
    "def save_model(model, output_dir):\n",
    "\n",
    "    # LOGGER.debug(\"rank[%s], model\", model, main_process_only=False)\n",
    "    # for n, p in model.named_parameters():\n",
    "    #     LOGGER.debug(\"rank[%s], %s: %s\", ACCELERATOR.process_index, n, p.shape, main_process_only=False)\n",
    "\n",
    "    # ACCELERATOR.wait_for_everyone()\n",
    "    # LOGGER.debug(\"rank[%s], model.state_dict()\", ACCELERATOR.process_index, main_process_only=False)\n",
    "    # for n, p in model.state_dict().items():\n",
    "    #     LOGGER.debug(\"rank[%s], %s: %s\", ACCELERATOR.process_index, n, p.shape, main_process_only=False)\n",
    "\n",
    "    # ACCELERATOR.wait_for_everyone()\n",
    "    # unwrapped_model = ACCELERATOR.unwrap_model(model)\n",
    "    # state_dict = ACCELERATOR.get_state_dict(model)\n",
    "    # LOGGER.debug(\"rank[%s], ACCELERATOR.get_state_dict(model)\", ACCELERATOR.process_index, main_process_only=False)\n",
    "    # for n, p in state_dict.items():\n",
    "    #     LOGGER.debug(\"rank[%s], %s: %s\", ACCELERATOR.process_index, n, p.shape, main_process_only=False)\n",
    "\n",
    "    # # 这是 unwrapped_model.save_pretrained 内部调用的方式\n",
    "    # state_dict = get_peft_model_state_dict(unwrapped_model)\n",
    "    # LOGGER.debug(\"rank[%s], get_peft_model_state_dict(unwrapped_model)\", ACCELERATOR.process_index, main_process_only=False)\n",
    "    # for n, p in state_dict.items():\n",
    "    #     LOGGER.debug(\"rank[%s], %s: %s\", ACCELERATOR.process_index, n, p.shape, main_process_only=False)\n",
    "\n",
    "    # accelerator.get_state_dict(model) 会导致 rank1 上的 state_dict 不完整，不能使用\n",
    "    # get_peft_model_state_dict(unwrapped_model) 可以让所有rank都拿到完整的 state_dict\n",
    "    # unwrapped_model.save_pretrained 不传入 state_dict 时，会使用 get_peft_model_state_dict(self) 来获取 state_dict。也就是说是正确的\n",
    "    # 当获取state_dict时，如果程序尝试在多个rank中同步state_dict，而由使用了 if ACCELERATOR.is_main_process: 来判断是否保存模型，则可能会导致进程卡主。比如rank0需要等到rank1同步state_dict，而rank1已经跳过了这个判断，进入wait的状态。\n",
    "    # 所以结论就是，unwrapped_model 是 peft model时，不需要传入 state_dict 给 unwrapped_model.save_pretrained()，让它自己在内部调用 get_peft_model_state_dict() 来获取 state_dict即可，它也会处理不同rank的情况。\n",
    "\n",
    "    ACCELERATOR.wait_for_everyone()\n",
    "    unwrapped_model = ACCELERATOR.unwrap_model(model)\n",
    "    if isinstance(unwrapped_model, PeftModel):\n",
    "        # accelerator.get_state_dict(model) 会导致 rank1 上的 state_dict 不完整，不能使用\n",
    "        # get_peft_model_state_dict(unwrapped_model) 可以让所有rank都拿到完整的 state_dict\n",
    "        # unwrapped_model.save_pretrained 不传入 state_dict 时，会使用 get_peft_model_state_dict(self) 来获取 state_dict。也就是说是正确的\n",
    "        # 当获取state_dict时，如果程序尝试在多个rank中同步state_dict，而由使用了 if ACCELERATOR.is_main_process: 来判断是否保存模型，则可能会导致进程卡主。比如rank0需要等到rank1同步state_dict，而rank1已经跳过了这个判断，进入wait的状态。\n",
    "        # 所以结论就是，unwrapped_model 是 peft model时，不需要传入 state_dict 给 unwrapped_model.save_pretrained()，让它自己在内部调用 get_peft_model_state_dict() 来获取 state_dict即可，它也会处理不同rank的情况。\n",
    "        # 目前的测试来看，不需要state_dict=ACCELERATOR.get_state_dict(model)， 模型也能够正确的保存完整的参数。\n",
    "        # 当前使用的FSDPPlugin 参数为 ：sharding_strategy = \"FULL_SHARD\"，state_dict_type = \"SHARDED_STATE_DICT\"\n",
    "        unwrapped_model.save_pretrained(\n",
    "            output_dir,\n",
    "            is_main_process=ACCELERATOR.is_main_process,\n",
    "            save_function=ACCELERATOR.save,\n",
    "            save_embedding_layers=True,\n",
    "        )\n",
    "    else:\n",
    "        # 虽然Accelerate FSDP 建议使用 state_dict=ACCELERATOR.get_state_dict(model)\n",
    "        # 但经过测试，使用该参数，会导致 rank1 额外保存一个 model.satetensors 文件，导致加载时出现识别错误。\n",
    "        # 目前的测试来看，不需要state_dict=ACCELERATOR.get_state_dict(model)， 模型也能够正确的保存完整的参数。\n",
    "        # 当前使用的FSDPPlugin 参数为 ：sharding_strategy = \"FULL_SHARD\"，state_dict_type = \"SHARDED_STATE_DICT\"\n",
    "        # https://huggingface.co/docs/accelerate/usage_guides/fsdp#saving-and-loading\n",
    "\n",
    "        unwrapped_model.save_pretrained(\n",
    "            output_dir,\n",
    "            is_main_process=ACCELERATOR.is_main_process,\n",
    "            save_function=ACCELERATOR.save,\n",
    "            # state_dict=ACCELERATOR.get_state_dict(model),  # suggested by Accelerate FSDP when using transformers\n",
    "        )\n",
    "\n",
    "    ACCELERATOR.wait_for_everyone()\n",
    "    LOGGER.info(\"Model saved to %s\", output_dir)\n",
    "\n",
    "\n",
    "def save_processors(img_processor, tokenizer, output_dir):\n",
    "    ACCELERATOR.wait_for_everyone()\n",
    "    if ACCELERATOR.is_main_process:\n",
    "        img_processor.save_pretrained(output_dir)\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "        LOGGER.info(\"Image Processor and tokenizer are saved to: %s\", output_dir)\n",
    "    ACCELERATOR.wait_for_everyone()\n",
    "\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)  # Python的随机性\n",
    "    # 设置Python哈希种子，为了禁止hash随机化，使得实验可复现\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)  # numpy的随机性\n",
    "    torch.manual_seed(seed)  # torch的CPU随机性，为CPU设置随机种子\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)  # torch的GPU随机性，为当前GPU设置随机种子\n",
    "        torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.   torch的GPU随机性，为所有GPU设置随机种子\n",
    "    torch.backends.cudnn.benchmark = False  # if benchmark=True, deterministic will be False\n",
    "    torch.backends.cudnn.deterministic = True  # 选择确定性算法\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "    os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "\n",
    "\n",
    "#############################################\n",
    "# Data pre-processing\n",
    "#############################################\n",
    "\n",
    "\n",
    "def select_images(images):\n",
    "    # 根据相似度选择2张图片：如果小于等于2张图片，就直接使用；否则，选择最不相似的两张图片\n",
    "    selected_images = []\n",
    "    selected_image_indices = []\n",
    "    if len(images) <= 2:\n",
    "        selected_images = images\n",
    "        selected_image_indices = list(range(len(images)))  # [0] / [0, 1]\n",
    "    else:\n",
    "        n = len(images)\n",
    "        max_hash_distance = -1\n",
    "        for i in range(0, n):\n",
    "            for j in range(i + 1, n):\n",
    "                # phash值越小，表示两张图片越相似，\n",
    "                # 默认的 hash_size=8 会生成一个 8x8=64 位的哈希值（位字符串）。\n",
    "                # 更大的 hash_size 会提取图像中更多的细节，因此更能区分复杂图像，但对噪声和微小变化的鲁棒性会降低。\n",
    "                # 较小的 hash_size 更关注图像整体的结构和轮廓信息，因此对噪声或轻微变化更鲁棒，但可能忽略细节差异。\n",
    "                # 可能出现不同的 images 的 hash_distance=0 的情况。\n",
    "                hash_distance = abs(imagehash.phash(images[i]) - imagehash.phash(images[j]))\n",
    "                if hash_distance > max_hash_distance:\n",
    "                    max_hash_distance = hash_distance\n",
    "                    selected_images = [images[i], images[j]]\n",
    "                    selected_image_indices = [int(i), int(j)]\n",
    "\n",
    "    return selected_images, selected_image_indices\n",
    "\n",
    "\n",
    "def resize_image_with_bspline_pil(image, target_size=518):\n",
    "    # 转换 PIL 图像为 numpy 数组\n",
    "    img_array = np.array(image)\n",
    "    h, w = img_array.shape[:2]\n",
    "\n",
    "    # 计算缩放比例，确保较短边为 target_size\n",
    "    scale = target_size / min(h, w)\n",
    "    new_h, new_w = int(h * scale), int(w * scale)\n",
    "\n",
    "    # 根据图像维度计算缩放因子\n",
    "    if img_array.ndim == 2:  # 灰度图像\n",
    "        zoom_factors = (new_h / h, new_w / w)\n",
    "    elif img_array.ndim == 3:  # RGB/多通道图像\n",
    "        zoom_factors = (new_h / h, new_w / w, 1)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported image dimension: {img_array.ndim}\")\n",
    "\n",
    "    # 使用 B-spline 插值 (order=3 表示 B-spline)\n",
    "    resized_array = zoom(img_array, zoom_factors, order=3)\n",
    "\n",
    "    # 转换回 PIL 图像\n",
    "    resized_image = Image.fromarray(np.uint8(resized_array))\n",
    "\n",
    "    return resized_image\n",
    "\n",
    "\n",
    "def pre_process_dataset(img_processor, img_dataset, text_dataset, shortest_edge, convert_to_rgb=True):\n",
    "    # align image_ds to text_ds\n",
    "    ds_textRowId_imgId = []\n",
    "    for textDs_row_idx, doc_key in enumerate(text_dataset[\"doc_key\"]):\n",
    "        data_split, img_id, section_name = doc_key.split(\"#\")\n",
    "        ds_textRowId_imgId.append((int(textDs_row_idx), int(img_id)))\n",
    "\n",
    "    # 按照 img_id 排序\n",
    "    sorted_ds_textRowId_imgId = sorted(ds_textRowId_imgId, key=lambda x: x[1])\n",
    "\n",
    "    # 如果传入的是裁剪后的 img_ds 数据集，那么 img_id 与 img_row_id 不一定是一一对应的\n",
    "    ds_imgId_imgRowId = {img_id: img_row_id for img_row_id, img_id in enumerate(img_dataset[\"img_id\"])}\n",
    "\n",
    "    # 按照 img_id 的顺序，将 img_ds 的数据拼接到 text_ds 的数据中\n",
    "    filtered_img_ds = img_dataset.select([ds_imgId_imgRowId[img_id] for _, img_id in sorted_ds_textRowId_imgId if img_id in ds_imgId_imgRowId])\n",
    "    filtered_text_ds = text_dataset.select([text_row_id for text_row_id, _ in sorted_ds_textRowId_imgId])\n",
    "    filtered_dataset = concatenate_datasets([filtered_img_ds, filtered_text_ds], axis=1)\n",
    "    LOGGER.debug(\"Concatenated image-text dataset dict (aligning image_ds to text_ds): \\n%s\", filtered_dataset)\n",
    "\n",
    "    def map_func(examples):\n",
    "        # Select images\n",
    "        # 保存图像的piexl_values会占用极大硬盘空间，且极大的减慢模型训练时的数据读取速度。\n",
    "        # 因此预处理只进行resize\n",
    "        selected_images_list = []\n",
    "        selected_indices_list = []\n",
    "        for example_idx, images_per_example in enumerate(examples[\"images\"]):\n",
    "            selected_images, selected_indices = select_images(images_per_example)\n",
    "            # LANCZOS 更适合处理含有精细细节的图像 (如 X-ray 图像), 可以更好地保留图像中高频信息。适合对病灶等微小特征的保留。\n",
    "            selected_images = [resize_image_with_bspline_pil(img) for img in selected_images]\n",
    "            selected_images_list.append(selected_images)\n",
    "            selected_indices_list.append(selected_indices)\n",
    "\n",
    "        examples[\"images\"] = selected_images_list\n",
    "        examples[\"selected_indices_list\"] = selected_images_list\n",
    "        return examples\n",
    "\n",
    "    preprocess_cfg = CONFIG[\"preprocess\"]\n",
    "    new_dataset = filtered_dataset.map(map_func, batched=preprocess_cfg[\"batched\"], batch_size=preprocess_cfg[\"batch_size\"], num_proc=preprocess_cfg[\"num_proc\"])  #\n",
    "    LOGGER.debug(\"Preprocessed final dataset dict: \\n%s\", new_dataset)\n",
    "    return new_dataset\n",
    "\n",
    "\n",
    "def load_src_datasets(data_paths):\n",
    "    dataset_interpret = load_from_disk(data_paths[\"interpret\"])\n",
    "    LOGGER.debug(\"%s loaded from interpret_cxr\", [f\"{split}:{len(ds)}\" for split, ds in dataset_interpret.items()])\n",
    "    dataset_mimic = load_from_disk(data_paths[\"mimic\"])\n",
    "    LOGGER.debug(\"%s loaded from mimic-cxr\", [f\"{split}:{len(ds)}\" for split, ds in dataset_mimic.items()])\n",
    "\n",
    "    # Concat both\n",
    "    dataset_train_dev = DatasetDict({\"train\": concatenate_datasets([dataset_interpret[\"train\"], dataset_mimic[\"train\"]]), \"validation\": concatenate_datasets([dataset_interpret[\"validation\"], dataset_mimic[\"validation\"]])})\n",
    "    dataset_test = load_from_disk(data_paths[\"interpret-test-public\"])\n",
    "\n",
    "    ds_img = DatasetDict({\"train\": dataset_train_dev[\"train\"], \"validation\": dataset_train_dev[\"validation\"], \"test\": dataset_test[\"test\"]})\n",
    "    for split in ds_img:\n",
    "        ds_img[split] = ds_img[split].add_column(\"img_id\", range(len(ds_img[split])))\n",
    "    LOGGER.debug(\"Loaded image-report dataset: \\n%s\", ds_img)\n",
    "\n",
    "    ds_text = load_from_disk(data_paths[\"custom_text\"])\n",
    "\n",
    "    for split in ds_text:\n",
    "        ds_text[split] = ds_text[split].add_column(\"text_id\", range(len(ds_text[split])))\n",
    "    LOGGER.debug(\"Loaded custom split_text dataset: \\n%s\", ds_text)\n",
    "\n",
    "    if CONFIG[\"target_section\"] == \"findings\":\n",
    "        ds_img = ds_img.remove_columns(\"impression\")\n",
    "        ds_img = ds_img.rename_column(\"findings\", \"section_text\")\n",
    "    elif CONFIG[\"target_section\"] == \"impression\":\n",
    "        ds_img = ds_img.remove_columns(\"findings\")\n",
    "        ds_img = ds_img.rename_column(\"impression\", \"section_text\")\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid target_section {CONFIG['target_section']}, expected 'findings' or 'impression'\")\n",
    "\n",
    "    return ds_img, ds_text\n",
    "\n",
    "\n",
    "def preprocess_dataset():\n",
    "    img_dataset, text_dataset = load_src_datasets(data_paths=CONFIG[\"data_path\"])\n",
    "\n",
    "    # Get dataloader for training and testing\n",
    "    image_processor_name = CONFIG[\"preprocess\"][\"image_processor\"]\n",
    "    model_name_or_path = CONFIG[\"model_name_or_path\"][image_processor_name]\n",
    "    # TODO 之前的数据是用slow版本处理的，可能会产生不一样的结果\n",
    "    img_processor = AutoImageProcessor.from_pretrained(model_name_or_path, use_fast=True)\n",
    "    shortest_edge = img_processor.size[\"shortest_edge\"]\n",
    "\n",
    "    ds_dict = {}\n",
    "    for split in [\"train\", \"validation\", \"test\"]:\n",
    "        ds_dict[split] = pre_process_dataset(img_processor=img_processor, img_dataset=img_dataset[split], text_dataset=text_dataset[split], shortest_edge=shortest_edge, convert_to_rgb=True)\n",
    "        # .select(range(len(text_dataset[split]) - 200, len(text_dataset[split])))\n",
    "\n",
    "    pre_processed_dataset_dict = DatasetDict(ds_dict)\n",
    "    pre_processed_dataset_dict.save_to_disk(CONFIG[\"preprocess\"][\"cache_path\"])\n",
    "    LOGGER.info(\"Preprocessed dataset dict saved to: %s\", CONFIG[\"preprocess\"][\"cache_path\"])\n",
    "\n",
    "\n",
    "#############################################\n",
    "def load_peft_model(base_model, peft_model_path):\n",
    "    peft_model = PeftModel.from_pretrained(base_model, peft_model_path)\n",
    "    LOGGER.info(\"Fine-tuned PEFT model loaded from %s\", peft_model_path)\n",
    "    return peft_model\n",
    "\n",
    "\n",
    "def load_module_state_dict_from(model_path, target_module_prefix):\n",
    "    index_file_path = os.path.join(model_path, \"model.safetensors.index.json\")\n",
    "    with open(index_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        sd_index = json.load(f)\n",
    "\n",
    "    target_model_file_paths = set()\n",
    "    for key, file_name in sd_index[\"weight_map\"].items():\n",
    "        if key.startswith(target_module_prefix):\n",
    "            target_model_file_paths.add(os.path.join(model_path, file_name))\n",
    "\n",
    "    target_state_dict = {}\n",
    "    for model_file_path in target_model_file_paths:\n",
    "        for name, param in load_file(model_file_path).items():\n",
    "            if name.startswith(target_module_prefix):\n",
    "                target_state_dict[name] = param\n",
    "\n",
    "    LOGGER.info(\"Loaded pretrained params: [%s] from file: %s\", target_state_dict.keys(), model_path)\n",
    "    return target_state_dict\n",
    "\n",
    "\n",
    "def load_state_dict_to_model(base_model, target_state_dict):\n",
    "    base_model.load_state_dict(target_state_dict, strict=False)\n",
    "\n",
    "    model_named_params = dict(base_model.named_parameters())\n",
    "    for n, p in target_state_dict.items():\n",
    "        assert torch.equal(model_named_params[n], p), f\"Model params update failed [{n}], expected: {p}, got:{model_named_params[n]}\"\n",
    "    LOGGER.info(\"Updated pretrained params to base model: [%s]\", target_state_dict.keys())\n",
    "\n",
    "    return base_model\n",
    "\n",
    "\n",
    "def load_model(model_path):\n",
    "    model = Vision2LanguageModel.from_pretrained(model_path)\n",
    "    LOGGER.info(\"Fine-tuned model loaded from %s\", model_path)\n",
    "    return model\n",
    "\n",
    "\n",
    "def load_processor(processor_path):\n",
    "    img_processor = AutoImageProcessor.from_pretrained(processor_path, use_fast=True)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(processor_path)\n",
    "    LOGGER.info(\"Image_processor and tokenizer are loaded from %s\", processor_path)\n",
    "    return img_processor, tokenizer\n",
    "\n",
    "\n",
    "def get_dataloaders(img_processor, tokenizer, ds_train=None, ds_valid=None, ds_test=None, train_bsz=1, eval_bsz=1, use_debug_subset=False):\n",
    "\n",
    "    train_dataloader, valid_dataloader, test_dataloader = None, None, None\n",
    "\n",
    "    if ds_train:\n",
    "        with ACCELERATOR.main_process_first():  # select是dataset caching 操作，主进程优先或许能快一点\n",
    "            if use_debug_subset:\n",
    "                train_dataset = ImageTextDataset(ds_train.select(range(len(ds_train) - 20, len(ds_train))), img_processor=img_processor, tokenizer=tokenizer, split=\"train\")\n",
    "            else:\n",
    "                train_dataset = ImageTextDataset(ds_train, img_processor=img_processor, tokenizer=tokenizer, split=\"train\")\n",
    "        train_dataloader = DataLoader(train_dataset, shuffle=True, collate_fn=lambda batch: collate_fn(batch, img_processor, tokenizer), batch_size=train_bsz, drop_last=True)\n",
    "\n",
    "    if ds_valid:\n",
    "        with ACCELERATOR.main_process_first():  # select是dataset caching 操作，主进程优先或许能快一点\n",
    "            if use_debug_subset:\n",
    "                vaild_dataset = ImageTextDataset(ds_valid.select(range(len(ds_valid) - 5, len(ds_valid))), img_processor=img_processor, tokenizer=tokenizer, split=\"validation\")\n",
    "            else:\n",
    "                vaild_dataset = ImageTextDataset(ds_valid, img_processor=img_processor, tokenizer=tokenizer, split=\"validation\")\n",
    "        valid_dataloader = DataLoader(vaild_dataset, shuffle=False, collate_fn=lambda batch: collate_fn(batch, img_processor, tokenizer, do_inference=True), batch_size=eval_bsz, drop_last=False)\n",
    "\n",
    "    if ds_test:\n",
    "        with ACCELERATOR.main_process_first():\n",
    "            if use_debug_subset:\n",
    "                test_dataset = ImageTextDataset(ds_test.select(range(len(ds_test) - 5, len(ds_test))), img_processor=img_processor, tokenizer=tokenizer, split=\"test\")\n",
    "            else:\n",
    "                test_dataset = ImageTextDataset(ds_test, img_processor=img_processor, tokenizer=tokenizer, split=\"test\")\n",
    "        test_dataloader = DataLoader(test_dataset, shuffle=False, collate_fn=lambda batch: collate_fn(batch, img_processor, tokenizer, do_inference=True), batch_size=eval_bsz, drop_last=False)\n",
    "\n",
    "    return train_dataloader, valid_dataloader, test_dataloader\n",
    "\n",
    "\n",
    "def load_preprocessed_dataset(ds_path):\n",
    "    ds_final = load_from_disk(ds_path)\n",
    "    LOGGER.info(\"Loaded pre_processed dataset dict: \\n%s\", ds_final)\n",
    "    return ds_final\n",
    "\n",
    "\n",
    "def post_init_model_and_tokenizer(model, tokenizer):\n",
    "    if len(tokenizer) >= model.config.decoder.vocab_size:\n",
    "        LOGGER.info(\"Decoder token_embedding [%d] and tokenizer [%d] size mismatch\", model.config.decoder.vocab_size, len(tokenizer))\n",
    "        model.decoder.resize_token_embeddings(len(tokenizer), pad_to_multiple_of=8, mean_resizing=True)\n",
    "        LOGGER.info(\"Decoder token_embedding resized to [%d] (pad_to_multiple_of=8)\", model.config.decoder.vocab_size)\n",
    "    else:\n",
    "        # 检查 tokenizer 的特殊 token 是否存在，用于判断 eval 是否加载了正确的 tokenizer\n",
    "        for spec_tok in GLOBAL_VARS.additional_special_tokens:\n",
    "            assert spec_tok in tokenizer.special_tokens_map[\"additional_special_tokens\"], f\"Missing special token: {spec_tok} in tokenizer, expect: {GLOBAL_VARS.additional_special_tokens}, got: {tokenizer.special_tokens_map['additional_special_tokens']} in tokenizer.\"\n",
    "\n",
    "    # 用于在 input_ids 中查找需要替换的图像占位符 <|image_token|>\n",
    "    if not hasattr(model.config, \"image_token_index\"):\n",
    "        model.config.image_token_index = tokenizer.convert_tokens_to_ids(\"<|image_token|>\")\n",
    "\n",
    "    # 计算 vision model 输出的图像特征的数量，该数量等于我们应该在 input_ids 中插入 <|image_token|> 的数量\n",
    "    img_size = model.config.encoder.image_size\n",
    "    dummy_img = torch.zeros((1, 3, img_size, img_size))\n",
    "    num_image_tokens = model.encoder(dummy_img).last_hidden_state.size(1)\n",
    "    GLOBAL_VARS.num_image_tokens = num_image_tokens\n",
    "\n",
    "\n",
    "def init_model(vision_model_path, language_model_path, model_base_cfg):\n",
    "    LOGGER.info(\"Initializing vision language mode: %s, %s\", vision_model_path, language_model_path)\n",
    "    model = Vision2LanguageModel.from_encoder_decoder_pretrained(vision_model_path, language_model_path)\n",
    "    return model\n",
    "\n",
    "\n",
    "def init_processor(vision_model_path, language_model_path, model_base_cfg):\n",
    "    LOGGER.info(\"Loading ImageProcessor from: %s\", vision_model_path)\n",
    "    img_processor = AutoImageProcessor.from_pretrained(vision_model_path, use_fast=True)\n",
    "\n",
    "    LOGGER.info(\"Loading Tokenizer from: %s\", language_model_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(language_model_path, use_fast=True)\n",
    "\n",
    "    # Add special tokens\n",
    "    LOGGER.info(\"Adding special tokens\")\n",
    "    bos_token = tokenizer.bos_token if tokenizer.bos_token else \"<BOS>\"\n",
    "    eos_token = tokenizer.eos_token if tokenizer.eos_token else \"<EOS>\"\n",
    "    pad_token = tokenizer.pad_token if tokenizer.pad_token else \"<PAD>\"\n",
    "    special_tokens_dict = {\n",
    "        \"bos_token\": bos_token,\n",
    "        \"eos_token\": eos_token,\n",
    "        \"pad_token\": pad_token,\n",
    "        \"additional_special_tokens\": [\"<|image_token|>\", \"<|image_start|>\", \"<|image_end|>\"],\n",
    "    }\n",
    "    tokenizer.add_special_tokens(special_tokens_dict)\n",
    "    # print special tokens and their ids\n",
    "    LOGGER.info(\"Special tokens: %s\", [(key, tok, tokenizer.convert_tokens_to_ids(tok)) for key, tok in tokenizer.special_tokens_map.items()])\n",
    "\n",
    "    # set chat template\n",
    "    assert tokenizer.chat_template == None, \"Tokenizer has chat_template, please check whether to use the existing one or our new chat template.\"\n",
    "    chat_template_path = model_base_cfg[\"chat_template\"]\n",
    "    LOGGER.info(\"Adding chat template to tokenizer from: %s\", chat_template_path)\n",
    "    with open(chat_template_path, \"r\") as f:\n",
    "        chat_template = \"\".join([line.strip() for line in f.readlines()])\n",
    "    tokenizer.chat_template = chat_template\n",
    "    LOGGER.info(\"Chat template: %s\", tokenizer.chat_template)\n",
    "\n",
    "    return img_processor, tokenizer\n",
    "\n",
    "\n",
    "def apply_peft_to_model(model):\n",
    "    # https://huggingface.co/docs/peft/developer_guides/troubleshooting#bad-results-from-a-loaded-peft-model\n",
    "\n",
    "    # named_modules = [(n, type(m)) for n, m in model.named_modules()]\n",
    "    # print([(n, type(m)) for n, m in model.named_modules()])\n",
    "\n",
    "    # The names of the modules to apply the adapter to.\n",
    "    # Also check TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING: https://github.com/huggingface/peft/blob/main/src/peft/utils/constants.py\n",
    "    # When the lora layers are applied to embedding layers, the corresponding base model embedding layers are also saved.\n",
    "    target_modules = [\"embed_tokens\", \"q_proj\", \"v_proj\", \"lm_head\"]  # 需要注入 LoRA 的模块。\n",
    "    # List of modules apart from adapter layers to be set as trainable and saved in the final checkpoint.\n",
    "    # e.g. Transformers adds a randomly initialized classification head on top of the model. If you do not add this layer to modules_to_save, the classification head won’t be saved. The next time you load the model, you’ll get a different randomly initialized classification head, resulting in completely different results.\n",
    "    modules_to_save = [\"v2l_projector\"]  # 没注入LoRA 但又需要训练和保存的模块。添加模块后，peft会包装一个一模一样的模块，并将requires_grad 会被设置为 True。原模块不变。\n",
    "    lora_config = LoraConfig(\n",
    "        target_modules=target_modules,\n",
    "        modules_to_save=modules_to_save,\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "        init_lora_weights=\"pissa_niter_16\",  # 不确定时：True 或 pissa 是最保险的起点；你想训练少轮就见效果：corda；做正式训练/部署，追求SOTA：eva（但初始化时要花点功夫）；想节省时间资源：pissa_niter_16；LoRA + 量化一起用：pissa / loftq；\n",
    "        # task_type=TaskType.CAUSAL_LM,\n",
    "    )\n",
    "    peft_model = get_peft_model(model, lora_config)\n",
    "    peft_model.print_trainable_parameters()\n",
    "    # LOGGER.debug(\"PEFT model trainable: %s\", \"\\n\".join([n for n, p in peft_model.named_parameters() if p.requires_grad == True]))\n",
    "\n",
    "    return peft_model\n",
    "\n",
    "\n",
    "def global_init_accelerator(model, fsdp_no_shard=False, **kwargs):\n",
    "    global ACCELERATOR, DEVICE, LOGGER\n",
    "\n",
    "    grad_accum_steps = kwargs[\"grad_accum_steps\"]\n",
    "    mixed_precision = kwargs[\"mixed_precision\"]\n",
    "\n",
    "    if isinstance(model, PeftModel):\n",
    "        # TODO 如何在不使用 ignore_modules 的情况下，让modules能够顺利的从flattened state_dict中恢复为unflattened的状态，然后保存\n",
    "        ignored_modules = []\n",
    "        for name, module in model.named_modules():\n",
    "            if isinstance(module, (nn.Conv2d, nn.Embedding, Dinov2Model, LlamaRMSNorm, LlamaRotaryEmbedding, lora.Embedding, lora.Linear)):\n",
    "                ignored_modules.append(module)\n",
    "        transformer_cls_names_to_wrap = [\"LlamaDecoderLayer\", \"Dinov2Layer\", \"VisionLanguageProjector\"]\n",
    "        sharding_strategy = \"FULL_SHARD\"\n",
    "        state_dict_type = \"SHARDED_STATE_DICT\"\n",
    "\n",
    "        # 关于 FSDP1 -> FSDP2 https://huggingface.co/docs/accelerate/main/en/concept_guides/fsdp1_vs_fsdp2\n",
    "    else:\n",
    "        ignored_modules = []\n",
    "        for name, module in model.named_modules():\n",
    "            if isinstance(module, (nn.Conv2d, nn.Embedding, Dinov2Model, LlamaRMSNorm, LlamaRotaryEmbedding)):\n",
    "                ignored_modules.append(module)\n",
    "        transformer_cls_names_to_wrap = [\"LlamaDecoderLayer\", \"Dinov2Layer\", \"VisionLanguageProjector\"]\n",
    "        sharding_strategy = \"FULL_SHARD\"\n",
    "        state_dict_type = \"SHARDED_STATE_DICT\"\n",
    "\n",
    "    # 如果不使用这段代码，我们在eval时会遇到 RuntimeError: mat2 must be a matrix, got 1-D tensor\n",
    "    # 可能是因为 PEFT modules_to_save 的部分与 FSDP 不兼容。目前的临时解决办法是改成 DDP\n",
    "    if fsdp_no_shard:\n",
    "        sharding_strategy = \"NO_SHARD\"\n",
    "        state_dict_type = \"FULL_STATE_DICT\"\n",
    "\n",
    "    # 关于 FSDP1 -> FSDP2 https://huggingface.co/docs/accelerate/main/en/concept_guides/fsdp1_vs_fsdp2\n",
    "    fsdp_plugin = FullyShardedDataParallelPlugin(\n",
    "        # mixed_precision_policy=mixed_precision,\n",
    "        sharding_strategy=sharding_strategy,  # FULL_SHARD=ZeRO3, SHARD_GRAD_OP=ZeRO2, NO_SHARD (DDP), HYBRID_SHARD, HYBRID_SHARD_ZERO2,\n",
    "        backward_prefetch=\"BACKWARD_PRE\",  # [1] BACKWARD_PRE 中等显存/通用场景, [2] BACKWARD_POST 显存充足/极致优化, [3] NO_PREFETCH 显存紧张\n",
    "        auto_wrap_policy=\"transformer_based_wrap\",  # transformer_based_wrap, size_based_wrap, or no_wrap\n",
    "        transformer_cls_names_to_wrap=transformer_cls_names_to_wrap,\n",
    "        ignored_modules=ignored_modules,\n",
    "        # transformer_layer_cls=int(1e6),\n",
    "        state_dict_type=state_dict_type,  # [1] FULL_STATE_DICT, [2] LOCAL_STATE_DICT, [3] SHARDED_STATE_DICT\n",
    "        use_orig_params=True,  # 设置为True才能手动调整params lr, requires_grad 等\n",
    "        cpu_offload=False,  # cpu_offload=True与FULL_SHARD组合可最大化显存节省，但通信开销最高。能节省5G的peak mem，但100iter从3s下降到5s\n",
    "        activation_checkpointing=False,  # A technique to reduce memory usage by clearing activations of certain layers and recomputing them during a backward pass. Effectively, this trades extra computation time for reduced memory usage. Will cause RuntimeError: The expanded size of the tensor (2896) must match the existing size (1448) at non-singleton dimension 3.  Target sizes: [2, 32, 1448, 2896].  Tensor sizes: [2, 1, 1448, 1448]\n",
    "        # cpu_ram_efficient_loading=True, #If True, only the first process loads the pretrained model checkoint while all other processes have empty weights. Only applicable for Transformers. When using this, sync_module_states needs to be True.\n",
    "        # sync_module_states=True,\n",
    "    )\n",
    "\n",
    "    # https://huggingface.co/docs/accelerate/v1.2.1/en/package_reference/utilities#accelerate.utils.GradientAccumulationPlugin\n",
    "    # 如果OOM，可以尝试设置 sync_each_batch=True，但是会导致训练速度变慢\n",
    "    # adjust_scheduler=False，我们在train方法中手动计算 scheduler 在使用梯度累计后的 step\n",
    "    ga_plugin = GradientAccumulationPlugin(\n",
    "        num_steps=grad_accum_steps,\n",
    "        adjust_scheduler=False,\n",
    "        sync_with_dataloader=True,\n",
    "        sync_each_batch=False,\n",
    "    )\n",
    "\n",
    "    dataloader_cfg = DataLoaderConfiguration(use_seedable_sampler=True)\n",
    "\n",
    "    ACCELERATOR = Accelerator(\n",
    "        mixed_precision=mixed_precision,\n",
    "        dataloader_config=dataloader_cfg,\n",
    "        gradient_accumulation_plugin=ga_plugin,\n",
    "        fsdp_plugin=fsdp_plugin,\n",
    "    )\n",
    "    DEVICE = ACCELERATOR.device\n",
    "\n",
    "    if ACCELERATOR.is_local_main_process:\n",
    "        datasets.utils.logging.set_verbosity_warning()\n",
    "        transformers.utils.logging.set_verbosity_info()\n",
    "    else:\n",
    "        datasets.utils.logging.set_verbosity_error()\n",
    "        transformers.utils.logging.set_verbosity_error()\n",
    "\n",
    "    LOGGER = MultiProcessAdapter(LOGGER, {})  # must initialize the accelerate state by calling either `PartialState()` or `Accelerator()` before using the logging utility.\n",
    "    LOGGER.info(\"Available cuda: %d\", torch.cuda.device_count())\n",
    "    LOGGER.info(\"Accelerator state: %s\", ACCELERATOR.state, main_process_only=False)\n",
    "    if hasattr(ACCELERATOR.state, \"fsdp_plugin\"):\n",
    "        LOGGER.info(\"FSDP sharding_strategy: %s\", ACCELERATOR.state.fsdp_plugin.sharding_strategy)\n",
    "        LOGGER.info(\"FSDP state_dict_type: %s\", ACCELERATOR.state.fsdp_plugin.state_dict_type)\n",
    "    LOGGER.info([i for i in CONFIG.items() if i[0][0] != \"_\"])\n",
    "\n",
    "\n",
    "def global_init_logger(log_level=logging.DEBUG, base_log_level=logging.WARNING, fsdp_log_level=logging.ERROR):\n",
    "    global LOGGER\n",
    "    logging.basicConfig(format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\", datefmt=\"%m/%d/%Y %H:%M:%S\", level=base_log_level)\n",
    "    logging.getLogger(\"torch.distributed.fsdp\").setLevel(fsdp_log_level)\n",
    "\n",
    "    log_file_mode = \"w\"\n",
    "    if CONFIG[\"resume_from_checkpoint\"]:\n",
    "        log_file_mode = \"a\"\n",
    "\n",
    "    curr_file_name = os.path.basename(os.path.abspath(__file__))\n",
    "    log_file_path = os.path.join(CONFIG[\"output_dir\"][\"result\"], f\"{curr_file_name}_{CONFIG['run_mode']}.log\")\n",
    "\n",
    "    file_handler = logging.FileHandler(log_file_path, log_file_mode)\n",
    "    file_formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\", datefmt=\"%m/%d/%Y %H:%M:%S\")\n",
    "    file_handler.setFormatter(file_formatter)\n",
    "\n",
    "    stream_handler = logging.StreamHandler()\n",
    "    stream_formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\", datefmt=\"%m/%d/%Y %H:%M:%S\")\n",
    "    stream_handler.setFormatter(stream_formatter)\n",
    "\n",
    "    LOGGER = logging.getLogger(curr_file_name)\n",
    "    LOGGER.addHandler(file_handler)\n",
    "    LOGGER.setLevel(log_level)  # This logger's level\n",
    "\n",
    "\n",
    "#############################################\n",
    "\n",
    "\n",
    "def pretrain_model(model_base_cfg, train_cfg):\n",
    "    \"\"\"pre-train the image_adaptor, freeze encoder and decoder\"\"\"\n",
    "    vision_model_path = CONFIG[\"model_name_or_path\"][model_base_cfg[\"vision_model\"]]\n",
    "    language_model_path = CONFIG[\"model_name_or_path\"][model_base_cfg[\"language_model\"]]\n",
    "\n",
    "    # Train and test\n",
    "    set_seed(train_cfg[\"seed\"])\n",
    "    ds_final = load_preprocessed_dataset(CONFIG[\"preprocess\"][\"cache_path\"])\n",
    "\n",
    "    img_processor, tokenizer = init_processor(vision_model_path, language_model_path, model_base_cfg)\n",
    "    model = init_model(vision_model_path, language_model_path, model_base_cfg)\n",
    "    post_init_model_and_tokenizer(model, tokenizer)\n",
    "    global_init_accelerator(model, **train_cfg)\n",
    "    check_memory()\n",
    "    model.to(DEVICE)\n",
    "    train_dataloader, _, _ = get_dataloaders(img_processor=img_processor, tokenizer=tokenizer, ds_train=ds_final[\"train\"], train_bsz=train_cfg[\"batch_size\"], use_debug_subset=CONFIG[\"use_debug_subset\"])\n",
    "    check_memory()\n",
    "\n",
    "    start = time.time()\n",
    "    train(model, train_dataloader, train_cfg=train_cfg)\n",
    "    end = time.time()\n",
    "    LOGGER.info(\"Total training time: %s\", seconds_to_time_str(end - start))\n",
    "\n",
    "    save_model(model, CONFIG[\"output_dir\"][\"model\"])\n",
    "    save_processors(img_processor=img_processor, tokenizer=tokenizer, output_dir=CONFIG[\"output_dir\"][\"model\"])\n",
    "\n",
    "\n",
    "def eval_pretrained_model(train_cfg):\n",
    "    pretain_model_path = CONFIG[\"output_dir\"][\"model\"]\n",
    "\n",
    "    # Train and test\n",
    "    set_seed(train_cfg[\"seed\"])\n",
    "    ds_final = load_preprocessed_dataset(CONFIG[\"preprocess\"][\"cache_path\"])\n",
    "\n",
    "    model = load_model(pretain_model_path)\n",
    "    img_processor, tokenizer = load_processor(pretain_model_path)\n",
    "    post_init_model_and_tokenizer(model, tokenizer)\n",
    "    global_init_accelerator(model, **train_cfg)\n",
    "    _, validation_dataloader, test_dataloader = get_dataloaders(img_processor=img_processor, tokenizer=tokenizer, ds_valid=ds_final[\"validation\"], ds_test=ds_final[\"test\"], use_debug_subset=CONFIG[\"use_debug_subset\"])\n",
    "    check_memory()\n",
    "    model.to(DEVICE)\n",
    "    model, validation_dataloader, test_dataloader = ACCELERATOR.prepare(model, validation_dataloader, test_dataloader)\n",
    "    check_memory()\n",
    "\n",
    "    start = time.time()\n",
    "    evaluate(model, validation_dataloader, output_result=True, **train_cfg)\n",
    "    start2 = time.time()\n",
    "    LOGGER.info(\"Valid time: %s\", seconds_to_time_str(start2 - start))\n",
    "\n",
    "    evaluate(model, test_dataloader, output_result=True, **train_cfg)\n",
    "    end = time.time()\n",
    "    LOGGER.info(\"Test time: %s\", seconds_to_time_str(end - start2))\n",
    "    LOGGER.info(\"Total evaluation time: %s\", seconds_to_time_str(end - start))\n",
    "\n",
    "\n",
    "def finetune_model(train_cfg):\n",
    "    \"\"\"use peft with fsdp to train image projector and decoder\"\"\"\n",
    "    pretain_model_path = train_cfg[\"pretain_model_path\"]\n",
    "\n",
    "    # Train and test\n",
    "    set_seed(train_cfg[\"seed\"])\n",
    "    ds_final = load_preprocessed_dataset(CONFIG[\"preprocess\"][\"cache_path\"])\n",
    "\n",
    "    img_processor, tokenizer = load_processor(pretain_model_path)\n",
    "    model_base_cfg = CONFIG[\"model\"]\n",
    "    vision_model_path = CONFIG[\"model_name_or_path\"][model_base_cfg[\"vision_model\"]]\n",
    "    language_model_path = CONFIG[\"model_name_or_path\"][model_base_cfg[\"language_model\"]]\n",
    "    model = init_model(vision_model_path, language_model_path, model_base_cfg)\n",
    "    target_state_dict = load_module_state_dict_from(model_path=pretain_model_path, target_module_prefix=\"v2l_projector\")\n",
    "    model = load_state_dict_to_model(base_model=model, target_state_dict=target_state_dict)\n",
    "    #####################\n",
    "\n",
    "    # model = load_model(pretain_model_path)\n",
    "    # LOGGER.debug(\"Loaded model dtype: %s\", model.dtype)\n",
    "    # img_processor, tokenizer = load_processor(pretain_model_path)\n",
    "    #####################\n",
    "\n",
    "    # # Train and test\n",
    "    # model_base_cfg = CONFIG[\"model\"]\n",
    "    # vision_model_path = CONFIG[\"model_name_or_path\"][model_base_cfg[\"vision_model\"]]\n",
    "    # language_model_path = CONFIG[\"model_name_or_path\"][model_base_cfg[\"language_model\"]]\n",
    "    # img_processor, tokenizer = init_processor(vision_model_path, language_model_path, model_base_cfg)\n",
    "    # model = init_model(vision_model_path, language_model_path, model_base_cfg)\n",
    "    # LOGGER.debug(\"Loaded model dtype: %s\", model.dtype)\n",
    "    #####################\n",
    "\n",
    "    post_init_model_and_tokenizer(model, tokenizer)\n",
    "\n",
    "    # Apply peft to model\n",
    "    model = apply_peft_to_model(model)\n",
    "\n",
    "    global_init_accelerator(model, **train_cfg)\n",
    "    check_memory()\n",
    "    model.to(DEVICE)\n",
    "    LOGGER.debug(\"Final model dtype: %s\", model.dtype)\n",
    "    train_dataloader, _, _ = get_dataloaders(img_processor=img_processor, tokenizer=tokenizer, ds_train=ds_final[\"train\"], train_bsz=train_cfg[\"batch_size\"], use_debug_subset=CONFIG[\"use_debug_subset\"])\n",
    "    check_memory()\n",
    "\n",
    "    start = time.time()\n",
    "    train(model, train_dataloader, train_cfg=train_cfg)\n",
    "    end = time.time()\n",
    "    LOGGER.info(\"Total training time: %s\", seconds_to_time_str(end - start))\n",
    "\n",
    "    save_model(model, CONFIG[\"output_dir\"][\"model\"])\n",
    "    save_processors(img_processor=img_processor, tokenizer=tokenizer, output_dir=CONFIG[\"output_dir\"][\"model\"])\n",
    "\n",
    "\n",
    "def eval_finetuned_model(train_cfg):\n",
    "    \"\"\"eval the peft model with FSDP set to NO_SHARD\"\"\"\n",
    "    pretain_model_path = train_cfg[\"pretain_model_path\"]\n",
    "    peft_model_path = CONFIG[\"output_dir\"][\"model\"]\n",
    "\n",
    "    # Train and test\n",
    "    set_seed(train_cfg[\"seed\"])\n",
    "    ds_final = load_preprocessed_dataset(CONFIG[\"preprocess\"][\"cache_path\"])\n",
    "\n",
    "    # Eval need to change accelerator fsdp sharding_strategy to no shard\n",
    "    model = load_model(pretain_model_path)\n",
    "    img_processor, tokenizer = load_processor(pretain_model_path)\n",
    "    #####################################\n",
    "\n",
    "    # model_base_cfg = CONFIG[\"model\"]\n",
    "    # vision_model_path = CONFIG[\"model_name_or_path\"][model_base_cfg[\"vision_model\"]]\n",
    "    # language_model_path = CONFIG[\"model_name_or_path\"][model_base_cfg[\"language_model\"]]\n",
    "    # img_processor, tokenizer = init_processor(vision_model_path, language_model_path, model_base_cfg)\n",
    "    # model = init_model(vision_model_path, language_model_path, model_base_cfg)\n",
    "    # LOGGER.debug(\"Loaded model dtype: %s\", model.dtype)\n",
    "    #####################################\n",
    "\n",
    "    post_init_model_and_tokenizer(model, tokenizer)\n",
    "\n",
    "    model = load_peft_model(base_model=model, peft_model_path=peft_model_path)\n",
    "\n",
    "    global_init_accelerator(model, fsdp_no_shard=True, **train_cfg)\n",
    "\n",
    "    _, validation_dataloader, test_dataloader = get_dataloaders(img_processor=img_processor, tokenizer=tokenizer, ds_valid=ds_final[\"validation\"], ds_test=ds_final[\"test\"], use_debug_subset=CONFIG[\"use_debug_subset\"])\n",
    "    check_memory()\n",
    "    model.to(DEVICE)\n",
    "    model, validation_dataloader, test_dataloader = ACCELERATOR.prepare(model, validation_dataloader, test_dataloader)\n",
    "    check_memory()\n",
    "\n",
    "    LOGGER.debug(\"Final model structure:\\n%s\", model)\n",
    "\n",
    "    start = time.time()\n",
    "    evaluate(model, validation_dataloader, output_result=True, **train_cfg)\n",
    "    start2 = time.time()\n",
    "    LOGGER.info(\"Valid time: %s\", seconds_to_time_str(start2 - start))\n",
    "\n",
    "    evaluate(model, test_dataloader, output_result=True, **train_cfg)\n",
    "    end = time.time()\n",
    "    LOGGER.info(\"Test time: %s\", seconds_to_time_str(end - start2))\n",
    "    LOGGER.info(\"Total evaluation time: %s\", seconds_to_time_str(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8af418f0-a5e5-40b5-aeea-028eed56716a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vision_model_path = \"/scratch/c.c21051562/resources/downloaded_models/rad-dino-maira-2\"\n",
    "language_model_path = \"/scratch/c.c21051562/resources/downloaded_models/Llama-3.2-1B\"\n",
    "init_model = Vision2LanguageModel.from_encoder_decoder_pretrained(vision_model_path, language_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ec8fcf8-6920-4922-8d67-3e3607660cba",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vision2LanguageModel(\n",
       "  (encoder): Dinov2Model(\n",
       "    (embeddings): Dinov2Embeddings(\n",
       "      (patch_embeddings): Dinov2PatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(14, 14), stride=(14, 14))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): Dinov2Encoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x Dinov2Layer(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attention): Dinov2SdpaAttention(\n",
       "            (attention): Dinov2SdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): Dinov2SelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (layer_scale1): Dinov2LayerScale()\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Dinov2MLP(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_scale2): Dinov2LayerScale()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): LlamaForCausalLM(\n",
       "    (model): LlamaModel(\n",
       "      (embed_tokens): Embedding(128256, 2048)\n",
       "      (layers): ModuleList(\n",
       "        (0-15): 16 x LlamaDecoderLayer(\n",
       "          (self_attn): LlamaAttention(\n",
       "            (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "            (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "            (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "            (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "            (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "          (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        )\n",
       "      )\n",
       "      (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       "  )\n",
       "  (v2l_projector): VisionLanguageProjector(\n",
       "    (linear_1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "    (act): SiLU()\n",
       "    (linear_2): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404df297-5292-4221-94e4-d010f0c5b49e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_fast` is set to `True` but the image processor class does not have a fast version.  Falling back to the slow version.\n"
     ]
    }
   ],
   "source": [
    "pretain_model_path = \"/scratch/c.c21051562/workspace/arrg_img2text/outputs/models/4_1_fsdo_peft_test_pretrain\"\n",
    "ds_final = load_preprocessed_dataset(\"/scratch/c.c21051562/workspace/arrg_img2text/dataset_cache/rad_dino_maira2_rbg518\")\n",
    "\n",
    "img_processor, tokenizer = load_processor(pretain_model_path)\n",
    "model_base_cfg = {\"chat_template\": \"/scratch/c.c21051562/workspace/arrg_img2text/llama3_chat_template.jinja\"}\n",
    "vision_model_path = \"/scratch/c.c21051562/resources/downloaded_models/rad-dino-maira-2\"\n",
    "language_model_path = \"/scratch/c.c21051562/resources/downloaded_models/Llama-3.2-1B\"\n",
    "model = init_model(vision_model_path, language_model_path, model_base_cfg)\n",
    "target_state_dict = load_module_state_dict_from(model_path=pretain_model_path, target_module_prefix=\"v2l_projector\")\n",
    "model = load_state_dict_to_model(base_model=model, target_state_dict=target_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b24dc3-d92f-45db-a785-bbc9dfed8b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    \"run_mode\": \"finetune\",\n",
    "    \"use_debug_subset\": True,\n",
    "}\n",
    "train_cfg = {\n",
    "    \"pretain_model_path\": \"/scratch/c.c21051562/workspace/arrg_img2text/outputs/models/4_1_fsdo_peft_test_pretrain\",\n",
    "    \"seed\": 42,\n",
    "    \"num_epochs\": 1,\n",
    "    \"batch_size\": 1,\n",
    "    \"grad_accum_steps\": 8,\n",
    "    \"warmup_proportion\": 0.1,\n",
    "    \"lr\": 0.0001,\n",
    "    \"clip_grad_norm\": 1.0,\n",
    "    \"mixed_precision\": \"bf16\",  # no, fp16, bf16. bf16 recommended for A100, fp16 for V100\n",
    "    \"print_loss_per_n_steps\": 1,  # logging loss on .log file\n",
    "    \"ckp_per_steps\": 1,  # 550395 / 32 = 17199, 170801 / 8 = 21350\n",
    "    \"eval_batch_size\": 1,\n",
    "    \"max_new_tokens\": 512,\n",
    "    \"print_pred_per_n_steps\": 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b55ab5f1-d9f3-4a85-8aad-bf431ca36006",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader, _, _ = get_dataloaders(img_processor=img_processor, tokenizer=tokenizer, ds_train=ds_final[\"train\"], train_bsz=train_cfg[\"batch_size\"], use_debug_subset=CONFIG[\"use_debug_subset\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bce585ad-8b76-4ba2-b3a7-25b8d1666a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "global_init_accelerator(model, **train_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ef3bf8de-86ed-49c0-b85a-73e0e72832ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = list(model.named_parameters())\n",
    "optimizer_grouped_parameters = prepare_optimizer_grouped_parameters(model_params, train_cfg)\n",
    "optimizer = AdamW(optimizer_grouped_parameters, eps=1e-8)\n",
    "total_num_steps = len(train_dataloader) // train_cfg[\"grad_accum_steps\"] * train_cfg[\"num_epochs\"]\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=int(total_num_steps * train_cfg[\"warmup_proportion\"]), num_training_steps=total_num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "180ee45f-b535-4b65-816e-b81214e180d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, train_dataloader, optimizer, scheduler = ACCELERATOR.prepare(model, train_dataloader, optimizer, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "88291ffe-a2f0-4496-913b-2f0ae094cbdd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AcceleratedOptimizer (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    initial_lr: 0.0001\n",
       "    lr: 0.0001\n",
       "    maximize: False\n",
       "    weight_decay: 0.0\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2ef3d1a-2eb1-4978-94f0-ee0afb2a784c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"/scratch/c.c21051562/workspace/arrg_img2text/outputs/models/4_2_ddp_peft_test_pretrain\", use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0cc438a3-9ab1-47b4-832f-64e14755a58e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|image_token|>', '<|image_start|>', '<|image_end|>']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.special_tokens_map[\"additional_special_tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec6f97c-a57e-4a56-b6f8-04d81fc7c34c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (arrg_img2text)",
   "language": "python",
   "name": "arrg_img2text"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
