{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8143331d-b0ef-4f33-be9f-6f51d865b6e3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import datetime\n",
    "import gc\n",
    "import glob\n",
    "import json\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "import time\n",
    "from collections import Counter\n",
    "from dataclasses import asdict, dataclass, field\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "from safetensors.torch import load_file\n",
    "import datasets\n",
    "import imagehash\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import requests\n",
    "import torch\n",
    "import transformers\n",
    "import yaml\n",
    "from accelerate import Accelerator, DataLoaderConfiguration\n",
    "from accelerate.logging import MultiProcessAdapter\n",
    "from accelerate.utils import (\n",
    "    DistributedDataParallelKwargs,\n",
    "    FullyShardedDataParallelPlugin,\n",
    "    GradientAccumulationPlugin,\n",
    "    gather,\n",
    "    gather_object,\n",
    ")\n",
    "from datasets import DatasetDict, concatenate_datasets, load_from_disk\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftModel,\n",
    "    TaskType,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    ")\n",
    "from peft.tuners import lora\n",
    "from peft.utils import AuxiliaryTrainingWrapper\n",
    "from PIL import Image\n",
    "from scipy.ndimage import zoom\n",
    "from scorers.scores import compute_scores\n",
    "from torch import nn\n",
    "from torch.distributed.fsdp import FullyShardedDataParallel, MixedPrecision\n",
    "from torch.distributed.fsdp.wrap import size_based_auto_wrap_policy\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, Dataset, SequentialSampler\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoImageProcessor,\n",
    "    AutoTokenizer,\n",
    "    Dinov2Config,\n",
    "    Dinov2Model,\n",
    "    LlamaConfig,\n",
    "    PretrainedConfig,\n",
    "    PreTrainedModel,\n",
    "    VisionEncoderDecoderModel,\n",
    "    get_cosine_schedule_with_warmup,\n",
    ")\n",
    "from transformers.generation import GenerationConfig\n",
    "from transformers.generation import utils as tf_generation_utils\n",
    "from transformers.modeling_outputs import BaseModelOutput, ModelOutput\n",
    "from transformers.models.dinov2.modeling_dinov2 import Dinov2Embeddings\n",
    "from transformers.models.llama.modeling_llama import LlamaRMSNorm, LlamaRotaryEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02075bde-2f26-4b52-bbe4-14f44cc2bc80",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Vision2LanguageOutputWithPast(ModelOutput):\n",
    "    loss: Optional[torch.FloatTensor] = None\n",
    "    logits: torch.FloatTensor = None\n",
    "    past_key_values: Optional[List[torch.FloatTensor]] = None\n",
    "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    image_hidden_states: Optional[torch.FloatTensor] = None\n",
    "\n",
    "\n",
    "class VisionLanguageProjector(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear_1 = nn.Linear(config.encoder_hidden_size, config.decoder_hidden_size, bias=True)\n",
    "        self.act = nn.SiLU()\n",
    "        self.linear_2 = nn.Linear(config.decoder_hidden_size, config.decoder_hidden_size, bias=True)\n",
    "\n",
    "    def forward(self, image_features):\n",
    "        hidden_states = self.linear_1(image_features)\n",
    "        hidden_states = self.act(hidden_states)\n",
    "        hidden_states = self.linear_2(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class Vision2LanguageModel(VisionEncoderDecoderModel):\n",
    "    def __init__(self, config=None, encoder=None, decoder=None):\n",
    "\n",
    "        super().__init__(config=config, encoder=encoder, decoder=decoder)\n",
    "        self.config.encoder_hidden_size = self.encoder.config.hidden_size\n",
    "        self.config.decoder_hidden_size = self.decoder.config.hidden_size\n",
    "\n",
    "        # replace enc_to_dec_proj with VisionLanguageProjector\n",
    "        self.v2l_projector = VisionLanguageProjector(self.config)\n",
    "        if hasattr(self, \"enc_to_dec_proj\"):\n",
    "            del self.enc_to_dec_proj  # 移除投影层\n",
    "\n",
    "    def _inject_image_features(self, input_ids, decoder_input_ids, image_features):\n",
    "        # image_indices_map 是一个嵌套list，每个样本对应一个list，list中的元素是图像在 last_hidden_state 中的索引\n",
    "        # e.g. [[0], [1], [2, 3], ...]\n",
    "\n",
    "        # replace img features with the <|image_token|> placeholder token in the input text\n",
    "        special_image_mask = (input_ids == self.config.image_token_index).unsqueeze(-1)\n",
    "        special_image_mask = special_image_mask.expand_as(decoder_input_ids).to(decoder_input_ids.device)\n",
    "\n",
    "        # 保证所有 image_features 都能够被复制到 decoder_input_ids 中\n",
    "        assert special_image_mask.sum() == image_features.numel(), f\"special_image_mask.sum()={special_image_mask.sum()}, image_features.numel()={image_features.numel()}, should be equal to guarantee that all image features are copied to decoder_input_ids\"\n",
    "\n",
    "        image_features = image_features.to(decoder_input_ids.device, decoder_input_ids.dtype)\n",
    "        decoder_input_ids = decoder_input_ids.masked_scatter(special_image_mask, image_features)\n",
    "\n",
    "        return decoder_input_ids\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        pixel_values: Optional[torch.FloatTensor] = None,\n",
    "        decoder_input_ids: Optional[torch.LongTensor] = None,\n",
    "        decoder_attention_mask: Optional[torch.BoolTensor] = None,\n",
    "        decoder_assistant_masks: Optional[torch.Tensor] = None,\n",
    "        encoder_outputs: Optional[Tuple[torch.FloatTensor]] = None,\n",
    "        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n",
    "        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = True,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "        logits_to_keep: Union[int, torch.Tensor] = 0,\n",
    "        output_loss: Optional[bool] = False,\n",
    "        **kwargs,\n",
    "    ) -> Union[Tuple, Vision2LanguageOutputWithPast]:\n",
    "        \"\"\"Additional args:\n",
    "        `decoder_inputs_embeds`: should represent the text embeddings with image features injected.\n",
    "        `encoder_outputs`: in inference statge, we encode `pixel_values` and get `encoder_outputs` outside this forward method. This is because the `pixel_values` and `decoder_input_ids` have different batch sizes, which cause error in generate().\n",
    "\n",
    "        If `output_loss` is True, by default we use `decoder_input_ids` as `labels`.\n",
    "        And the `decoder_assistant_masks` should be provided to compute the loss.\n",
    "        `decoder_assistant_masks` is provided by `tokenizer.apply_chat_template`.\n",
    "        `decoder_assistant_masks` is a tensor with the same shape as decoder_input_ids, and the value is 0 or 1. 0: system/user tokens, 1: assistant tokens, which is the tokens that need to be generated.\n",
    "        \"\"\"\n",
    "        LOGGER.debug(\"rank[%s], kwargs %s\", ACCELERATOR.process_index, kwargs)\n",
    "        LOGGER.debug(\"rank[%s], pixel_values: %s\", ACCELERATOR.process_index, pixel_values.shape if pixel_values is not None else None)\n",
    "        LOGGER.debug(\"rank[%s], decoder_input_ids: %s\", ACCELERATOR.process_index, decoder_input_ids)\n",
    "        LOGGER.debug(\"rank[%s], decoder_attention_mask: %s\", ACCELERATOR.process_index, decoder_attention_mask.shape)\n",
    "        LOGGER.debug(\"rank[%s], encoder_outputs.last_hidden_state: %s\", ACCELERATOR.process_index, encoder_outputs.last_hidden_state.shape if encoder_outputs is not None else None)\n",
    "        LOGGER.debug(\"rank[%s], past_key_values: %s\", ACCELERATOR.process_index, past_key_values)\n",
    "        LOGGER.debug(\"rank[%s], decoder_inputs_embeds: %s\", ACCELERATOR.process_index, decoder_inputs_embeds)\n",
    "        LOGGER.debug(\"rank[%s], position_ids: %s\", ACCELERATOR.process_index, position_ids)\n",
    "        LOGGER.debug(\"rank[%s], logits_to_keep: %s\", ACCELERATOR.process_index, logits_to_keep)\n",
    "\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "\n",
    "        # train时，有pixel_values，没有encoder_outputs\n",
    "        # inference时，没有pixel_values，有encoder_outputs；encoder_outputs只有第一轮才需要，后续需要忽略\n",
    "        if (pixel_values is not None) and (encoder_outputs is not None):\n",
    "            raise ValueError(\"You must not specify both pixel_values and encoder_outputs.\")\n",
    "\n",
    "        # 我们目前没有使用过 decoder_inputs_embeds\n",
    "        if (decoder_input_ids is None) ^ (decoder_inputs_embeds is not None):\n",
    "            raise ValueError(\"You must specify exactly one of decoder_input_ids or decoder_inputs_embeds\")\n",
    "\n",
    "        if (pixel_values is not None or encoder_outputs is not None) and decoder_inputs_embeds is not None:\n",
    "            raise ValueError(\"You cannot specify both `pixel_values`/`encoder_outputs` and `decoder_inputs_embeds` at the same time, and must specify either one\")\n",
    "\n",
    "        if decoder_inputs_embeds is None:\n",
    "            # get text embeddings\n",
    "            decoder_inputs_embeds = self.decoder.get_input_embeddings()(decoder_input_ids)\n",
    "\n",
    "        # 如果有encoder_outputs，就不需要再次 encode pixel_values\n",
    "        if (pixel_values is not None) and (encoder_outputs is None):\n",
    "            # get img features\n",
    "            encoder_outputs = self.encoder(pixel_values=pixel_values, return_dict=True)\n",
    "\n",
    "        # train forward 以及 inference first round，需要进行这一步\n",
    "        # train forward 会提供 pixel_values\n",
    "        # inference all rounds 会提供 encoder_outputs，而pixel_values=None；在first round时，past_key_values=None，后续为past_key_values=DynamicCache()\n",
    "        if encoder_outputs is not None and past_key_values is None:\n",
    "            image_features = encoder_outputs.last_hidden_state  # torch.Size([4, 1370, enc_dim])\n",
    "            # project image features\n",
    "            LOGGER.debug(\"rank[%s], v2lmodel forward image_features shape: %s\", ACCELERATOR.process_index, image_features.shape)\n",
    "            image_features = self.v2l_projector(image_features)\n",
    "            # inject image features into text embeddings\n",
    "            decoder_inputs_embeds = self._inject_image_features(decoder_input_ids, decoder_inputs_embeds, image_features)\n",
    "\n",
    "        # Text generation. decoder_inputs_embeds is used in replace of decoder_input_ids on decoder in all cases.\n",
    "        # In train statge, decoder_input_ids is encoded into decoder_inputs_embeds and then merged with image features.\n",
    "        # In inference stage, encoder_outputs is passed from generate() in replace of pixel_values.\n",
    "        decoder_outputs = self.decoder(\n",
    "            attention_mask=decoder_attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=decoder_inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=True,\n",
    "            cache_position=cache_position,\n",
    "            logits_to_keep=logits_to_keep,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        logits = decoder_outputs.logits\n",
    "\n",
    "        # text loss\n",
    "        loss = None\n",
    "        if output_loss:\n",
    "            labels = labels if labels is not None else decoder_input_ids\n",
    "\n",
    "            # Shift so that tokens < n predict n\n",
    "            if decoder_assistant_masks is not None:\n",
    "                shift_label_mask = decoder_assistant_masks[:, 1:]  # torch.Size([bsz, seq_len - 1])\n",
    "            elif decoder_attention_mask is not None:\n",
    "                shift_label_mask = decoder_attention_mask[:, 1:]\n",
    "            else:\n",
    "                raise ValueError(\"decoder_assistant_masks or decoder_attention_mask should be provided\")\n",
    "\n",
    "            shift_logits = logits[:, :-1, :]  # torch.Size([bsz, seq_len - 1, vocab_size])\n",
    "            shift_labels = labels[:, 1:]  # torch.Size([bsz, seq_len - 1])\n",
    "            active_shift_logits = shift_logits[shift_label_mask != 0].contiguous()  # torch.Size([num_acitve_labels, vocab_size])\n",
    "            active_shift_labels = shift_labels[shift_label_mask != 0].contiguous()  # torch.Size([num_acitve_labels])\n",
    "\n",
    "            ce_loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = ce_loss_fct(active_shift_logits, active_shift_labels)\n",
    "\n",
    "        return Vision2LanguageOutputWithPast(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            past_key_values=decoder_outputs.past_key_values,\n",
    "            hidden_states=decoder_outputs.hidden_states,\n",
    "            attentions=decoder_outputs.attentions,\n",
    "            image_hidden_states=image_features if pixel_values is not None else None,\n",
    "        )\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(\n",
    "        self,\n",
    "        inputs,\n",
    "        generation_config=None,\n",
    "        logits_processor=None,\n",
    "        stopping_criteria=None,\n",
    "        prefix_allowed_tokens_fn=None,\n",
    "        synced_gpus=None,\n",
    "        assistant_model=None,\n",
    "        streamer=None,\n",
    "        negative_prompt_ids=None,\n",
    "        negative_prompt_attention_mask=None,\n",
    "        **kwargs,  # If the model is an encoder-decoder model, encoder specific kwargs should not be prefixed and decoder specific kwargs should be prefixed with decoder_.\n",
    "    ):\n",
    "        # 1. Handle `generation_config` and kwargs that might update it, and validate the `.generate()` call\n",
    "        self._validate_model_class()\n",
    "        tokenizer = kwargs.pop(\"tokenizer\", None)  # Pull this out first, we only use it for stopping criteria\n",
    "        assistant_tokenizer = kwargs.pop(\"assistant_tokenizer\", None)  # only used for assisted generation\n",
    "        LOGGER.debug(\"rank[%s], step1\", ACCELERATOR.process_index)\n",
    "        LOGGER.debug(\"rank[%s], tokenizer %s\", ACCELERATOR.process_index, tokenizer)\n",
    "        LOGGER.debug(\"rank[%s], assistant_tokenizer: %s\", ACCELERATOR.process_index, assistant_tokenizer)\n",
    "\n",
    "        generation_config, model_kwargs = self._prepare_generation_config(generation_config, **kwargs)\n",
    "        self._validate_model_kwargs(model_kwargs.copy())\n",
    "        self._validate_assistant(assistant_model, tokenizer, assistant_tokenizer)\n",
    "        LOGGER.debug(\"rank[%s], generation_config: %s\", ACCELERATOR.process_index, generation_config)\n",
    "        LOGGER.debug(\"rank[%s], model_kwargs step1: %s\", ACCELERATOR.process_index, model_kwargs)\n",
    "        LOGGER.debug(\"rank[%s], decoder_input_ids: %s\", ACCELERATOR.process_index, model_kwargs[\"decoder_input_ids\"].shape)\n",
    "        LOGGER.debug(\"rank[%s], decoder_attention_mask: %s\", ACCELERATOR.process_index, model_kwargs[\"decoder_attention_mask\"].shape)\n",
    "\n",
    "        # 2. Set generation parameters if not already defined\n",
    "        if synced_gpus is None:\n",
    "            synced_gpus = (tf_generation_utils.is_deepspeed_zero3_enabled() or tf_generation_utils.is_fsdp_managed_module(self)) and tf_generation_utils.dist.get_world_size() > 1\n",
    "        LOGGER.debug(\"rank[%s], step2\", ACCELERATOR.process_index)\n",
    "        LOGGER.debug(\"rank[%s], synced_gpus: %s (should be True)\", ACCELERATOR.process_index, synced_gpus, main_process_only=False)\n",
    "\n",
    "        logits_processor = logits_processor if logits_processor is not None else tf_generation_utils.LogitsProcessorList()\n",
    "        stopping_criteria = stopping_criteria if stopping_criteria is not None else tf_generation_utils.StoppingCriteriaList()\n",
    "        LOGGER.debug(\"rank[%s], logits_processor: %s\", ACCELERATOR.process_index, logits_processor)\n",
    "        LOGGER.debug(\"rank[%s], stopping_criteria: %s\", ACCELERATOR.process_index, stopping_criteria)\n",
    "\n",
    "        accepts_attention_mask = \"attention_mask\" in set(tf_generation_utils.inspect.signature(self.forward).parameters.keys())\n",
    "        requires_attention_mask = \"encoder_outputs\" not in model_kwargs\n",
    "        kwargs_has_attention_mask = model_kwargs.get(\"attention_mask\", None) is not None\n",
    "        LOGGER.debug(\"rank[%s], accepts_attention_mask: %s\", ACCELERATOR.process_index, accepts_attention_mask)\n",
    "        LOGGER.debug(\"rank[%s], requires_attention_mask: %s\", ACCELERATOR.process_index, requires_attention_mask)\n",
    "        LOGGER.debug(\"rank[%s], kwargs_has_attention_mask: %s\", ACCELERATOR.process_index, kwargs_has_attention_mask)\n",
    "\n",
    "        # 3. Define model inputs\n",
    "        inputs_tensor, model_input_name, model_kwargs = self._prepare_model_inputs(inputs, generation_config.bos_token_id, model_kwargs)\n",
    "        # batch_size = inputs_tensor.shape[0]\n",
    "        # encoder和decoder的bsz可能不一样，我们以decoder的bsz为准\n",
    "        batch_size = model_kwargs[\"decoder_input_ids\"].shape[0]\n",
    "        LOGGER.debug(\"rank[%s], step3\", ACCELERATOR.process_index)\n",
    "        LOGGER.debug(\"rank[%s], inputs_tensor: %s\", ACCELERATOR.process_index, inputs_tensor.shape)\n",
    "        LOGGER.debug(\"rank[%s], model_input_name: %s\", ACCELERATOR.process_index, model_input_name)\n",
    "        LOGGER.debug(\"rank[%s], model_kwargs step3: %s\", ACCELERATOR.process_index, model_kwargs)\n",
    "        LOGGER.debug(\"rank[%s], batch_size: %s\", ACCELERATOR.process_index, batch_size)\n",
    "\n",
    "        device = inputs_tensor.device\n",
    "        self._prepare_special_tokens(generation_config, kwargs_has_attention_mask, device=device)\n",
    "\n",
    "        # decoder-only models must use left-padding for batched generation.\n",
    "        LOGGER.debug(\"rank[%s], self.config.is_encoder_decoder %s\", ACCELERATOR.process_index, self.config.is_encoder_decoder)\n",
    "        if not self.config.is_encoder_decoder and not tf_generation_utils.is_torchdynamo_compiling():\n",
    "            # If `input_ids` was given, check if the last id in any sequence is `pad_token_id`\n",
    "            # Note: If using, `inputs_embeds` this check does not work, because we want to be more hands-off.\n",
    "            LOGGER.warning(\"Should not see this warning!!! A decoder-only architecture is detected, while we are using encoder-decoder model.\")\n",
    "            if generation_config._pad_token_tensor is not None and batch_size > 1 and len(inputs_tensor.shape) == 2 and torch.sum(inputs_tensor[:, -1] == generation_config._pad_token_tensor) > 0:\n",
    "                LOGGER.warning(\"A decoder-only architecture is being used, but right-padding was detected! For correct \" \"generation results, please set `padding_side='left'` when initializing the tokenizer.\")\n",
    "\n",
    "        # 4. Define other model kwargs\n",
    "        # decoder-only models with inputs_embeds forwarding must use caching (otherwise we can't detect whether we are\n",
    "        # generating the first new token or not, and we only want to use the embeddings for the first new token)\n",
    "        LOGGER.debug(\"rank[%s], step4\", ACCELERATOR.process_index)\n",
    "        LOGGER.debug(\"rank[%s], Conv2D weight shape: %s\", ACCELERATOR.process_index, self.encoder.embeddings.patch_embeddings.projection.weight.shape, main_process_only=False)\n",
    "        if not self.config.is_encoder_decoder and model_input_name == \"inputs_embeds\":\n",
    "            generation_config.use_cache = True\n",
    "\n",
    "        if not kwargs_has_attention_mask and requires_attention_mask and accepts_attention_mask:\n",
    "            model_kwargs[\"attention_mask\"] = self._prepare_attention_mask_for_generation(inputs_tensor, generation_config, model_kwargs)\n",
    "            LOGGER.debug(\"rank[%s], model_kwargs['attention_mask']: %s\", ACCELERATOR.process_index, model_kwargs[\"attention_mask\"].shape)\n",
    "        elif kwargs_has_attention_mask:\n",
    "            # TODO (joao): generalize this check with other types of inputs\n",
    "            if model_input_name == \"input_ids\" and len(model_kwargs[\"attention_mask\"].shape) > 2:\n",
    "                raise ValueError(\"`attention_mask` passed to `generate` must be 2D.\")\n",
    "\n",
    "        if self.config.is_encoder_decoder and \"encoder_outputs\" not in model_kwargs:\n",
    "            # if model is encoder decoder encoder_outputs are created and added to `model_kwargs`\n",
    "            model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(inputs_tensor, model_kwargs, model_input_name, generation_config)\n",
    "            LOGGER.debug(\"rank[%s], model_kwargs step4: %s\", ACCELERATOR.process_index, model_kwargs)\n",
    "            LOGGER.debug(\"rank[%s], model_kwargs['encoder_outputs'].last_hidden_state: %s\", ACCELERATOR.process_index, model_kwargs[\"encoder_outputs\"].last_hidden_state.shape)\n",
    "            LOGGER.debug(\"rank[%s], model_kwargs['encoder_outputs'].pooler_output: %s\", ACCELERATOR.process_index, model_kwargs[\"encoder_outputs\"].pooler_output.shape)\n",
    "\n",
    "        # 5. Prepare `input_ids` which will be used for auto-regressive generation\n",
    "        LOGGER.debug(\"rank[%s], step5\", ACCELERATOR.process_index)\n",
    "        if self.config.is_encoder_decoder:\n",
    "            LOGGER.debug(\"rank[%s], model_input_name: %s\", ACCELERATOR.process_index, model_input_name)\n",
    "            LOGGER.debug(\"rank[%s], before decoder_start_token_id: %s\", ACCELERATOR.process_index, generation_config._decoder_start_token_tensor)\n",
    "            # 原始方法，当input_ids不是以decoder_start_token_id开头时，添加decoder_start_token_id\n",
    "            # 更新后的方法，当input_ids不是以decoder_start_token_id 或 pad_token_id 开头时，添加decoder_start_token_id\n",
    "            # 因为我们在collect_fn中，会将input_ids以8的倍数填充left padding，然后紧跟着decoder_start_token_id和正文\n",
    "            input_ids, model_kwargs = self._prepare_decoder_input_ids_for_generation(\n",
    "                batch_size=batch_size,\n",
    "                model_input_name=model_input_name,\n",
    "                model_kwargs=model_kwargs,\n",
    "                decoder_start_token_id=generation_config._decoder_start_token_tensor,\n",
    "                pad_token_id=torch.tensor(generation_config.pad_token_id, device=inputs_tensor.device),\n",
    "                device=inputs_tensor.device,\n",
    "            )\n",
    "            LOGGER.debug(\"rank[%s], input_ids: %s\", ACCELERATOR.process_index, input_ids.shape)\n",
    "            LOGGER.debug(\"rank[%s], input_ids: %s\", ACCELERATOR.process_index, input_ids.tolist())\n",
    "            LOGGER.debug(\"rank[%s], model_kwargs step5: %s\", ACCELERATOR.process_index, model_kwargs)\n",
    "        else:\n",
    "            input_ids = inputs_tensor if model_input_name == \"input_ids\" else model_kwargs.pop(\"input_ids\")\n",
    "\n",
    "        if generation_config.token_healing:\n",
    "            input_ids = self.heal_tokens(input_ids, tokenizer)\n",
    "\n",
    "        if streamer is not None:\n",
    "            streamer.put(input_ids.cpu())\n",
    "\n",
    "        # 6. Prepare `max_length` depending on other stopping criteria.\n",
    "        LOGGER.debug(\"rank[%s], step6\", ACCELERATOR.process_index)\n",
    "        input_ids_length = input_ids.shape[-1]\n",
    "        has_default_max_length = kwargs.get(\"max_length\") is None and generation_config.max_length is not None\n",
    "        has_default_min_length = kwargs.get(\"min_length\") is None and generation_config.min_length is not None\n",
    "        generation_config = self._prepare_generated_length(\n",
    "            generation_config=generation_config,\n",
    "            has_default_max_length=has_default_max_length,\n",
    "            has_default_min_length=has_default_min_length,\n",
    "            model_input_name=model_input_name,\n",
    "            inputs_tensor=inputs_tensor,\n",
    "            input_ids_length=input_ids_length,\n",
    "        )\n",
    "        LOGGER.debug(\"rank[%s], input_ids_length: %s\", ACCELERATOR.process_index, input_ids_length)\n",
    "        LOGGER.debug(\"rank[%s], has_default_max_length: %s\", ACCELERATOR.process_index, has_default_max_length)\n",
    "        LOGGER.debug(\"rank[%s], has_default_min_length: %s\", ACCELERATOR.process_index, has_default_min_length)\n",
    "        LOGGER.debug(\"rank[%s], generation_config: %s\", ACCELERATOR.process_index, type(generation_config))\n",
    "\n",
    "        # If the model supports `logits_to_keep` in forward(), set it to 1 to avoid computing the whole\n",
    "        # logit matrix. This can save a lot of memory during the first forward pass. Note that assisted decoding\n",
    "        # dynamically overrides this value as it can need more than the last token logits\n",
    "        if self._supports_logits_to_keep() and \"logits_to_keep\" not in model_kwargs:\n",
    "            model_kwargs[\"logits_to_keep\"] = 1\n",
    "            LOGGER.debug(\"rank[%s], model_kwargs step6: %s\", ACCELERATOR.process_index, model_kwargs)\n",
    "\n",
    "        self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)\n",
    "\n",
    "        # 7. Prepare the cache.\n",
    "        # - `model_kwargs` may be updated in place with a cache as defined by the parameters in `generation_config`.\n",
    "        # - different models have a different cache name expected by the model (default = \"past_key_values\")\n",
    "        # - `max_length`, prepared above, is used to determine the maximum cache length\n",
    "        max_cache_length = generation_config.max_length - 1\n",
    "        if inputs_tensor.shape[1] != input_ids_length and model_input_name == \"inputs_embeds\" and not self.config.is_encoder_decoder:\n",
    "            max_cache_length += inputs_tensor.shape[1]\n",
    "        self._prepare_cache_for_generation(generation_config, model_kwargs, assistant_model, batch_size, max_cache_length, device)\n",
    "\n",
    "        # 8. determine generation mode\n",
    "        LOGGER.debug(\"rank[%s], step8\", ACCELERATOR.process_index)\n",
    "        generation_mode = generation_config.get_generation_mode(assistant_model)\n",
    "        LOGGER.debug(\"rank[%s], generation_mode %s\", ACCELERATOR.process_index, generation_mode)\n",
    "\n",
    "        if streamer is not None and (generation_config.num_beams > 1):\n",
    "            raise ValueError(\"`streamer` cannot be used with beam search (yet!). Make sure that `num_beams` is set to 1.\")\n",
    "\n",
    "        if not tf_generation_utils.is_torchdynamo_compiling() and self.device.type != input_ids.device.type:\n",
    "            tf_generation_utils.warnings.warn(\n",
    "                \"You are calling .generate() with the `input_ids` being on a device type different\" f\" than your model's device. `input_ids` is on {input_ids.device.type}, whereas the model\" f\" is on {self.device.type}. You may experience unexpected behaviors or slower generation.\" \" Please make sure that you have put `input_ids` to the\" f\" correct device by calling for example input_ids = input_ids.to('{self.device.type}') before\" \" running `.generate()`.\",\n",
    "                UserWarning,\n",
    "            )\n",
    "\n",
    "        # 9. prepare logits processors and stopping criteria\n",
    "        prepared_logits_processor = self._get_logits_processor(\n",
    "            generation_config=generation_config,\n",
    "            input_ids_seq_length=input_ids_length,\n",
    "            encoder_input_ids=inputs_tensor,\n",
    "            prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,\n",
    "            logits_processor=logits_processor,\n",
    "            device=inputs_tensor.device,\n",
    "            model_kwargs=model_kwargs,\n",
    "            negative_prompt_ids=negative_prompt_ids,\n",
    "            negative_prompt_attention_mask=negative_prompt_attention_mask,\n",
    "        )\n",
    "        prepared_stopping_criteria = self._get_stopping_criteria(generation_config=generation_config, stopping_criteria=stopping_criteria, tokenizer=tokenizer, **kwargs)\n",
    "\n",
    "        # Set model_kwargs `use_cache` so we can use it later in forward runs\n",
    "        model_kwargs[\"use_cache\"] = generation_config.use_cache\n",
    "        LOGGER.debug(\"rank[%s], model_kwargs step9: %s\", ACCELERATOR.process_index, model_kwargs)\n",
    "\n",
    "        # 10. go into different generation modes\n",
    "        result = None\n",
    "        if generation_mode == tf_generation_utils.GenerationMode.ASSISTED_GENERATION:\n",
    "            if generation_config.num_return_sequences > 1:\n",
    "                raise ValueError(\"num_return_sequences has to be 1 when doing assisted generate, \" f\"but is {generation_config.num_return_sequences}.\")\n",
    "            if batch_size > 1:\n",
    "                raise ValueError(\"assisted generate is only supported for batch_size = 1\")\n",
    "            if not model_kwargs[\"use_cache\"]:\n",
    "                raise ValueError(\"assisted generate requires `use_cache=True`\")\n",
    "            if generation_config.cache_implementation in [\"static\", \"hybrid\", \"sliding_window\"]:\n",
    "                raise ValueError(\"assisted generate is not supported with Static cache classes`\")\n",
    "            if self._is_stateful:\n",
    "                # In assisted generation we need the ability to confirm whether the model would pick certain tokens,\n",
    "                # which is not possible with stateful models (they can't reset to a previous subset of generated text)\n",
    "                raise ValueError(f\"assisted generation is not supported with stateful models, such as {self.__class__.__name__}\")\n",
    "\n",
    "            # 11. Get the candidate generator, given the parameterization\n",
    "            candidate_generator = self._get_candidate_generator(\n",
    "                generation_config=generation_config,\n",
    "                input_ids=input_ids,\n",
    "                inputs_tensor=inputs_tensor,\n",
    "                assistant_model=assistant_model,\n",
    "                logits_processor=logits_processor,\n",
    "                target_tokenizer=tokenizer,\n",
    "                assistant_tokenizer=assistant_tokenizer,\n",
    "                model_kwargs=model_kwargs,\n",
    "            )\n",
    "\n",
    "            # 12. run assisted generate\n",
    "            result = self._assisted_decoding(\n",
    "                input_ids,\n",
    "                candidate_generator=candidate_generator,\n",
    "                logits_processor=prepared_logits_processor,\n",
    "                stopping_criteria=prepared_stopping_criteria,\n",
    "                generation_config=generation_config,\n",
    "                synced_gpus=synced_gpus,\n",
    "                streamer=streamer,\n",
    "                **model_kwargs,\n",
    "            )\n",
    "        elif generation_mode == tf_generation_utils.GenerationMode.DOLA_GENERATION:\n",
    "            if self._is_stateful:\n",
    "                # DoLa decoding was not designed for stateful models, and would require some changes\n",
    "                raise ValueError(f\"dola decoding is not supported with stateful models, such as {self.__class__.__name__}\")\n",
    "            result = self._dola_decoding(\n",
    "                input_ids,\n",
    "                dola_layers=generation_config.dola_layers,\n",
    "                logits_processor=prepared_logits_processor,\n",
    "                stopping_criteria=prepared_stopping_criteria,\n",
    "                generation_config=generation_config,\n",
    "                synced_gpus=synced_gpus,\n",
    "                streamer=streamer,\n",
    "                **model_kwargs,\n",
    "            )\n",
    "\n",
    "        elif generation_mode == tf_generation_utils.GenerationMode.CONTRASTIVE_SEARCH:\n",
    "            if not model_kwargs[\"use_cache\"]:\n",
    "                raise ValueError(\"Contrastive search requires `use_cache=True`\")\n",
    "            if self._is_stateful:\n",
    "                # Just like assisted generation, we need to be able to rollback to a previous state (see comment above)\n",
    "                raise ValueError(f\"contrastive search is not supported with stateful models, such as {self.__class__.__name__}\")\n",
    "\n",
    "            result = self._contrastive_search(\n",
    "                input_ids,\n",
    "                logits_processor=prepared_logits_processor,\n",
    "                stopping_criteria=prepared_stopping_criteria,\n",
    "                generation_config=generation_config,\n",
    "                synced_gpus=synced_gpus,\n",
    "                streamer=streamer,\n",
    "                **model_kwargs,\n",
    "            )\n",
    "\n",
    "        elif generation_mode in (tf_generation_utils.GenerationMode.SAMPLE, tf_generation_utils.GenerationMode.GREEDY_SEARCH):\n",
    "            # 11. expand input_ids with `num_return_sequences` additional sequences per batch\n",
    "            input_ids, model_kwargs = self._expand_inputs_for_generation(\n",
    "                input_ids=input_ids,\n",
    "                expand_size=generation_config.num_return_sequences,\n",
    "                is_encoder_decoder=self.config.is_encoder_decoder,\n",
    "                **model_kwargs,\n",
    "            )\n",
    "\n",
    "            # 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\n",
    "            result = self._sample(\n",
    "                input_ids,\n",
    "                logits_processor=prepared_logits_processor,\n",
    "                stopping_criteria=prepared_stopping_criteria,\n",
    "                generation_config=generation_config,\n",
    "                synced_gpus=synced_gpus,\n",
    "                streamer=streamer,\n",
    "                **model_kwargs,\n",
    "            )\n",
    "\n",
    "        elif generation_mode in (tf_generation_utils.GenerationMode.BEAM_SAMPLE, tf_generation_utils.GenerationMode.BEAM_SEARCH):\n",
    "            # 11. prepare beam search scorer\n",
    "            beam_scorer = tf_generation_utils.BeamSearchScorer(\n",
    "                batch_size=batch_size,\n",
    "                num_beams=generation_config.num_beams,\n",
    "                device=inputs_tensor.device,\n",
    "                length_penalty=generation_config.length_penalty,\n",
    "                do_early_stopping=generation_config.early_stopping,\n",
    "                num_beam_hyps_to_keep=generation_config.num_return_sequences,\n",
    "                max_length=generation_config.max_length,\n",
    "            )\n",
    "\n",
    "            # 12. interleave input_ids with `num_beams` additional sequences per batch\n",
    "            input_ids, model_kwargs = self._expand_inputs_for_generation(\n",
    "                input_ids=input_ids,\n",
    "                expand_size=generation_config.num_beams,\n",
    "                is_encoder_decoder=self.config.is_encoder_decoder,\n",
    "                **model_kwargs,\n",
    "            )\n",
    "\n",
    "            # 13. run beam sample\n",
    "            result = self._beam_search(\n",
    "                input_ids,\n",
    "                beam_scorer,\n",
    "                logits_processor=prepared_logits_processor,\n",
    "                stopping_criteria=prepared_stopping_criteria,\n",
    "                generation_config=generation_config,\n",
    "                synced_gpus=synced_gpus,\n",
    "                **model_kwargs,\n",
    "            )\n",
    "\n",
    "        elif generation_mode == tf_generation_utils.GenerationMode.GROUP_BEAM_SEARCH:\n",
    "            # 11. prepare beam search scorer\n",
    "            beam_scorer = tf_generation_utils.BeamSearchScorer(\n",
    "                batch_size=batch_size,\n",
    "                num_beams=generation_config.num_beams,\n",
    "                device=inputs_tensor.device,\n",
    "                length_penalty=generation_config.length_penalty,\n",
    "                do_early_stopping=generation_config.early_stopping,\n",
    "                num_beam_hyps_to_keep=generation_config.num_return_sequences,\n",
    "                num_beam_groups=generation_config.num_beam_groups,\n",
    "                max_length=generation_config.max_length,\n",
    "            )\n",
    "            # 12. interleave input_ids with `num_beams` additional sequences per batch\n",
    "            input_ids, model_kwargs = self._expand_inputs_for_generation(\n",
    "                input_ids=input_ids,\n",
    "                expand_size=generation_config.num_beams,\n",
    "                is_encoder_decoder=self.config.is_encoder_decoder,\n",
    "                **model_kwargs,\n",
    "            )\n",
    "            # 13. run beam search\n",
    "            result = self._group_beam_search(\n",
    "                input_ids,\n",
    "                beam_scorer,\n",
    "                logits_processor=prepared_logits_processor,\n",
    "                stopping_criteria=prepared_stopping_criteria,\n",
    "                generation_config=generation_config,\n",
    "                synced_gpus=synced_gpus,\n",
    "                **model_kwargs,\n",
    "            )\n",
    "\n",
    "        elif generation_mode == tf_generation_utils.GenerationMode.CONSTRAINED_BEAM_SEARCH:\n",
    "            final_constraints = []\n",
    "            if generation_config.constraints is not None:\n",
    "                final_constraints = generation_config.constraints\n",
    "\n",
    "            if generation_config.force_words_ids is not None:\n",
    "\n",
    "                def typeerror():\n",
    "                    raise ValueError(\"`force_words_ids` has to either be a `List[List[List[int]]]` or `List[List[int]]` \" f\"of positive integers, but is {generation_config.force_words_ids}.\")\n",
    "\n",
    "                if not isinstance(generation_config.force_words_ids, list) or len(generation_config.force_words_ids) == 0:\n",
    "                    typeerror()\n",
    "\n",
    "                for word_ids in generation_config.force_words_ids:\n",
    "                    if isinstance(word_ids[0], list):\n",
    "                        if not isinstance(word_ids, list) or len(word_ids) == 0:\n",
    "                            typeerror()\n",
    "                        if any(not isinstance(token_ids, list) for token_ids in word_ids):\n",
    "                            typeerror()\n",
    "                        if any(any((not isinstance(token_id, int) or token_id < 0) for token_id in token_ids) for token_ids in word_ids):\n",
    "                            typeerror()\n",
    "\n",
    "                        constraint = tf_generation_utils.DisjunctiveConstraint(word_ids)\n",
    "                    else:\n",
    "                        if not isinstance(word_ids, list) or len(word_ids) == 0:\n",
    "                            typeerror()\n",
    "                        if any((not isinstance(token_id, int) or token_id < 0) for token_id in word_ids):\n",
    "                            typeerror()\n",
    "\n",
    "                        constraint = tf_generation_utils.PhrasalConstraint(word_ids)\n",
    "                    final_constraints.append(constraint)\n",
    "\n",
    "            # 11. prepare beam search scorer\n",
    "            constrained_beam_scorer = tf_generation_utils.ConstrainedBeamSearchScorer(\n",
    "                constraints=final_constraints,\n",
    "                batch_size=batch_size,\n",
    "                num_beams=generation_config.num_beams,\n",
    "                device=inputs_tensor.device,\n",
    "                length_penalty=generation_config.length_penalty,\n",
    "                do_early_stopping=generation_config.early_stopping,\n",
    "                num_beam_hyps_to_keep=generation_config.num_return_sequences,\n",
    "                max_length=generation_config.max_length,\n",
    "            )\n",
    "            # 12. interleave input_ids with `num_beams` additional sequences per batch\n",
    "            input_ids, model_kwargs = self._expand_inputs_for_generation(\n",
    "                input_ids=input_ids,\n",
    "                expand_size=generation_config.num_beams,\n",
    "                is_encoder_decoder=self.config.is_encoder_decoder,\n",
    "                **model_kwargs,\n",
    "            )\n",
    "            # 13. run beam search\n",
    "            result = self._constrained_beam_search(\n",
    "                input_ids,\n",
    "                constrained_beam_scorer=constrained_beam_scorer,\n",
    "                logits_processor=prepared_logits_processor,\n",
    "                stopping_criteria=prepared_stopping_criteria,\n",
    "                generation_config=generation_config,\n",
    "                synced_gpus=synced_gpus,\n",
    "                **model_kwargs,\n",
    "            )\n",
    "\n",
    "        # Convert to legacy cache format if requested\n",
    "        if generation_config.return_legacy_cache is True and not tf_generation_utils.is_torchdynamo_compiling() and hasattr(result, \"past_key_values\") and getattr(result.past_key_values, \"to_legacy_cache\") is not None:\n",
    "            result.past_key_values = result.past_key_values.to_legacy_cache()\n",
    "        return result\n",
    "\n",
    "    def _prepare_decoder_input_ids_for_generation(\n",
    "        self,\n",
    "        batch_size: int,\n",
    "        model_input_name: str,\n",
    "        model_kwargs: Dict[str, torch.Tensor],\n",
    "        decoder_start_token_id: torch.Tensor,\n",
    "        pad_token_id: torch.Tensor,\n",
    "        device: torch.device = None,\n",
    "    ) -> Tuple[torch.LongTensor, Dict[str, torch.Tensor]]:\n",
    "        \"\"\"Prepares `decoder_input_ids` for generation with encoder-decoder models\n",
    "        Update: if the first token is not decoder_start_token_id or pad_token_id, we need to prepend decoder_start_token_id. Because our input_ids are left padded to multiple of 8, and then followed by decoder_start_token_id and the real input_ids. It is done in the collate_fn.\n",
    "        \"\"\"\n",
    "        # 1. Check whether the user has defined `decoder_input_ids` manually. To facilitate in terms of input naming,\n",
    "        # we also allow the user to pass it under `input_ids`, if the encoder does not use it as the main input.\n",
    "        if model_kwargs is not None and \"decoder_input_ids\" in model_kwargs:\n",
    "            decoder_input_ids = model_kwargs.pop(\"decoder_input_ids\")\n",
    "        elif \"input_ids\" in model_kwargs and model_input_name != \"input_ids\":\n",
    "            decoder_input_ids = model_kwargs.pop(\"input_ids\")\n",
    "        else:\n",
    "            decoder_input_ids = None\n",
    "\n",
    "        # 2. `decoder_start_token_id` must have shape (batch_size, 1)\n",
    "        if device is None:\n",
    "            device = self.device\n",
    "        if decoder_start_token_id.ndim == 1:\n",
    "            if decoder_start_token_id.shape[0] != batch_size:\n",
    "                raise ValueError(f\"`decoder_start_token_id` expected to have length {batch_size} but got {decoder_start_token_id.shape[0]}\")\n",
    "            decoder_start_token_id = decoder_start_token_id.view(-1, 1)\n",
    "        else:\n",
    "            decoder_start_token_id = torch.ones((batch_size, 1), dtype=torch.long, device=device) * decoder_start_token_id\n",
    "\n",
    "        # 3. Encoder-decoder models expect the `decoder_input_ids` to start with a special token. Let's ensure that.\n",
    "        # no user input -> use decoder_start_token_id as decoder_input_ids\n",
    "        if decoder_input_ids is None:\n",
    "            decoder_input_ids = decoder_start_token_id\n",
    "        # exception: Donut checkpoints have task-specific decoder starts and don't expect a BOS token. Note that the\n",
    "        # original checkpoints can't be detected through `self.__class__.__name__.lower()`, needing custom logic.\n",
    "        # See: https://github.com/huggingface/transformers/pull/31470\n",
    "        elif \"donut\" in self.__class__.__name__.lower() or (self.config.model_type == \"vision-encoder-decoder\" and \"donut\" in self.config.encoder.model_type.lower()):\n",
    "            pass\n",
    "        elif self.config.model_type in [\"whisper\"]:\n",
    "            pass\n",
    "        # user input but doesn't start with decoder_start_token_id -> prepend decoder_start_token_id (and adjust\n",
    "        # decoder_attention_mask if provided)\n",
    "        #######################################\n",
    "        # !!! Update: if the first token is not decoder_start_token_id or pad_token_id, we need to prepend decoder_start_token_id\n",
    "        #######################################\n",
    "        elif ((decoder_input_ids[:, 0] != decoder_start_token_id[:, 0]) & (decoder_input_ids[:, 0] != pad_token_id)).all().item():\n",
    "            decoder_input_ids = torch.cat([decoder_start_token_id, decoder_input_ids], dim=-1)\n",
    "            if \"decoder_attention_mask\" in model_kwargs:\n",
    "                decoder_attention_mask = model_kwargs[\"decoder_attention_mask\"]\n",
    "                decoder_attention_mask = torch.cat(\n",
    "                    (torch.ones_like(decoder_attention_mask)[:, :1], decoder_attention_mask),\n",
    "                    dim=-1,\n",
    "                )\n",
    "                model_kwargs[\"decoder_attention_mask\"] = decoder_attention_mask\n",
    "\n",
    "        return decoder_input_ids, model_kwargs\n",
    "\n",
    "\n",
    "class ImageTextDataset(Dataset):\n",
    "    def __init__(self, hf_dataset, img_processor, tokenizer, split):\n",
    "        # column_names: ['source', 'images_path', 'images', 'section_text', 'doc_key', 'split_sents', 'split_sent_toks', 'sent_idx_split_idx', 'radlex', 'cxrgraph_ent', 'cxrgraph_attr', 'cxrgraph_rel']\n",
    "        self.split = split\n",
    "        self.src_path = os.path.dirname(hf_dataset.cache_files[0][\"filename\"]) if hf_dataset.cache_files else \"\"\n",
    "        self.img_processor = img_processor\n",
    "        self.tokenizer = tokenizer\n",
    "        self.samples = hf_dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    # 返回索引的数据与标签\n",
    "    def __getitem__(self, index):\n",
    "        return self.samples[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3a4fcb3-d3cd-44a2-a1cf-a383abefc434",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:11<00:00,  5.87s/it]\n"
     ]
    }
   ],
   "source": [
    "model_path = \"/scratch/c.c21051562/workspace/arrg_img2text/outputs/models/4_1_test_pretrain_save_with_FULL_STATE_DICT\"\n",
    "pre_treained_model = Vision2LanguageModel.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8af418f0-a5e5-40b5-aeea-028eed56716a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vision_model_path = \"/scratch/c.c21051562/resources/downloaded_models/rad-dino-maira-2\"\n",
    "language_model_path = \"/scratch/c.c21051562/resources/downloaded_models/Llama-3.2-1B\"\n",
    "init_model = Vision2LanguageModel.from_encoder_decoder_pretrained(vision_model_path, language_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24305553-855a-477c-a9c4-85d47c5198ac",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vision2LanguageModel(\n",
       "  (encoder): Dinov2Model(\n",
       "    (embeddings): Dinov2Embeddings(\n",
       "      (patch_embeddings): Dinov2PatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(14, 14), stride=(14, 14))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): Dinov2Encoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x Dinov2Layer(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attention): Dinov2Attention(\n",
       "            (attention): Dinov2SelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): Dinov2SelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (layer_scale1): Dinov2LayerScale()\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Dinov2MLP(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_scale2): Dinov2LayerScale()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): LlamaForCausalLM(\n",
       "    (model): LlamaModel(\n",
       "      (embed_tokens): Embedding(128264, 2048)\n",
       "      (layers): ModuleList(\n",
       "        (0-15): 16 x LlamaDecoderLayer(\n",
       "          (self_attn): LlamaAttention(\n",
       "            (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "            (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "            (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "            (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "            (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "          (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        )\n",
       "      )\n",
       "      (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=2048, out_features=128264, bias=False)\n",
       "  )\n",
       "  (v2l_projector): VisionLanguageProjector(\n",
       "    (linear_1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "    (act): SiLU()\n",
       "    (linear_2): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_treained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ec8fcf8-6920-4922-8d67-3e3607660cba",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vision2LanguageModel(\n",
       "  (encoder): Dinov2Model(\n",
       "    (embeddings): Dinov2Embeddings(\n",
       "      (patch_embeddings): Dinov2PatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(14, 14), stride=(14, 14))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): Dinov2Encoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x Dinov2Layer(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attention): Dinov2SdpaAttention(\n",
       "            (attention): Dinov2SdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): Dinov2SelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (layer_scale1): Dinov2LayerScale()\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Dinov2MLP(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_scale2): Dinov2LayerScale()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): LlamaForCausalLM(\n",
       "    (model): LlamaModel(\n",
       "      (embed_tokens): Embedding(128256, 2048)\n",
       "      (layers): ModuleList(\n",
       "        (0-15): 16 x LlamaDecoderLayer(\n",
       "          (self_attn): LlamaAttention(\n",
       "            (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "            (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "            (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "            (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "            (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "          (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        )\n",
       "      )\n",
       "      (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       "  )\n",
       "  (v2l_projector): VisionLanguageProjector(\n",
       "    (linear_1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "    (act): SiLU()\n",
       "    (linear_2): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "739f6a58-036a-4e6f-b6b8-b73a64f12b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sd = load_file(\"/scratch/c.c21051562/workspace/arrg_img2text/outputs/models/4_1_vlgen_effu_fsdp_peft_test_finetune/adapter_model.safetensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8560c5d0-23bb-47f7-b97e-1bc66550a014",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.decoder.lm_head.base_layer.weight torch.Size([128264, 2048])\n",
      "base_model.model.decoder.lm_head.lora_A.weight torch.Size([32768])\n",
      "base_model.model.decoder.lm_head.lora_B.weight torch.Size([2052224])\n",
      "base_model.model.decoder.model.embed_tokens.base_layer.weight torch.Size([128264, 2048])\n",
      "base_model.model.decoder.model.embed_tokens.lora_embedding_A torch.Size([16, 128264])\n",
      "base_model.model.decoder.model.embed_tokens.lora_embedding_B torch.Size([2048, 16])\n",
      "base_model.model.decoder.model.layers.0.self_attn.q_proj.lora_A.weight torch.Size([16, 2048])\n",
      "base_model.model.decoder.model.layers.0.self_attn.q_proj.lora_B.weight torch.Size([2048, 16])\n",
      "base_model.model.decoder.model.layers.0.self_attn.v_proj.lora_A.weight torch.Size([16, 2048])\n",
      "base_model.model.decoder.model.layers.0.self_attn.v_proj.lora_B.weight torch.Size([512, 16])\n",
      "base_model.model.decoder.model.layers.1.self_attn.q_proj.lora_A.weight torch.Size([16, 2048])\n",
      "base_model.model.decoder.model.layers.1.self_attn.q_proj.lora_B.weight torch.Size([2048, 16])\n",
      "base_model.model.decoder.model.layers.1.self_attn.v_proj.lora_A.weight torch.Size([16, 2048])\n",
      "base_model.model.decoder.model.layers.1.self_attn.v_proj.lora_B.weight torch.Size([512, 16])\n",
      "base_model.model.decoder.model.layers.10.self_attn.q_proj.lora_A.weight torch.Size([16, 2048])\n",
      "base_model.model.decoder.model.layers.10.self_attn.q_proj.lora_B.weight torch.Size([2048, 16])\n",
      "base_model.model.decoder.model.layers.10.self_attn.v_proj.lora_A.weight torch.Size([16, 2048])\n",
      "base_model.model.decoder.model.layers.10.self_attn.v_proj.lora_B.weight torch.Size([512, 16])\n",
      "base_model.model.decoder.model.layers.11.self_attn.q_proj.lora_A.weight torch.Size([16, 2048])\n",
      "base_model.model.decoder.model.layers.11.self_attn.q_proj.lora_B.weight torch.Size([2048, 16])\n",
      "base_model.model.decoder.model.layers.11.self_attn.v_proj.lora_A.weight torch.Size([16, 2048])\n",
      "base_model.model.decoder.model.layers.11.self_attn.v_proj.lora_B.weight torch.Size([512, 16])\n",
      "base_model.model.decoder.model.layers.12.self_attn.q_proj.lora_A.weight torch.Size([16, 2048])\n",
      "base_model.model.decoder.model.layers.12.self_attn.q_proj.lora_B.weight torch.Size([2048, 16])\n",
      "base_model.model.decoder.model.layers.12.self_attn.v_proj.lora_A.weight torch.Size([16, 2048])\n",
      "base_model.model.decoder.model.layers.12.self_attn.v_proj.lora_B.weight torch.Size([512, 16])\n",
      "base_model.model.decoder.model.layers.13.self_attn.q_proj.lora_A.weight torch.Size([16, 2048])\n",
      "base_model.model.decoder.model.layers.13.self_attn.q_proj.lora_B.weight torch.Size([2048, 16])\n",
      "base_model.model.decoder.model.layers.13.self_attn.v_proj.lora_A.weight torch.Size([16, 2048])\n",
      "base_model.model.decoder.model.layers.13.self_attn.v_proj.lora_B.weight torch.Size([512, 16])\n",
      "base_model.model.decoder.model.layers.14.self_attn.q_proj.lora_A.weight torch.Size([16, 2048])\n",
      "base_model.model.decoder.model.layers.14.self_attn.q_proj.lora_B.weight torch.Size([2048, 16])\n",
      "base_model.model.decoder.model.layers.14.self_attn.v_proj.lora_A.weight torch.Size([16, 2048])\n",
      "base_model.model.decoder.model.layers.14.self_attn.v_proj.lora_B.weight torch.Size([512, 16])\n",
      "base_model.model.decoder.model.layers.15.self_attn.q_proj.lora_A.weight torch.Size([16, 2048])\n",
      "base_model.model.decoder.model.layers.15.self_attn.q_proj.lora_B.weight torch.Size([2048, 16])\n",
      "base_model.model.decoder.model.layers.15.self_attn.v_proj.lora_A.weight torch.Size([16, 2048])\n",
      "base_model.model.decoder.model.layers.15.self_attn.v_proj.lora_B.weight torch.Size([512, 16])\n",
      "base_model.model.decoder.model.layers.2.self_attn.q_proj.lora_A.weight torch.Size([16, 2048])\n",
      "base_model.model.decoder.model.layers.2.self_attn.q_proj.lora_B.weight torch.Size([2048, 16])\n",
      "base_model.model.decoder.model.layers.2.self_attn.v_proj.lora_A.weight torch.Size([16, 2048])\n",
      "base_model.model.decoder.model.layers.2.self_attn.v_proj.lora_B.weight torch.Size([512, 16])\n",
      "base_model.model.decoder.model.layers.3.self_attn.q_proj.lora_A.weight torch.Size([16, 2048])\n",
      "base_model.model.decoder.model.layers.3.self_attn.q_proj.lora_B.weight torch.Size([2048, 16])\n",
      "base_model.model.decoder.model.layers.3.self_attn.v_proj.lora_A.weight torch.Size([16, 2048])\n",
      "base_model.model.decoder.model.layers.3.self_attn.v_proj.lora_B.weight torch.Size([512, 16])\n",
      "base_model.model.decoder.model.layers.4.self_attn.q_proj.lora_A.weight torch.Size([16, 2048])\n",
      "base_model.model.decoder.model.layers.4.self_attn.q_proj.lora_B.weight torch.Size([2048, 16])\n",
      "base_model.model.decoder.model.layers.4.self_attn.v_proj.lora_A.weight torch.Size([16, 2048])\n",
      "base_model.model.decoder.model.layers.4.self_attn.v_proj.lora_B.weight torch.Size([512, 16])\n",
      "base_model.model.decoder.model.layers.5.self_attn.q_proj.lora_A.weight torch.Size([16, 2048])\n",
      "base_model.model.decoder.model.layers.5.self_attn.q_proj.lora_B.weight torch.Size([2048, 16])\n",
      "base_model.model.decoder.model.layers.5.self_attn.v_proj.lora_A.weight torch.Size([16, 2048])\n",
      "base_model.model.decoder.model.layers.5.self_attn.v_proj.lora_B.weight torch.Size([512, 16])\n",
      "base_model.model.decoder.model.layers.6.self_attn.q_proj.lora_A.weight torch.Size([16, 2048])\n",
      "base_model.model.decoder.model.layers.6.self_attn.q_proj.lora_B.weight torch.Size([2048, 16])\n",
      "base_model.model.decoder.model.layers.6.self_attn.v_proj.lora_A.weight torch.Size([16, 2048])\n",
      "base_model.model.decoder.model.layers.6.self_attn.v_proj.lora_B.weight torch.Size([512, 16])\n",
      "base_model.model.decoder.model.layers.7.self_attn.q_proj.lora_A.weight torch.Size([16, 2048])\n",
      "base_model.model.decoder.model.layers.7.self_attn.q_proj.lora_B.weight torch.Size([2048, 16])\n",
      "base_model.model.decoder.model.layers.7.self_attn.v_proj.lora_A.weight torch.Size([16, 2048])\n",
      "base_model.model.decoder.model.layers.7.self_attn.v_proj.lora_B.weight torch.Size([512, 16])\n",
      "base_model.model.decoder.model.layers.8.self_attn.q_proj.lora_A.weight torch.Size([16, 2048])\n",
      "base_model.model.decoder.model.layers.8.self_attn.q_proj.lora_B.weight torch.Size([2048, 16])\n",
      "base_model.model.decoder.model.layers.8.self_attn.v_proj.lora_A.weight torch.Size([16, 2048])\n",
      "base_model.model.decoder.model.layers.8.self_attn.v_proj.lora_B.weight torch.Size([512, 16])\n",
      "base_model.model.decoder.model.layers.9.self_attn.q_proj.lora_A.weight torch.Size([16, 2048])\n",
      "base_model.model.decoder.model.layers.9.self_attn.q_proj.lora_B.weight torch.Size([2048, 16])\n",
      "base_model.model.decoder.model.layers.9.self_attn.v_proj.lora_A.weight torch.Size([16, 2048])\n",
      "base_model.model.decoder.model.layers.9.self_attn.v_proj.lora_B.weight torch.Size([512, 16])\n",
      "base_model.model.v2l_projector.linear_1.bias torch.Size([0])\n",
      "base_model.model.v2l_projector.linear_1.weight torch.Size([0])\n",
      "base_model.model.v2l_projector.linear_2.bias torch.Size([0])\n",
      "base_model.model.v2l_projector.linear_2.weight torch.Size([0])\n"
     ]
    }
   ],
   "source": [
    "for n, p in sd.items():\n",
    "    print(n, p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d74fb23b-c703-48fa-8960-adfe3529e59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sd2 = load_file(\"/scratch/c.c21051562/workspace/arrg_img2text/outputs/models/test/model.safetensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0481fda-d28c-4146-b689-acfdf8b9b217",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder.lm_head.weight torch.Size([128264, 2048])\n",
      "decoder.model.embed_tokens.weight torch.Size([128264, 2048])\n",
      "decoder.model.layers.0.input_layernorm.weight torch.Size([2048])\n",
      "decoder.model.layers.0.mlp.down_proj.weight torch.Size([2048, 8192])\n",
      "decoder.model.layers.0.mlp.gate_proj.weight torch.Size([8192, 2048])\n",
      "decoder.model.layers.0.mlp.up_proj.weight torch.Size([8192, 2048])\n",
      "decoder.model.layers.0.post_attention_layernorm.weight torch.Size([2048])\n",
      "decoder.model.layers.0.self_attn.k_proj.weight torch.Size([512, 2048])\n",
      "decoder.model.layers.0.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "decoder.model.layers.0.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "decoder.model.layers.0.self_attn.v_proj.weight torch.Size([512, 2048])\n",
      "decoder.model.layers.1.input_layernorm.weight torch.Size([2048])\n",
      "decoder.model.layers.1.mlp.down_proj.weight torch.Size([2048, 8192])\n",
      "decoder.model.layers.1.mlp.gate_proj.weight torch.Size([8192, 2048])\n",
      "decoder.model.layers.1.mlp.up_proj.weight torch.Size([8192, 2048])\n",
      "decoder.model.layers.1.post_attention_layernorm.weight torch.Size([2048])\n",
      "decoder.model.layers.1.self_attn.k_proj.weight torch.Size([512, 2048])\n",
      "decoder.model.layers.1.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "decoder.model.layers.1.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "decoder.model.layers.1.self_attn.v_proj.weight torch.Size([512, 2048])\n",
      "decoder.model.layers.10.input_layernorm.weight torch.Size([2048])\n",
      "decoder.model.layers.10.mlp.down_proj.weight torch.Size([2048, 8192])\n",
      "decoder.model.layers.10.mlp.gate_proj.weight torch.Size([8192, 2048])\n",
      "decoder.model.layers.10.mlp.up_proj.weight torch.Size([8192, 2048])\n",
      "decoder.model.layers.10.post_attention_layernorm.weight torch.Size([2048])\n",
      "decoder.model.layers.10.self_attn.k_proj.weight torch.Size([512, 2048])\n",
      "decoder.model.layers.10.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "decoder.model.layers.10.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "decoder.model.layers.10.self_attn.v_proj.weight torch.Size([512, 2048])\n",
      "decoder.model.layers.11.input_layernorm.weight torch.Size([2048])\n",
      "decoder.model.layers.11.mlp.down_proj.weight torch.Size([2048, 8192])\n",
      "decoder.model.layers.11.mlp.gate_proj.weight torch.Size([8192, 2048])\n",
      "decoder.model.layers.11.mlp.up_proj.weight torch.Size([8192, 2048])\n",
      "decoder.model.layers.11.post_attention_layernorm.weight torch.Size([2048])\n",
      "decoder.model.layers.11.self_attn.k_proj.weight torch.Size([512, 2048])\n",
      "decoder.model.layers.11.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "decoder.model.layers.11.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "decoder.model.layers.11.self_attn.v_proj.weight torch.Size([512, 2048])\n",
      "decoder.model.layers.12.input_layernorm.weight torch.Size([2048])\n",
      "decoder.model.layers.12.mlp.down_proj.weight torch.Size([2048, 8192])\n",
      "decoder.model.layers.12.mlp.gate_proj.weight torch.Size([8192, 2048])\n",
      "decoder.model.layers.12.mlp.up_proj.weight torch.Size([8192, 2048])\n",
      "decoder.model.layers.12.post_attention_layernorm.weight torch.Size([2048])\n",
      "decoder.model.layers.12.self_attn.k_proj.weight torch.Size([512, 2048])\n",
      "decoder.model.layers.12.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "decoder.model.layers.12.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "decoder.model.layers.12.self_attn.v_proj.weight torch.Size([512, 2048])\n",
      "decoder.model.layers.13.input_layernorm.weight torch.Size([2048])\n",
      "decoder.model.layers.13.mlp.down_proj.weight torch.Size([2048, 8192])\n",
      "decoder.model.layers.13.mlp.gate_proj.weight torch.Size([8192, 2048])\n",
      "decoder.model.layers.13.mlp.up_proj.weight torch.Size([8192, 2048])\n",
      "decoder.model.layers.13.post_attention_layernorm.weight torch.Size([2048])\n",
      "decoder.model.layers.13.self_attn.k_proj.weight torch.Size([512, 2048])\n",
      "decoder.model.layers.13.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "decoder.model.layers.13.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "decoder.model.layers.13.self_attn.v_proj.weight torch.Size([512, 2048])\n",
      "decoder.model.layers.14.input_layernorm.weight torch.Size([2048])\n",
      "decoder.model.layers.14.mlp.down_proj.weight torch.Size([2048, 8192])\n",
      "decoder.model.layers.14.mlp.gate_proj.weight torch.Size([8192, 2048])\n",
      "decoder.model.layers.14.mlp.up_proj.weight torch.Size([8192, 2048])\n",
      "decoder.model.layers.14.post_attention_layernorm.weight torch.Size([2048])\n",
      "decoder.model.layers.14.self_attn.k_proj.weight torch.Size([512, 2048])\n",
      "decoder.model.layers.14.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "decoder.model.layers.14.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "decoder.model.layers.14.self_attn.v_proj.weight torch.Size([512, 2048])\n",
      "decoder.model.layers.15.input_layernorm.weight torch.Size([2048])\n",
      "decoder.model.layers.15.mlp.down_proj.weight torch.Size([2048, 8192])\n",
      "decoder.model.layers.15.mlp.gate_proj.weight torch.Size([8192, 2048])\n",
      "decoder.model.layers.15.mlp.up_proj.weight torch.Size([8192, 2048])\n",
      "decoder.model.layers.15.post_attention_layernorm.weight torch.Size([2048])\n",
      "decoder.model.layers.15.self_attn.k_proj.weight torch.Size([512, 2048])\n",
      "decoder.model.layers.15.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "decoder.model.layers.15.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "decoder.model.layers.15.self_attn.v_proj.weight torch.Size([512, 2048])\n",
      "decoder.model.layers.2.input_layernorm.weight torch.Size([2048])\n",
      "decoder.model.layers.2.mlp.down_proj.weight torch.Size([2048, 8192])\n",
      "decoder.model.layers.2.mlp.gate_proj.weight torch.Size([8192, 2048])\n",
      "decoder.model.layers.2.mlp.up_proj.weight torch.Size([8192, 2048])\n",
      "decoder.model.layers.2.post_attention_layernorm.weight torch.Size([2048])\n",
      "decoder.model.layers.2.self_attn.k_proj.weight torch.Size([512, 2048])\n",
      "decoder.model.layers.2.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "decoder.model.layers.2.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "decoder.model.layers.2.self_attn.v_proj.weight torch.Size([512, 2048])\n",
      "decoder.model.layers.3.input_layernorm.weight torch.Size([2048])\n",
      "decoder.model.layers.3.mlp.down_proj.weight torch.Size([2048, 8192])\n",
      "decoder.model.layers.3.mlp.gate_proj.weight torch.Size([8192, 2048])\n",
      "decoder.model.layers.3.mlp.up_proj.weight torch.Size([8192, 2048])\n",
      "decoder.model.layers.3.post_attention_layernorm.weight torch.Size([2048])\n",
      "decoder.model.layers.3.self_attn.k_proj.weight torch.Size([512, 2048])\n",
      "decoder.model.layers.3.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "decoder.model.layers.3.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "decoder.model.layers.3.self_attn.v_proj.weight torch.Size([512, 2048])\n",
      "decoder.model.layers.4.input_layernorm.weight torch.Size([2048])\n",
      "decoder.model.layers.4.mlp.down_proj.weight torch.Size([2048, 8192])\n",
      "decoder.model.layers.4.mlp.gate_proj.weight torch.Size([8192, 2048])\n",
      "decoder.model.layers.4.mlp.up_proj.weight torch.Size([8192, 2048])\n",
      "decoder.model.layers.4.post_attention_layernorm.weight torch.Size([2048])\n",
      "decoder.model.layers.4.self_attn.k_proj.weight torch.Size([512, 2048])\n",
      "decoder.model.layers.4.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "decoder.model.layers.4.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "decoder.model.layers.4.self_attn.v_proj.weight torch.Size([512, 2048])\n",
      "decoder.model.layers.5.input_layernorm.weight torch.Size([2048])\n",
      "decoder.model.layers.5.mlp.down_proj.weight torch.Size([2048, 8192])\n",
      "decoder.model.layers.5.mlp.gate_proj.weight torch.Size([8192, 2048])\n",
      "decoder.model.layers.5.mlp.up_proj.weight torch.Size([8192, 2048])\n",
      "decoder.model.layers.5.post_attention_layernorm.weight torch.Size([2048])\n",
      "decoder.model.layers.5.self_attn.k_proj.weight torch.Size([512, 2048])\n",
      "decoder.model.layers.5.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "decoder.model.layers.5.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "decoder.model.layers.5.self_attn.v_proj.weight torch.Size([512, 2048])\n",
      "decoder.model.layers.6.input_layernorm.weight torch.Size([2048])\n",
      "decoder.model.layers.6.mlp.down_proj.weight torch.Size([2048, 8192])\n",
      "decoder.model.layers.6.mlp.gate_proj.weight torch.Size([8192, 2048])\n",
      "decoder.model.layers.6.mlp.up_proj.weight torch.Size([8192, 2048])\n",
      "decoder.model.layers.6.post_attention_layernorm.weight torch.Size([2048])\n",
      "decoder.model.layers.6.self_attn.k_proj.weight torch.Size([512, 2048])\n",
      "decoder.model.layers.6.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "decoder.model.layers.6.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "decoder.model.layers.6.self_attn.v_proj.weight torch.Size([512, 2048])\n",
      "decoder.model.layers.7.input_layernorm.weight torch.Size([2048])\n",
      "decoder.model.layers.7.mlp.down_proj.weight torch.Size([2048, 8192])\n",
      "decoder.model.layers.7.mlp.gate_proj.weight torch.Size([8192, 2048])\n",
      "decoder.model.layers.7.mlp.up_proj.weight torch.Size([8192, 2048])\n",
      "decoder.model.layers.7.post_attention_layernorm.weight torch.Size([2048])\n",
      "decoder.model.layers.7.self_attn.k_proj.weight torch.Size([512, 2048])\n",
      "decoder.model.layers.7.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "decoder.model.layers.7.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "decoder.model.layers.7.self_attn.v_proj.weight torch.Size([512, 2048])\n",
      "decoder.model.layers.8.input_layernorm.weight torch.Size([2048])\n",
      "decoder.model.layers.8.mlp.down_proj.weight torch.Size([2048, 8192])\n",
      "decoder.model.layers.8.mlp.gate_proj.weight torch.Size([8192, 2048])\n",
      "decoder.model.layers.8.mlp.up_proj.weight torch.Size([8192, 2048])\n",
      "decoder.model.layers.8.post_attention_layernorm.weight torch.Size([2048])\n",
      "decoder.model.layers.8.self_attn.k_proj.weight torch.Size([512, 2048])\n",
      "decoder.model.layers.8.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "decoder.model.layers.8.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "decoder.model.layers.8.self_attn.v_proj.weight torch.Size([512, 2048])\n",
      "decoder.model.layers.9.input_layernorm.weight torch.Size([2048])\n",
      "decoder.model.layers.9.mlp.down_proj.weight torch.Size([2048, 8192])\n",
      "decoder.model.layers.9.mlp.gate_proj.weight torch.Size([8192, 2048])\n",
      "decoder.model.layers.9.mlp.up_proj.weight torch.Size([8192, 2048])\n",
      "decoder.model.layers.9.post_attention_layernorm.weight torch.Size([2048])\n",
      "decoder.model.layers.9.self_attn.k_proj.weight torch.Size([512, 2048])\n",
      "decoder.model.layers.9.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "decoder.model.layers.9.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "decoder.model.layers.9.self_attn.v_proj.weight torch.Size([512, 2048])\n",
      "decoder.model.norm.weight torch.Size([2048])\n",
      "encoder.embeddings.cls_token torch.Size([1, 1, 768])\n",
      "encoder.embeddings.mask_token torch.Size([1, 768])\n",
      "encoder.embeddings.patch_embeddings.projection.bias torch.Size([768])\n",
      "encoder.embeddings.patch_embeddings.projection.weight torch.Size([768, 3, 14, 14])\n",
      "encoder.embeddings.position_embeddings torch.Size([1, 1370, 768])\n",
      "encoder.encoder.layer.0.attention.attention.key.bias torch.Size([768])\n",
      "encoder.encoder.layer.0.attention.attention.key.weight torch.Size([768, 768])\n",
      "encoder.encoder.layer.0.attention.attention.query.bias torch.Size([768])\n",
      "encoder.encoder.layer.0.attention.attention.query.weight torch.Size([768, 768])\n",
      "encoder.encoder.layer.0.attention.attention.value.bias torch.Size([768])\n",
      "encoder.encoder.layer.0.attention.attention.value.weight torch.Size([768, 768])\n",
      "encoder.encoder.layer.0.attention.output.dense.bias torch.Size([768])\n",
      "encoder.encoder.layer.0.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.encoder.layer.0.layer_scale1.lambda1 torch.Size([768])\n",
      "encoder.encoder.layer.0.layer_scale2.lambda1 torch.Size([768])\n",
      "encoder.encoder.layer.0.mlp.fc1.bias torch.Size([3072])\n",
      "encoder.encoder.layer.0.mlp.fc1.weight torch.Size([3072, 768])\n",
      "encoder.encoder.layer.0.mlp.fc2.bias torch.Size([768])\n",
      "encoder.encoder.layer.0.mlp.fc2.weight torch.Size([768, 3072])\n",
      "encoder.encoder.layer.0.norm1.bias torch.Size([768])\n",
      "encoder.encoder.layer.0.norm1.weight torch.Size([768])\n",
      "encoder.encoder.layer.0.norm2.bias torch.Size([768])\n",
      "encoder.encoder.layer.0.norm2.weight torch.Size([768])\n",
      "encoder.encoder.layer.1.attention.attention.key.bias torch.Size([768])\n",
      "encoder.encoder.layer.1.attention.attention.key.weight torch.Size([768, 768])\n",
      "encoder.encoder.layer.1.attention.attention.query.bias torch.Size([768])\n",
      "encoder.encoder.layer.1.attention.attention.query.weight torch.Size([768, 768])\n",
      "encoder.encoder.layer.1.attention.attention.value.bias torch.Size([768])\n",
      "encoder.encoder.layer.1.attention.attention.value.weight torch.Size([768, 768])\n",
      "encoder.encoder.layer.1.attention.output.dense.bias torch.Size([768])\n",
      "encoder.encoder.layer.1.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.encoder.layer.1.layer_scale1.lambda1 torch.Size([768])\n",
      "encoder.encoder.layer.1.layer_scale2.lambda1 torch.Size([768])\n",
      "encoder.encoder.layer.1.mlp.fc1.bias torch.Size([3072])\n",
      "encoder.encoder.layer.1.mlp.fc1.weight torch.Size([3072, 768])\n",
      "encoder.encoder.layer.1.mlp.fc2.bias torch.Size([768])\n",
      "encoder.encoder.layer.1.mlp.fc2.weight torch.Size([768, 3072])\n",
      "encoder.encoder.layer.1.norm1.bias torch.Size([768])\n",
      "encoder.encoder.layer.1.norm1.weight torch.Size([768])\n",
      "encoder.encoder.layer.1.norm2.bias torch.Size([768])\n",
      "encoder.encoder.layer.1.norm2.weight torch.Size([768])\n",
      "encoder.encoder.layer.10.attention.attention.key.bias torch.Size([768])\n",
      "encoder.encoder.layer.10.attention.attention.key.weight torch.Size([768, 768])\n",
      "encoder.encoder.layer.10.attention.attention.query.bias torch.Size([768])\n",
      "encoder.encoder.layer.10.attention.attention.query.weight torch.Size([768, 768])\n",
      "encoder.encoder.layer.10.attention.attention.value.bias torch.Size([768])\n",
      "encoder.encoder.layer.10.attention.attention.value.weight torch.Size([768, 768])\n",
      "encoder.encoder.layer.10.attention.output.dense.bias torch.Size([768])\n",
      "encoder.encoder.layer.10.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.encoder.layer.10.layer_scale1.lambda1 torch.Size([768])\n",
      "encoder.encoder.layer.10.layer_scale2.lambda1 torch.Size([768])\n",
      "encoder.encoder.layer.10.mlp.fc1.bias torch.Size([3072])\n",
      "encoder.encoder.layer.10.mlp.fc1.weight torch.Size([3072, 768])\n",
      "encoder.encoder.layer.10.mlp.fc2.bias torch.Size([768])\n",
      "encoder.encoder.layer.10.mlp.fc2.weight torch.Size([768, 3072])\n",
      "encoder.encoder.layer.10.norm1.bias torch.Size([768])\n",
      "encoder.encoder.layer.10.norm1.weight torch.Size([768])\n",
      "encoder.encoder.layer.10.norm2.bias torch.Size([768])\n",
      "encoder.encoder.layer.10.norm2.weight torch.Size([768])\n",
      "encoder.encoder.layer.11.attention.attention.key.bias torch.Size([768])\n",
      "encoder.encoder.layer.11.attention.attention.key.weight torch.Size([768, 768])\n",
      "encoder.encoder.layer.11.attention.attention.query.bias torch.Size([768])\n",
      "encoder.encoder.layer.11.attention.attention.query.weight torch.Size([768, 768])\n",
      "encoder.encoder.layer.11.attention.attention.value.bias torch.Size([768])\n",
      "encoder.encoder.layer.11.attention.attention.value.weight torch.Size([768, 768])\n",
      "encoder.encoder.layer.11.attention.output.dense.bias torch.Size([768])\n",
      "encoder.encoder.layer.11.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.encoder.layer.11.layer_scale1.lambda1 torch.Size([768])\n",
      "encoder.encoder.layer.11.layer_scale2.lambda1 torch.Size([768])\n",
      "encoder.encoder.layer.11.mlp.fc1.bias torch.Size([3072])\n",
      "encoder.encoder.layer.11.mlp.fc1.weight torch.Size([3072, 768])\n",
      "encoder.encoder.layer.11.mlp.fc2.bias torch.Size([768])\n",
      "encoder.encoder.layer.11.mlp.fc2.weight torch.Size([768, 3072])\n",
      "encoder.encoder.layer.11.norm1.bias torch.Size([768])\n",
      "encoder.encoder.layer.11.norm1.weight torch.Size([768])\n",
      "encoder.encoder.layer.11.norm2.bias torch.Size([768])\n",
      "encoder.encoder.layer.11.norm2.weight torch.Size([768])\n",
      "encoder.encoder.layer.2.attention.attention.key.bias torch.Size([768])\n",
      "encoder.encoder.layer.2.attention.attention.key.weight torch.Size([768, 768])\n",
      "encoder.encoder.layer.2.attention.attention.query.bias torch.Size([768])\n",
      "encoder.encoder.layer.2.attention.attention.query.weight torch.Size([768, 768])\n",
      "encoder.encoder.layer.2.attention.attention.value.bias torch.Size([768])\n",
      "encoder.encoder.layer.2.attention.attention.value.weight torch.Size([768, 768])\n",
      "encoder.encoder.layer.2.attention.output.dense.bias torch.Size([768])\n",
      "encoder.encoder.layer.2.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.encoder.layer.2.layer_scale1.lambda1 torch.Size([768])\n",
      "encoder.encoder.layer.2.layer_scale2.lambda1 torch.Size([768])\n",
      "encoder.encoder.layer.2.mlp.fc1.bias torch.Size([3072])\n",
      "encoder.encoder.layer.2.mlp.fc1.weight torch.Size([3072, 768])\n",
      "encoder.encoder.layer.2.mlp.fc2.bias torch.Size([768])\n",
      "encoder.encoder.layer.2.mlp.fc2.weight torch.Size([768, 3072])\n",
      "encoder.encoder.layer.2.norm1.bias torch.Size([768])\n",
      "encoder.encoder.layer.2.norm1.weight torch.Size([768])\n",
      "encoder.encoder.layer.2.norm2.bias torch.Size([768])\n",
      "encoder.encoder.layer.2.norm2.weight torch.Size([768])\n",
      "encoder.encoder.layer.3.attention.attention.key.bias torch.Size([768])\n",
      "encoder.encoder.layer.3.attention.attention.key.weight torch.Size([768, 768])\n",
      "encoder.encoder.layer.3.attention.attention.query.bias torch.Size([768])\n",
      "encoder.encoder.layer.3.attention.attention.query.weight torch.Size([768, 768])\n",
      "encoder.encoder.layer.3.attention.attention.value.bias torch.Size([768])\n",
      "encoder.encoder.layer.3.attention.attention.value.weight torch.Size([768, 768])\n",
      "encoder.encoder.layer.3.attention.output.dense.bias torch.Size([768])\n",
      "encoder.encoder.layer.3.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.encoder.layer.3.layer_scale1.lambda1 torch.Size([768])\n",
      "encoder.encoder.layer.3.layer_scale2.lambda1 torch.Size([768])\n",
      "encoder.encoder.layer.3.mlp.fc1.bias torch.Size([3072])\n",
      "encoder.encoder.layer.3.mlp.fc1.weight torch.Size([3072, 768])\n",
      "encoder.encoder.layer.3.mlp.fc2.bias torch.Size([768])\n",
      "encoder.encoder.layer.3.mlp.fc2.weight torch.Size([768, 3072])\n",
      "encoder.encoder.layer.3.norm1.bias torch.Size([768])\n",
      "encoder.encoder.layer.3.norm1.weight torch.Size([768])\n",
      "encoder.encoder.layer.3.norm2.bias torch.Size([768])\n",
      "encoder.encoder.layer.3.norm2.weight torch.Size([768])\n",
      "encoder.encoder.layer.4.attention.attention.key.bias torch.Size([768])\n",
      "encoder.encoder.layer.4.attention.attention.key.weight torch.Size([768, 768])\n",
      "encoder.encoder.layer.4.attention.attention.query.bias torch.Size([768])\n",
      "encoder.encoder.layer.4.attention.attention.query.weight torch.Size([768, 768])\n",
      "encoder.encoder.layer.4.attention.attention.value.bias torch.Size([768])\n",
      "encoder.encoder.layer.4.attention.attention.value.weight torch.Size([768, 768])\n",
      "encoder.encoder.layer.4.attention.output.dense.bias torch.Size([768])\n",
      "encoder.encoder.layer.4.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.encoder.layer.4.layer_scale1.lambda1 torch.Size([768])\n",
      "encoder.encoder.layer.4.layer_scale2.lambda1 torch.Size([768])\n",
      "encoder.encoder.layer.4.mlp.fc1.bias torch.Size([3072])\n",
      "encoder.encoder.layer.4.mlp.fc1.weight torch.Size([3072, 768])\n",
      "encoder.encoder.layer.4.mlp.fc2.bias torch.Size([768])\n",
      "encoder.encoder.layer.4.mlp.fc2.weight torch.Size([768, 3072])\n",
      "encoder.encoder.layer.4.norm1.bias torch.Size([768])\n",
      "encoder.encoder.layer.4.norm1.weight torch.Size([768])\n",
      "encoder.encoder.layer.4.norm2.bias torch.Size([768])\n",
      "encoder.encoder.layer.4.norm2.weight torch.Size([768])\n",
      "encoder.encoder.layer.5.attention.attention.key.bias torch.Size([768])\n",
      "encoder.encoder.layer.5.attention.attention.key.weight torch.Size([768, 768])\n",
      "encoder.encoder.layer.5.attention.attention.query.bias torch.Size([768])\n",
      "encoder.encoder.layer.5.attention.attention.query.weight torch.Size([768, 768])\n",
      "encoder.encoder.layer.5.attention.attention.value.bias torch.Size([768])\n",
      "encoder.encoder.layer.5.attention.attention.value.weight torch.Size([768, 768])\n",
      "encoder.encoder.layer.5.attention.output.dense.bias torch.Size([768])\n",
      "encoder.encoder.layer.5.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.encoder.layer.5.layer_scale1.lambda1 torch.Size([768])\n",
      "encoder.encoder.layer.5.layer_scale2.lambda1 torch.Size([768])\n",
      "encoder.encoder.layer.5.mlp.fc1.bias torch.Size([3072])\n",
      "encoder.encoder.layer.5.mlp.fc1.weight torch.Size([3072, 768])\n",
      "encoder.encoder.layer.5.mlp.fc2.bias torch.Size([768])\n",
      "encoder.encoder.layer.5.mlp.fc2.weight torch.Size([768, 3072])\n",
      "encoder.encoder.layer.5.norm1.bias torch.Size([768])\n",
      "encoder.encoder.layer.5.norm1.weight torch.Size([768])\n",
      "encoder.encoder.layer.5.norm2.bias torch.Size([768])\n",
      "encoder.encoder.layer.5.norm2.weight torch.Size([768])\n",
      "encoder.encoder.layer.6.attention.attention.key.bias torch.Size([768])\n",
      "encoder.encoder.layer.6.attention.attention.key.weight torch.Size([768, 768])\n",
      "encoder.encoder.layer.6.attention.attention.query.bias torch.Size([768])\n",
      "encoder.encoder.layer.6.attention.attention.query.weight torch.Size([768, 768])\n",
      "encoder.encoder.layer.6.attention.attention.value.bias torch.Size([768])\n",
      "encoder.encoder.layer.6.attention.attention.value.weight torch.Size([768, 768])\n",
      "encoder.encoder.layer.6.attention.output.dense.bias torch.Size([768])\n",
      "encoder.encoder.layer.6.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.encoder.layer.6.layer_scale1.lambda1 torch.Size([768])\n",
      "encoder.encoder.layer.6.layer_scale2.lambda1 torch.Size([768])\n",
      "encoder.encoder.layer.6.mlp.fc1.bias torch.Size([3072])\n",
      "encoder.encoder.layer.6.mlp.fc1.weight torch.Size([3072, 768])\n",
      "encoder.encoder.layer.6.mlp.fc2.bias torch.Size([768])\n",
      "encoder.encoder.layer.6.mlp.fc2.weight torch.Size([768, 3072])\n",
      "encoder.encoder.layer.6.norm1.bias torch.Size([768])\n",
      "encoder.encoder.layer.6.norm1.weight torch.Size([768])\n",
      "encoder.encoder.layer.6.norm2.bias torch.Size([768])\n",
      "encoder.encoder.layer.6.norm2.weight torch.Size([768])\n",
      "encoder.encoder.layer.7.attention.attention.key.bias torch.Size([768])\n",
      "encoder.encoder.layer.7.attention.attention.key.weight torch.Size([768, 768])\n",
      "encoder.encoder.layer.7.attention.attention.query.bias torch.Size([768])\n",
      "encoder.encoder.layer.7.attention.attention.query.weight torch.Size([768, 768])\n",
      "encoder.encoder.layer.7.attention.attention.value.bias torch.Size([768])\n",
      "encoder.encoder.layer.7.attention.attention.value.weight torch.Size([768, 768])\n",
      "encoder.encoder.layer.7.attention.output.dense.bias torch.Size([768])\n",
      "encoder.encoder.layer.7.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.encoder.layer.7.layer_scale1.lambda1 torch.Size([768])\n",
      "encoder.encoder.layer.7.layer_scale2.lambda1 torch.Size([768])\n",
      "encoder.encoder.layer.7.mlp.fc1.bias torch.Size([3072])\n",
      "encoder.encoder.layer.7.mlp.fc1.weight torch.Size([3072, 768])\n",
      "encoder.encoder.layer.7.mlp.fc2.bias torch.Size([768])\n",
      "encoder.encoder.layer.7.mlp.fc2.weight torch.Size([768, 3072])\n",
      "encoder.encoder.layer.7.norm1.bias torch.Size([768])\n",
      "encoder.encoder.layer.7.norm1.weight torch.Size([768])\n",
      "encoder.encoder.layer.7.norm2.bias torch.Size([768])\n",
      "encoder.encoder.layer.7.norm2.weight torch.Size([768])\n",
      "encoder.encoder.layer.8.attention.attention.key.bias torch.Size([768])\n",
      "encoder.encoder.layer.8.attention.attention.key.weight torch.Size([768, 768])\n",
      "encoder.encoder.layer.8.attention.attention.query.bias torch.Size([768])\n",
      "encoder.encoder.layer.8.attention.attention.query.weight torch.Size([768, 768])\n",
      "encoder.encoder.layer.8.attention.attention.value.bias torch.Size([768])\n",
      "encoder.encoder.layer.8.attention.attention.value.weight torch.Size([768, 768])\n",
      "encoder.encoder.layer.8.attention.output.dense.bias torch.Size([768])\n",
      "encoder.encoder.layer.8.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.encoder.layer.8.layer_scale1.lambda1 torch.Size([768])\n",
      "encoder.encoder.layer.8.layer_scale2.lambda1 torch.Size([768])\n",
      "encoder.encoder.layer.8.mlp.fc1.bias torch.Size([3072])\n",
      "encoder.encoder.layer.8.mlp.fc1.weight torch.Size([3072, 768])\n",
      "encoder.encoder.layer.8.mlp.fc2.bias torch.Size([768])\n",
      "encoder.encoder.layer.8.mlp.fc2.weight torch.Size([768, 3072])\n",
      "encoder.encoder.layer.8.norm1.bias torch.Size([768])\n",
      "encoder.encoder.layer.8.norm1.weight torch.Size([768])\n",
      "encoder.encoder.layer.8.norm2.bias torch.Size([768])\n",
      "encoder.encoder.layer.8.norm2.weight torch.Size([768])\n",
      "encoder.encoder.layer.9.attention.attention.key.bias torch.Size([768])\n",
      "encoder.encoder.layer.9.attention.attention.key.weight torch.Size([768, 768])\n",
      "encoder.encoder.layer.9.attention.attention.query.bias torch.Size([768])\n",
      "encoder.encoder.layer.9.attention.attention.query.weight torch.Size([768, 768])\n",
      "encoder.encoder.layer.9.attention.attention.value.bias torch.Size([768])\n",
      "encoder.encoder.layer.9.attention.attention.value.weight torch.Size([768, 768])\n",
      "encoder.encoder.layer.9.attention.output.dense.bias torch.Size([768])\n",
      "encoder.encoder.layer.9.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.encoder.layer.9.layer_scale1.lambda1 torch.Size([768])\n",
      "encoder.encoder.layer.9.layer_scale2.lambda1 torch.Size([768])\n",
      "encoder.encoder.layer.9.mlp.fc1.bias torch.Size([3072])\n",
      "encoder.encoder.layer.9.mlp.fc1.weight torch.Size([3072, 768])\n",
      "encoder.encoder.layer.9.mlp.fc2.bias torch.Size([768])\n",
      "encoder.encoder.layer.9.mlp.fc2.weight torch.Size([768, 3072])\n",
      "encoder.encoder.layer.9.norm1.bias torch.Size([768])\n",
      "encoder.encoder.layer.9.norm1.weight torch.Size([768])\n",
      "encoder.encoder.layer.9.norm2.bias torch.Size([768])\n",
      "encoder.encoder.layer.9.norm2.weight torch.Size([768])\n",
      "encoder.layernorm.bias torch.Size([768])\n",
      "encoder.layernorm.weight torch.Size([768])\n",
      "v2l_projector.linear_1.bias torch.Size([2048])\n",
      "v2l_projector.linear_1.weight torch.Size([2048, 768])\n",
      "v2l_projector.linear_2.bias torch.Size([2048])\n",
      "v2l_projector.linear_2.weight torch.Size([2048, 2048])\n"
     ]
    }
   ],
   "source": [
    "for n, p in sd2.items():\n",
    "    print(n, p.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (arrg_img2text)",
   "language": "python",
   "name": "arrg_img2text"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
